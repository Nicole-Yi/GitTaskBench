{"id": 9, "timestamp": "2025-05-12T12:58:46.794015", "source": "agent", "message": "I read the file /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md.", "cause": 8, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "toolu_03", "model_response": {"id": "02174702592088186aa9a1d8ec7bfb23fb54dd22d5baf9bd3bca5", "created": 1747025926, "model": "deepseek-v3", "object": "chat.completion", "system_fingerprint": "", "choices": [{"finish_reason": "stop", "index": 0, "message": {"content": "Now let me view the \"Quick Start\" section (lines 129+) and the \"Model Zoo\" section (lines 97+) to understand how to perform speech recognition with the provided audio file.", "role": "assistant", "tool_calls": [{"index": 1, "function": {"arguments": "{\"command\": \"view\", \"path\": \"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md\", \"view_range\": [97, 150]}", "name": "str_replace_editor"}, "id": "toolu_03", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 98, "prompt_tokens": 13447, "total_tokens": 13545, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": "default"}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md:\n    97\t## Model Zoo\n    98\tFunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).\n    99\t\n   100\t(Note: \u2b50 represents the ModelScope model zoo, \ud83e\udd17 represents the Huggingface model zoo, \ud83c\udf40 represents the OpenAI model zoo)\n   101\t\n   102\t\n   103\t|                                                                                                         Model Name                                                                                                         |                                   Task Details                                   |          Training Data           | Parameters |\n   104\t|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|:--------------------------------:|:----------:|\n   105\t|                                        SenseVoiceSmall <br> ([\u2b50](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [\ud83e\udd17](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         | multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko   |           300000 hours           |    234M    |\n   106\t|          paraformer-zh <br> ([\u2b50](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [\ud83e\udd17](https://huggingface.co/funasr/paraformer-zh) )           |                speech recognition, with timestamps, non-streaming                |      60000 hours, Mandarin       |    220M    |\n   107\t| <nobr>paraformer-zh-streaming <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [\ud83e\udd17](https://huggingface.co/funasr/paraformer-zh-streaming) )</nobr> |                          speech recognition, streaming                           |      60000 hours, Mandarin       |    220M    |\n   108\t|               paraformer-en <br> ( [\u2b50](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [\ud83e\udd17](https://huggingface.co/funasr/paraformer-en) )                |              speech recognition, without timestamps, non-streaming               |       50000 hours, English       |    220M    |\n   109\t|                            conformer-en <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [\ud83e\udd17](https://huggingface.co/funasr/conformer-en) )                             |                        speech recognition, non-streaming                         |       50000 hours, English       |    220M    |\n   110\t|                               ct-punc <br> ( [\u2b50](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [\ud83e\udd17](https://huggingface.co/funasr/ct-punc) )                               |                             punctuation restoration                              |    100M, Mandarin and English    |    290M    | \n   111\t|                                   fsmn-vad <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [\ud83e\udd17](https://huggingface.co/funasr/fsmn-vad) )                                   |                             voice activity detection                             | 5000 hours, Mandarin and English |    0.4M    | \n   112\t|                                                              fsmn-kws <br> ( [\u2b50](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |     keyword spotting\uff0cstreaming      |  5000 hours, Mandarin  |    0.7M    | \n   113\t|                                     fa-zh <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [\ud83e\udd17](https://huggingface.co/funasr/fa-zh) )                                     |                               timestamp prediction                               |       5000 hours, Mandarin       |    38M     | \n   114\t|                                       cam++ <br> ( [\u2b50](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [\ud83e\udd17](https://huggingface.co/funasr/campplus) )                                        |                         speaker verification/diarization                         |            5000 hours            |    7.2M    | \n   115\t|                                            Whisper-large-v3 <br> ([\u2b50](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [\ud83c\udf40](https://github.com/openai/whisper) )                                            |                speech recognition, with timestamps, non-streaming                |           multilingual           |   1550 M   |\n   116\t|                                      Whisper-large-v3-turbo <br> ([\u2b50](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [\ud83c\udf40](https://github.com/openai/whisper) )                                      |                speech recognition, with timestamps, non-streaming                |           multilingual           |   809 M    |\n   117\t|                                               Qwen-Audio <br> ([\u2b50](examples/industrial_data_pretraining/qwen_audio/demo.py)  [\ud83e\udd17](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                    audio-text multimodal models (pretraining)                    |           multilingual           |     8B     |\n   118\t|                                        Qwen-Audio-Chat <br> ([\u2b50](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [\ud83e\udd17](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                        |                       audio-text multimodal models (chat)                        |           multilingual           |     8B     |\n   119\t|                              emotion2vec+large <br> ([\u2b50](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [\ud83e\udd17](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                               |                           speech emotion recongintion                            |           40000 hours            |    300M    |\n   120\t\n   121\t\n   122\t\n   123\t\n   124\t[//]: # ()\n   125\t[//]: # (FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in [docs]&#40;funasr/runtime/docs/SDK_tutorial.md&#41;. More detailed information about service deployment can be found in the [deployment roadmap]&#40;funasr/runtime/readme_cn.md&#41;.)\n   126\t\n   127\t\n   128\t<a name=\"quick-start\"></a>\n   129\t## Quick Start\n   130\t\n   131\tBelow is a quick start tutorial. Test audio files ([Mandarin](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav), [English](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)).\n   132\t\n   133\t### Command-line usage\n   134\t\n   135\t```shell\n   136\tfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n   137\t```\n   138\t\n   139\tNotes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: `wav_id wav_pat`\n   140\t\n   141\t### Speech Recognition (Non-streaming)\n   142\t#### SenseVoice\n   143\t```python\n   144\tfrom funasr import AutoModel\n   145\tfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n   146\t\n   147\tmodel_dir = \"iic/SenseVoiceSmall\"\n   148\t\n   149\tmodel = AutoModel(\n   150\t    model=model_dir,\n", "extras": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md", "impl_source": "oh_aci"}}