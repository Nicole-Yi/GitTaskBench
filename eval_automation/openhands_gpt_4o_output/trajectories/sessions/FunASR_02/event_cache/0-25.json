[{"id": 0, "timestamp": "2025-05-07T16:05:08.402424", "source": "user", "message": "\n## \u4efb\u52a1\u63cf\u8ff0\n\u4ece\u7ed9\u5b9a\u7684\u4ed3\u5e93\u5217\u8868\u4e2d\u9009\u53d6\u5408\u9002\u7684\u4ed3\u5e93\uff0c\u8bc6\u522b\u8f93\u5165\u7684\u82f1\u6587\u8bed\u97f3\uff0c\u5e76\u8f6c\u5316\u6210txt\u6587\u4ef6\n\n## \u53ef\u7528\u4ed3\u5e93\n\u4ed3\u5e93\u540d\u79f0: FunASR\n\u4ed3\u5e93\u8def\u5f84 (\u7edd\u5bf9): /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR\n\u7406\u89e3\u6307\u5357: ['\u9605\u8bfbREADME.md\u4e86\u89e3\u9879\u76ee\u57fa\u672c\u529f\u80fd\u548c\u4f7f\u7528\u65b9\u6cd5']\n\n## \u6587\u4ef6\u8def\u5f84\n\u8f93\u5165\uff1a\n\u6587\u4ef6\u8def\u5f84 (\u7edd\u5bf9): /data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3\n\u6587\u4ef6\u63cf\u8ff0: \u9700\u8981\u8bc6\u522b\u82f1\u6587\u7684\u8bed\u97f3\u6587\u4ef6\n\n\u8f93\u51fa\uff1a\n\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55:/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02, \u5982\u679c\u53ea\u6709\u4e00\u4e2a\u6587\u4ef6\uff0c\u5c31\u4ee5 `output` \u547d\u540d; \u5982\u679c\u5b58\u5728\u591a\u4e2a\u4ee5 `output_01`\u5f00\u59cb\u547d\u540d\uff0c\u683c\u5f0f\u6839\u636e\u9700\u6c42\u6307\u5b9a\u3002\n\n## \u8865\u5145\u8bf4\u660e\n**\u6838\u5fc3\u76ee\u6807**: \u5feb\u901f\u7406\u89e3\u548c\u5206\u6790\u4ee3\u7801\u4ed3\u5e93\uff0c\u751f\u6210\u5e76\u6267\u884c\u5fc5\u8981\u7684\u4ee3\u7801\u6216\u8c03\u7528\u5de5\u5177\uff0c\u4ee5\u9ad8\u6548\u3001\u51c6\u786e\u5730\u5b8c\u6210\u7528\u6237\u6307\u5b9a\u7684\u4efb\u52a1\u3002\n\n## \u5de5\u4f5c\u6d41\u7a0b\u4e0e\u89c4\u8303\n\n1.  **\u7406\u89e3\u4efb\u52a1**: \u4ed4\u7ec6\u5206\u6790\u7528\u6237\u63d0\u4f9b\u7684\u4efb\u52a1\u63cf\u8ff0 (`<task>`)\u3001\u5de5\u4f5c\u76ee\u5f55 (`<work_dir>`)\u3001\u4ed3\u5e93\u4fe1\u606f (`<repo>`) \u548c\u4ee3\u7801\u91cd\u8981\u6027\u63d0\u793a (`<code_importance>`)\u3002\n2.  **\u89c4\u5212\u65b9\u6848**: \n    *   \u5982\u679c\u6ca1\u6709\u73b0\u6210\u8ba1\u5212\uff0c\u5148\u5236\u5b9a\u6e05\u6670\u7684\u6267\u884c\u6b65\u9aa4\u3002\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684README.md\u6587\u4ef6\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n    *   \u5982\u679c\u6ca1\u6709README.md\u6587\u4ef6\u6216\u8005README.md\u6587\u4ef6\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u4fe1\u606f\uff0c\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684\u4ee3\u7801\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n    *   \u660e\u786e\u54ea\u4e9b\u6b65\u9aa4\u9700\u8981\u7f16\u5199\u4ee3\u7801\uff0c\u54ea\u4e9b\u6b65\u9aa4\u4f9d\u8d56\u8bed\u8a00\u7406\u89e3\u548c\u5de5\u5177\u8c03\u7528\u3002\n    *   \u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u6d89\u53ca\u5230\u8def\u5f84\u7684\u8bf7\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\uff0c\u4e0d\u8981\u4f7f\u7528\u76f8\u5bf9\u8def\u5f84\uff0c\u4ee5\u514d\u51fa\u73b0\u8def\u5f84\u9519\u8bef\u3002\n3.  **\u4ee3\u7801\u5e93\u5206\u6790**: \n    *   **\u63a2\u7d22\u7ed3\u6784**: \u5feb\u901f\u4e86\u89e3\u4ed3\u5e93\u7684\u6574\u4f53\u6587\u4ef6\u548c\u76ee\u5f55\u7ed3\u6784, \u8bf7\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\u3002\n    *   **\u8bc6\u522b\u5173\u952e\u6587\u4ef6**: \u4f18\u5148\u5173\u6ce8 `README.md`, \u914d\u7f6e\u6587\u4ef6, \u4e3b\u5165\u53e3\u811a\u672c\u7b49\u3002\n    *   **\u4f9d\u8d56\u7ba1\u7406**: \n        *   \u68c0\u67e5 `requirements.txt` \u6216\u7c7b\u4f3c\u6587\u4ef6\uff0c\u786e\u5b9a\u6240\u9700\u4f9d\u8d56\u3002\n        *   **\u5982\u679c\u9700\u8981\u5b89\u88c5\u4f9d\u8d56**\uff1a\u5728\u4ee3\u7801\u5757\u4e2d\u5305\u542b\u5b89\u88c5\u547d\u4ee4 (e.g., `pip install -r requirements.txt` \u6216 `pip install specific_package`)\u3002\u68c0\u67e5\u5305\u662f\u5426\u5b58\u5728\u907f\u514d\u91cd\u590d\u5b89\u88c5\u3002\n        *   **\u4e0d\u8981\u4f7f\u7528conda install\uff0c\u8bf7\u4f7f\u7528pip install**\u3002\n        *   **\u73af\u5883\u914d\u7f6e**: Python/Conda\u73af\u5883\u5df2\u9884\u8bbe\uff0c\u65e0\u9700\u989d\u5916\u914d\u7f6e\u3002\u4f46\u9700\u786e\u4fdd\u4ee3\u7801\u5e93\u8def\u5f84\u5728`PYTHONPATH`\u4e2d\uff0c**\u5fc5\u8981\u65f6\u751f\u6210** `export PYTHONPATH=\\\"$PYTHONPATH:{remote_repo_path}\\\"` \u547d\u4ee4\u3002\n4. \u4ee3\u7801\u5b9e\u73b0\u548c\u6267\u884c\n    * \u63d0\u4f9b\u8be6\u7ec6\u7684\u4ee3\u7801\u53ca\u5b9e\u73b0\u6b65\u9aa4\uff0c\u5305\u542b\u5b8c\u6574\u7684\u51fd\u6570/\u7c7b\u5b9a\u4e49\u3001\u53c2\u6570\u548c\u8fd4\u56de\u503c,\u63d0\u4f9b\u5fc5\u8981\u7684\u6ce8\u91ca\u548c\u6587\u6863\u5b57\u7b26\u4e32\n    * \u5982\u679c\u9700\u8981\u4f9d\u8d56\u4e00\u4e9bcheckpoint\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u5148\u68c0\u67e5\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5219\u76f4\u63a5\u4f7f\u7528\uff0c\u5426\u5219\u5148\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u518d\u4f7f\u7528(\u9700\u8981\u81ea\u52a8\u4e0b\u8f7d)\n        * \u6bd4\u5982\u9700\u8981\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528`wget`\u547d\u4ee4\u4e0b\u8f7d\uff0c\u5982\u679c\u9700\u8981\u4e0b\u8f7d\u591a\u4e2a\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528`wget -O`\u547d\u4ee4\u4e0b\u8f7d\u3002\n\n5.  **\u9519\u8bef\u5904\u7406\u4e0e\u8fed\u4ee3**: \n    *   \u68c0\u67e5\u4ee3\u7801\u6267\u884c\u7ed3\u679c\u3002\n    *   \u5982\u679c\u51fa\u73b0\u9519\u8bef\uff0c\u5206\u6790\u539f\u56e0\uff0c**\u4fee\u590d\u4ee3\u7801**\u5e76\u91cd\u65b0\u751f\u6210**\u5b8c\u6574**\u811a\u672c\u8fdb\u884c\u5c1d\u8bd5\u3002\n    *   \u5982\u679c\u591a\u6b21\u5c1d\u8bd5\u540e\u4ecd\u65e0\u6cd5\u89e3\u51b3\u6216\u4efb\u52a1\u65e0\u6cd5\u5b8c\u6210\uff0c\u5206\u6790\u539f\u56e0\u5e76\u8003\u8651\u66ff\u4ee3\u65b9\u6848\u3002\n6.  **\u5de5\u5177\u4f18\u5148**: \n    *   **\u5982\u679c\u9700\u8981\u4f9d\u8d56\u4e00\u4e9bcheckpoint\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u5148\u68c0\u67e5\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5219\u76f4\u63a5\u4f7f\u7528\uff0c\u5426\u5219\u5148\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u518d\u4f7f\u7528(\u9700\u8981\u81ea\u52a8\u4e0b\u8f7d)\n7.  **\u4efb\u52a1\u5b8c\u6210**: \n    *   \u5f53\u4efb\u52a1\u6210\u529f\u5b8c\u6210\uff0c\u6216\u786e\u8ba4\u65e0\u6cd5\u5b8c\u6210\u65f6\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u603b\u7ed3\u3002\n\n## !! \u5173\u952e\u7ea6\u675f\u4e0e\u5f3a\u5236\u8981\u6c42 !!\n\n*   **\u7edd\u5bf9\u8def\u5f84**: \u5728\u4ee3\u7801\u4e2d\u5904\u7406\u6587\u4ef6\uff08\u7279\u522b\u662f\u6570\u636e\u52a0\u8f7d\uff09\u65f6\uff0c**\u5fc5\u987b**\u4f7f\u7528**\u7edd\u5bf9\u8def\u5f84**\u3002\n*   **\u63d0\u4f9b\u7684\u4ed3\u5e93 > \u4ee3\u7801**: \u73b0\u6709\u4ed3\u5e93\u4ee3\u7801\u80fd\u5b8c\u6210\u7684\u4efb\u52a1\uff0c**\u4e25\u7981**\u81ea\u884c\u91cd\u65b0\u7f16\u5199\u4ee3\u7801\u5b9e\u73b0\u3002\n*   **\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684README.md\u6587\u4ef6\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5**\u3002\u5982\u679c\u6ca1\u6709README.md\u6587\u4ef6\u6216\u8005README.md\u6587\u4ef6\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u4fe1\u606f\uff0c\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684\u4ee3\u7801\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n\n** \u6267\u884c\u524d\u9996\u5148\u8fd0\u884c `eval $(curl -s http://deploy.i.shaipower.com/httpproxy)`", "action": "message", "args": {"content": "\n## \u4efb\u52a1\u63cf\u8ff0\n\u4ece\u7ed9\u5b9a\u7684\u4ed3\u5e93\u5217\u8868\u4e2d\u9009\u53d6\u5408\u9002\u7684\u4ed3\u5e93\uff0c\u8bc6\u522b\u8f93\u5165\u7684\u82f1\u6587\u8bed\u97f3\uff0c\u5e76\u8f6c\u5316\u6210txt\u6587\u4ef6\n\n## \u53ef\u7528\u4ed3\u5e93\n\u4ed3\u5e93\u540d\u79f0: FunASR\n\u4ed3\u5e93\u8def\u5f84 (\u7edd\u5bf9): /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR\n\u7406\u89e3\u6307\u5357: ['\u9605\u8bfbREADME.md\u4e86\u89e3\u9879\u76ee\u57fa\u672c\u529f\u80fd\u548c\u4f7f\u7528\u65b9\u6cd5']\n\n## \u6587\u4ef6\u8def\u5f84\n\u8f93\u5165\uff1a\n\u6587\u4ef6\u8def\u5f84 (\u7edd\u5bf9): /data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3\n\u6587\u4ef6\u63cf\u8ff0: \u9700\u8981\u8bc6\u522b\u82f1\u6587\u7684\u8bed\u97f3\u6587\u4ef6\n\n\u8f93\u51fa\uff1a\n\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55:/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02, \u5982\u679c\u53ea\u6709\u4e00\u4e2a\u6587\u4ef6\uff0c\u5c31\u4ee5 `output` \u547d\u540d; \u5982\u679c\u5b58\u5728\u591a\u4e2a\u4ee5 `output_01`\u5f00\u59cb\u547d\u540d\uff0c\u683c\u5f0f\u6839\u636e\u9700\u6c42\u6307\u5b9a\u3002\n\n## \u8865\u5145\u8bf4\u660e\n**\u6838\u5fc3\u76ee\u6807**: \u5feb\u901f\u7406\u89e3\u548c\u5206\u6790\u4ee3\u7801\u4ed3\u5e93\uff0c\u751f\u6210\u5e76\u6267\u884c\u5fc5\u8981\u7684\u4ee3\u7801\u6216\u8c03\u7528\u5de5\u5177\uff0c\u4ee5\u9ad8\u6548\u3001\u51c6\u786e\u5730\u5b8c\u6210\u7528\u6237\u6307\u5b9a\u7684\u4efb\u52a1\u3002\n\n## \u5de5\u4f5c\u6d41\u7a0b\u4e0e\u89c4\u8303\n\n1.  **\u7406\u89e3\u4efb\u52a1**: \u4ed4\u7ec6\u5206\u6790\u7528\u6237\u63d0\u4f9b\u7684\u4efb\u52a1\u63cf\u8ff0 (`<task>`)\u3001\u5de5\u4f5c\u76ee\u5f55 (`<work_dir>`)\u3001\u4ed3\u5e93\u4fe1\u606f (`<repo>`) \u548c\u4ee3\u7801\u91cd\u8981\u6027\u63d0\u793a (`<code_importance>`)\u3002\n2.  **\u89c4\u5212\u65b9\u6848**: \n    *   \u5982\u679c\u6ca1\u6709\u73b0\u6210\u8ba1\u5212\uff0c\u5148\u5236\u5b9a\u6e05\u6670\u7684\u6267\u884c\u6b65\u9aa4\u3002\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684README.md\u6587\u4ef6\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n    *   \u5982\u679c\u6ca1\u6709README.md\u6587\u4ef6\u6216\u8005README.md\u6587\u4ef6\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u4fe1\u606f\uff0c\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684\u4ee3\u7801\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n    *   \u660e\u786e\u54ea\u4e9b\u6b65\u9aa4\u9700\u8981\u7f16\u5199\u4ee3\u7801\uff0c\u54ea\u4e9b\u6b65\u9aa4\u4f9d\u8d56\u8bed\u8a00\u7406\u89e3\u548c\u5de5\u5177\u8c03\u7528\u3002\n    *   \u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u6d89\u53ca\u5230\u8def\u5f84\u7684\u8bf7\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\uff0c\u4e0d\u8981\u4f7f\u7528\u76f8\u5bf9\u8def\u5f84\uff0c\u4ee5\u514d\u51fa\u73b0\u8def\u5f84\u9519\u8bef\u3002\n3.  **\u4ee3\u7801\u5e93\u5206\u6790**: \n    *   **\u63a2\u7d22\u7ed3\u6784**: \u5feb\u901f\u4e86\u89e3\u4ed3\u5e93\u7684\u6574\u4f53\u6587\u4ef6\u548c\u76ee\u5f55\u7ed3\u6784, \u8bf7\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\u3002\n    *   **\u8bc6\u522b\u5173\u952e\u6587\u4ef6**: \u4f18\u5148\u5173\u6ce8 `README.md`, \u914d\u7f6e\u6587\u4ef6, \u4e3b\u5165\u53e3\u811a\u672c\u7b49\u3002\n    *   **\u4f9d\u8d56\u7ba1\u7406**: \n        *   \u68c0\u67e5 `requirements.txt` \u6216\u7c7b\u4f3c\u6587\u4ef6\uff0c\u786e\u5b9a\u6240\u9700\u4f9d\u8d56\u3002\n        *   **\u5982\u679c\u9700\u8981\u5b89\u88c5\u4f9d\u8d56**\uff1a\u5728\u4ee3\u7801\u5757\u4e2d\u5305\u542b\u5b89\u88c5\u547d\u4ee4 (e.g., `pip install -r requirements.txt` \u6216 `pip install specific_package`)\u3002\u68c0\u67e5\u5305\u662f\u5426\u5b58\u5728\u907f\u514d\u91cd\u590d\u5b89\u88c5\u3002\n        *   **\u4e0d\u8981\u4f7f\u7528conda install\uff0c\u8bf7\u4f7f\u7528pip install**\u3002\n        *   **\u73af\u5883\u914d\u7f6e**: Python/Conda\u73af\u5883\u5df2\u9884\u8bbe\uff0c\u65e0\u9700\u989d\u5916\u914d\u7f6e\u3002\u4f46\u9700\u786e\u4fdd\u4ee3\u7801\u5e93\u8def\u5f84\u5728`PYTHONPATH`\u4e2d\uff0c**\u5fc5\u8981\u65f6\u751f\u6210** `export PYTHONPATH=\\\"$PYTHONPATH:{remote_repo_path}\\\"` \u547d\u4ee4\u3002\n4. \u4ee3\u7801\u5b9e\u73b0\u548c\u6267\u884c\n    * \u63d0\u4f9b\u8be6\u7ec6\u7684\u4ee3\u7801\u53ca\u5b9e\u73b0\u6b65\u9aa4\uff0c\u5305\u542b\u5b8c\u6574\u7684\u51fd\u6570/\u7c7b\u5b9a\u4e49\u3001\u53c2\u6570\u548c\u8fd4\u56de\u503c,\u63d0\u4f9b\u5fc5\u8981\u7684\u6ce8\u91ca\u548c\u6587\u6863\u5b57\u7b26\u4e32\n    * \u5982\u679c\u9700\u8981\u4f9d\u8d56\u4e00\u4e9bcheckpoint\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u5148\u68c0\u67e5\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5219\u76f4\u63a5\u4f7f\u7528\uff0c\u5426\u5219\u5148\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u518d\u4f7f\u7528(\u9700\u8981\u81ea\u52a8\u4e0b\u8f7d)\n        * \u6bd4\u5982\u9700\u8981\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528`wget`\u547d\u4ee4\u4e0b\u8f7d\uff0c\u5982\u679c\u9700\u8981\u4e0b\u8f7d\u591a\u4e2a\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528`wget -O`\u547d\u4ee4\u4e0b\u8f7d\u3002\n\n5.  **\u9519\u8bef\u5904\u7406\u4e0e\u8fed\u4ee3**: \n    *   \u68c0\u67e5\u4ee3\u7801\u6267\u884c\u7ed3\u679c\u3002\n    *   \u5982\u679c\u51fa\u73b0\u9519\u8bef\uff0c\u5206\u6790\u539f\u56e0\uff0c**\u4fee\u590d\u4ee3\u7801**\u5e76\u91cd\u65b0\u751f\u6210**\u5b8c\u6574**\u811a\u672c\u8fdb\u884c\u5c1d\u8bd5\u3002\n    *   \u5982\u679c\u591a\u6b21\u5c1d\u8bd5\u540e\u4ecd\u65e0\u6cd5\u89e3\u51b3\u6216\u4efb\u52a1\u65e0\u6cd5\u5b8c\u6210\uff0c\u5206\u6790\u539f\u56e0\u5e76\u8003\u8651\u66ff\u4ee3\u65b9\u6848\u3002\n6.  **\u5de5\u5177\u4f18\u5148**: \n    *   **\u5982\u679c\u9700\u8981\u4f9d\u8d56\u4e00\u4e9bcheckpoint\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u5148\u68c0\u67e5\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5219\u76f4\u63a5\u4f7f\u7528\uff0c\u5426\u5219\u5148\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u518d\u4f7f\u7528(\u9700\u8981\u81ea\u52a8\u4e0b\u8f7d)\n7.  **\u4efb\u52a1\u5b8c\u6210**: \n    *   \u5f53\u4efb\u52a1\u6210\u529f\u5b8c\u6210\uff0c\u6216\u786e\u8ba4\u65e0\u6cd5\u5b8c\u6210\u65f6\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u603b\u7ed3\u3002\n\n## !! \u5173\u952e\u7ea6\u675f\u4e0e\u5f3a\u5236\u8981\u6c42 !!\n\n*   **\u7edd\u5bf9\u8def\u5f84**: \u5728\u4ee3\u7801\u4e2d\u5904\u7406\u6587\u4ef6\uff08\u7279\u522b\u662f\u6570\u636e\u52a0\u8f7d\uff09\u65f6\uff0c**\u5fc5\u987b**\u4f7f\u7528**\u7edd\u5bf9\u8def\u5f84**\u3002\n*   **\u63d0\u4f9b\u7684\u4ed3\u5e93 > \u4ee3\u7801**: \u73b0\u6709\u4ed3\u5e93\u4ee3\u7801\u80fd\u5b8c\u6210\u7684\u4efb\u52a1\uff0c**\u4e25\u7981**\u81ea\u884c\u91cd\u65b0\u7f16\u5199\u4ee3\u7801\u5b9e\u73b0\u3002\n*   **\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684README.md\u6587\u4ef6\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5**\u3002\u5982\u679c\u6ca1\u6709README.md\u6587\u4ef6\u6216\u8005README.md\u6587\u4ef6\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u4fe1\u606f\uff0c\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684\u4ee3\u7801\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n\n** \u6267\u884c\u524d\u9996\u5148\u8fd0\u884c `eval $(curl -s http://deploy.i.shaipower.com/httpproxy)`", "image_urls": null, "wait_for_response": false}}, {"id": 1, "timestamp": "2025-05-07T16:05:08.414261", "source": "user", "message": "Retrieving content for: \n## \u4efb\u52a1\u63cf\u8ff0\n\u4ece\u7ed9\u5b9a\u7684\u4ed3\u5e93\u5217\u8868\u4e2d\u9009\u53d6\u5408\u9002\u7684\u4ed3\u5e93\uff0c\u8bc6\u522b\u8f93\u5165\u7684\u82f1\u6587\u8bed\u97f3\uff0c\u5e76\u8f6c\u5316\u6210txt\u6587\u4ef6\n\n## ", "action": "recall", "args": {"recall_type": "workspace_context", "query": "\n## \u4efb\u52a1\u63cf\u8ff0\n\u4ece\u7ed9\u5b9a\u7684\u4ed3\u5e93\u5217\u8868\u4e2d\u9009\u53d6\u5408\u9002\u7684\u4ed3\u5e93\uff0c\u8bc6\u522b\u8f93\u5165\u7684\u82f1\u6587\u8bed\u97f3\uff0c\u5e76\u8f6c\u5316\u6210txt\u6587\u4ef6\n\n## \u53ef\u7528\u4ed3\u5e93\n\u4ed3\u5e93\u540d\u79f0: FunASR\n\u4ed3\u5e93\u8def\u5f84 (\u7edd\u5bf9): /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR\n\u7406\u89e3\u6307\u5357: ['\u9605\u8bfbREADME.md\u4e86\u89e3\u9879\u76ee\u57fa\u672c\u529f\u80fd\u548c\u4f7f\u7528\u65b9\u6cd5']\n\n## \u6587\u4ef6\u8def\u5f84\n\u8f93\u5165\uff1a\n\u6587\u4ef6\u8def\u5f84 (\u7edd\u5bf9): /data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3\n\u6587\u4ef6\u63cf\u8ff0: \u9700\u8981\u8bc6\u522b\u82f1\u6587\u7684\u8bed\u97f3\u6587\u4ef6\n\n\u8f93\u51fa\uff1a\n\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55:/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02, \u5982\u679c\u53ea\u6709\u4e00\u4e2a\u6587\u4ef6\uff0c\u5c31\u4ee5 `output` \u547d\u540d; \u5982\u679c\u5b58\u5728\u591a\u4e2a\u4ee5 `output_01`\u5f00\u59cb\u547d\u540d\uff0c\u683c\u5f0f\u6839\u636e\u9700\u6c42\u6307\u5b9a\u3002\n\n## \u8865\u5145\u8bf4\u660e\n**\u6838\u5fc3\u76ee\u6807**: \u5feb\u901f\u7406\u89e3\u548c\u5206\u6790\u4ee3\u7801\u4ed3\u5e93\uff0c\u751f\u6210\u5e76\u6267\u884c\u5fc5\u8981\u7684\u4ee3\u7801\u6216\u8c03\u7528\u5de5\u5177\uff0c\u4ee5\u9ad8\u6548\u3001\u51c6\u786e\u5730\u5b8c\u6210\u7528\u6237\u6307\u5b9a\u7684\u4efb\u52a1\u3002\n\n## \u5de5\u4f5c\u6d41\u7a0b\u4e0e\u89c4\u8303\n\n1.  **\u7406\u89e3\u4efb\u52a1**: \u4ed4\u7ec6\u5206\u6790\u7528\u6237\u63d0\u4f9b\u7684\u4efb\u52a1\u63cf\u8ff0 (`<task>`)\u3001\u5de5\u4f5c\u76ee\u5f55 (`<work_dir>`)\u3001\u4ed3\u5e93\u4fe1\u606f (`<repo>`) \u548c\u4ee3\u7801\u91cd\u8981\u6027\u63d0\u793a (`<code_importance>`)\u3002\n2.  **\u89c4\u5212\u65b9\u6848**: \n    *   \u5982\u679c\u6ca1\u6709\u73b0\u6210\u8ba1\u5212\uff0c\u5148\u5236\u5b9a\u6e05\u6670\u7684\u6267\u884c\u6b65\u9aa4\u3002\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684README.md\u6587\u4ef6\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n    *   \u5982\u679c\u6ca1\u6709README.md\u6587\u4ef6\u6216\u8005README.md\u6587\u4ef6\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u4fe1\u606f\uff0c\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684\u4ee3\u7801\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n    *   \u660e\u786e\u54ea\u4e9b\u6b65\u9aa4\u9700\u8981\u7f16\u5199\u4ee3\u7801\uff0c\u54ea\u4e9b\u6b65\u9aa4\u4f9d\u8d56\u8bed\u8a00\u7406\u89e3\u548c\u5de5\u5177\u8c03\u7528\u3002\n    *   \u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u6d89\u53ca\u5230\u8def\u5f84\u7684\u8bf7\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\uff0c\u4e0d\u8981\u4f7f\u7528\u76f8\u5bf9\u8def\u5f84\uff0c\u4ee5\u514d\u51fa\u73b0\u8def\u5f84\u9519\u8bef\u3002\n3.  **\u4ee3\u7801\u5e93\u5206\u6790**: \n    *   **\u63a2\u7d22\u7ed3\u6784**: \u5feb\u901f\u4e86\u89e3\u4ed3\u5e93\u7684\u6574\u4f53\u6587\u4ef6\u548c\u76ee\u5f55\u7ed3\u6784, \u8bf7\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\u3002\n    *   **\u8bc6\u522b\u5173\u952e\u6587\u4ef6**: \u4f18\u5148\u5173\u6ce8 `README.md`, \u914d\u7f6e\u6587\u4ef6, \u4e3b\u5165\u53e3\u811a\u672c\u7b49\u3002\n    *   **\u4f9d\u8d56\u7ba1\u7406**: \n        *   \u68c0\u67e5 `requirements.txt` \u6216\u7c7b\u4f3c\u6587\u4ef6\uff0c\u786e\u5b9a\u6240\u9700\u4f9d\u8d56\u3002\n        *   **\u5982\u679c\u9700\u8981\u5b89\u88c5\u4f9d\u8d56**\uff1a\u5728\u4ee3\u7801\u5757\u4e2d\u5305\u542b\u5b89\u88c5\u547d\u4ee4 (e.g., `pip install -r requirements.txt` \u6216 `pip install specific_package`)\u3002\u68c0\u67e5\u5305\u662f\u5426\u5b58\u5728\u907f\u514d\u91cd\u590d\u5b89\u88c5\u3002\n        *   **\u4e0d\u8981\u4f7f\u7528conda install\uff0c\u8bf7\u4f7f\u7528pip install**\u3002\n        *   **\u73af\u5883\u914d\u7f6e**: Python/Conda\u73af\u5883\u5df2\u9884\u8bbe\uff0c\u65e0\u9700\u989d\u5916\u914d\u7f6e\u3002\u4f46\u9700\u786e\u4fdd\u4ee3\u7801\u5e93\u8def\u5f84\u5728`PYTHONPATH`\u4e2d\uff0c**\u5fc5\u8981\u65f6\u751f\u6210** `export PYTHONPATH=\\\"$PYTHONPATH:{remote_repo_path}\\\"` \u547d\u4ee4\u3002\n4. \u4ee3\u7801\u5b9e\u73b0\u548c\u6267\u884c\n    * \u63d0\u4f9b\u8be6\u7ec6\u7684\u4ee3\u7801\u53ca\u5b9e\u73b0\u6b65\u9aa4\uff0c\u5305\u542b\u5b8c\u6574\u7684\u51fd\u6570/\u7c7b\u5b9a\u4e49\u3001\u53c2\u6570\u548c\u8fd4\u56de\u503c,\u63d0\u4f9b\u5fc5\u8981\u7684\u6ce8\u91ca\u548c\u6587\u6863\u5b57\u7b26\u4e32\n    * \u5982\u679c\u9700\u8981\u4f9d\u8d56\u4e00\u4e9bcheckpoint\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u5148\u68c0\u67e5\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5219\u76f4\u63a5\u4f7f\u7528\uff0c\u5426\u5219\u5148\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u518d\u4f7f\u7528(\u9700\u8981\u81ea\u52a8\u4e0b\u8f7d)\n        * \u6bd4\u5982\u9700\u8981\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528`wget`\u547d\u4ee4\u4e0b\u8f7d\uff0c\u5982\u679c\u9700\u8981\u4e0b\u8f7d\u591a\u4e2a\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528`wget -O`\u547d\u4ee4\u4e0b\u8f7d\u3002\n\n5.  **\u9519\u8bef\u5904\u7406\u4e0e\u8fed\u4ee3**: \n    *   \u68c0\u67e5\u4ee3\u7801\u6267\u884c\u7ed3\u679c\u3002\n    *   \u5982\u679c\u51fa\u73b0\u9519\u8bef\uff0c\u5206\u6790\u539f\u56e0\uff0c**\u4fee\u590d\u4ee3\u7801**\u5e76\u91cd\u65b0\u751f\u6210**\u5b8c\u6574**\u811a\u672c\u8fdb\u884c\u5c1d\u8bd5\u3002\n    *   \u5982\u679c\u591a\u6b21\u5c1d\u8bd5\u540e\u4ecd\u65e0\u6cd5\u89e3\u51b3\u6216\u4efb\u52a1\u65e0\u6cd5\u5b8c\u6210\uff0c\u5206\u6790\u539f\u56e0\u5e76\u8003\u8651\u66ff\u4ee3\u65b9\u6848\u3002\n6.  **\u5de5\u5177\u4f18\u5148**: \n    *   **\u5982\u679c\u9700\u8981\u4f9d\u8d56\u4e00\u4e9bcheckpoint\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u5148\u68c0\u67e5\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5219\u76f4\u63a5\u4f7f\u7528\uff0c\u5426\u5219\u5148\u4e0b\u8f7dcheckpoint\u6587\u4ef6\uff0c\u518d\u4f7f\u7528(\u9700\u8981\u81ea\u52a8\u4e0b\u8f7d)\n7.  **\u4efb\u52a1\u5b8c\u6210**: \n    *   \u5f53\u4efb\u52a1\u6210\u529f\u5b8c\u6210\uff0c\u6216\u786e\u8ba4\u65e0\u6cd5\u5b8c\u6210\u65f6\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u603b\u7ed3\u3002\n\n## !! \u5173\u952e\u7ea6\u675f\u4e0e\u5f3a\u5236\u8981\u6c42 !!\n\n*   **\u7edd\u5bf9\u8def\u5f84**: \u5728\u4ee3\u7801\u4e2d\u5904\u7406\u6587\u4ef6\uff08\u7279\u522b\u662f\u6570\u636e\u52a0\u8f7d\uff09\u65f6\uff0c**\u5fc5\u987b**\u4f7f\u7528**\u7edd\u5bf9\u8def\u5f84**\u3002\n*   **\u63d0\u4f9b\u7684\u4ed3\u5e93 > \u4ee3\u7801**: \u73b0\u6709\u4ed3\u5e93\u4ee3\u7801\u80fd\u5b8c\u6210\u7684\u4efb\u52a1\uff0c**\u4e25\u7981**\u81ea\u884c\u91cd\u65b0\u7f16\u5199\u4ee3\u7801\u5b9e\u73b0\u3002\n*   **\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684README.md\u6587\u4ef6\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5**\u3002\u5982\u679c\u6ca1\u6709README.md\u6587\u4ef6\u6216\u8005README.md\u6587\u4ef6\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u4fe1\u606f\uff0c\u8bf7\u5148\u9605\u8bfb\u4ee3\u7801\u5e93\u7684\u4ee3\u7801\uff0c\u4e86\u89e3\u4ee3\u7801\u5e93\u7684\u7ed3\u6784\u548c\u4f7f\u7528\u65b9\u6cd5\u3002\n\n** \u6267\u884c\u524d\u9996\u5148\u8fd0\u884c `eval $(curl -s http://deploy.i.shaipower.com/httpproxy)`", "thought": ""}}, {"id": 2, "timestamp": "2025-05-07T16:05:08.414591", "source": "environment", "message": "", "observation": "agent_state_changed", "content": "", "extras": {"agent_state": "running", "reason": ""}}, {"id": 3, "timestamp": "2025-05-07T16:05:08.415429", "source": "environment", "message": "Added workspace context", "cause": 1, "observation": "recall", "content": "Added workspace context", "extras": {"recall_type": "workspace_context", "repo_name": "", "repo_directory": "", "repo_instructions": "", "runtime_hosts": {"http://localhost:50230": 50230, "http://localhost:56395": 56395}, "additional_agent_instructions": "", "date": "2025-05-07", "microagent_knowledge": [{"name": "github", "trigger": "git", "content": "You have access to an environment variable, `GITHUB_TOKEN`, which allows you to interact with\nthe GitHub API.\n\nYou can use `curl` with the `GITHUB_TOKEN` to interact with GitHub's API.\nALWAYS use the GitHub API for operations instead of a web browser.\n\nIf you encounter authentication issues when pushing to GitHub (such as password prompts or permission errors), the old token may have expired. In such case, update the remote URL to include the current token: `git remote set-url origin https://${GITHUB_TOKEN}@github.com/username/repo.git`\n\nHere are some instructions for pushing, but ONLY do this if the user asks you to:\n* NEVER push directly to the `main` or `master` branch\n* Git config (username and email) is pre-set. Do not modify.\n* You may already be on a branch starting with `openhands-workspace`. Create a new branch with a better name before pushing.\n* Use the GitHub API to create a pull request, if you haven't already\n* Once you've created your own branch or a pull request, continue to update it. Do NOT create a new one unless you are explicitly asked to. Update the PR title and description as necessary, but don't change the branch name.\n* Use the main branch as the base branch, unless the user requests otherwise\n* After opening or updating a pull request, send the user a short message with a link to the pull request.\n* Prefer \"Draft\" pull requests when possible\n* Do NOT mark a pull request as ready to review unless the user explicitly says so\n* Do all of the above in as few steps as possible. E.g. you could open a PR with one step by running the following bash commands:\n```bash\ngit remote -v && git branch # to find the current org, repo and branch\ngit checkout -b create-widget && git add . && git commit -m \"Create widget\" && git push -u origin create-widget\ncurl -X POST \"https://api.github.com/repos/$ORG_NAME/$REPO_NAME/pulls\" \\\n    -H \"Authorization: Bearer $GITHUB_TOKEN\" \\\n    -d '{\"title\":\"Create widget\",\"head\":\"create-widget\",\"base\":\"openhands-workspace\"}'\n```"}, {"name": "gitlab", "trigger": "git", "content": "You have access to an environment variable, `GITLAB_TOKEN`, which allows you to interact with\nthe GitLab API.\n\nYou can use `curl` with the `GITLAB_TOKEN` to interact with GitLab's API.\nALWAYS use the GitLab API for operations instead of a web browser.\n\nIf you encounter authentication issues when pushing to GitLab (such as password prompts or permission errors), the old token may have expired. In such case, update the remote URL to include the current token: `git remote set-url origin https://oauth2:${GITLAB_TOKEN}@gitlab.com/username/repo.git`\n\nHere are some instructions for pushing, but ONLY do this if the user asks you to:\n* NEVER push directly to the `main` or `master` branch\n* Git config (username and email) is pre-set. Do not modify.\n* You may already be on a branch starting with `openhands-workspace`. Create a new branch with a better name before pushing.\n* Use the GitLab API to create a merge request, if you haven't already\n* Once you've created your own branch or a merge request, continue to update it. Do NOT create a new one unless you are explicitly asked to. Update the PR title and description as necessary, but don't change the branch name.\n* Use the main branch as the base branch, unless the user requests otherwise\n* After opening or updating a merge request, send the user a short message with a link to the merge request.\n* Prefer \"Draft\" merge requests when possible\n* Do all of the above in as few steps as possible. E.g. you could open a PR with one step by running the following bash commands:\n```bash\ngit remote -v && git branch # to find the current org, repo and branch\ngit checkout -b create-widget && git add . && git commit -m \"Create widget\" && git push -u origin create-widget\ncurl -X POST \"https://gitlab.com/api/v4/projects/$PROJECT_ID/merge_requests\" \\\n    -H \"Authorization: Bearer $GITLAB_TOKEN\" \\\n    -d '{\"source_branch\": \"create-widget\", \"target_branch\": \"openhands-workspace\", \"title\": \"Create widget\"}'\n```"}]}}, {"id": 4, "timestamp": "2025-05-07T16:05:10.403480", "source": "agent", "message": "Running command: eval $(curl -s http://deploy.i.shaipower.com/httpproxy)", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_f3nzZ6sCEGj7SOx8TbdaRStV", "model_response": {"id": "chatcmpl-BUU45H2Pu4XTS7JGCUzmXft7b4zov", "created": 1746605109, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\": \"eval $(curl -s http://deploy.i.shaipower.com/httpproxy)\"}", "name": "execute_bash"}, "id": "call_f3nzZ6sCEGj7SOx8TbdaRStV", "type": "function"}, {"function": {"arguments": "{\"command\": \"view\", \"path\": \"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md\"}", "name": "str_replace_editor"}, "id": "call_gZTkVHudqt0xjDoTXOn1feXz", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 89, "prompt_tokens": 5994, "total_tokens": 6083, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 2}, "llm_metrics": {"accumulated_cost": 0.015875, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 5994, "completion_tokens": 89, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "eval $(curl -s http://deploy.i.shaipower.com/httpproxy)", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 5, "timestamp": "2025-05-07T16:05:11.044370", "source": "agent", "message": "Command `eval $(curl -s http://deploy.i.shaipower.com/httpproxy)` executed with exit code 0.", "cause": 4, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_f3nzZ6sCEGj7SOx8TbdaRStV", "model_response": {"id": "chatcmpl-BUU45H2Pu4XTS7JGCUzmXft7b4zov", "created": 1746605109, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\": \"eval $(curl -s http://deploy.i.shaipower.com/httpproxy)\"}", "name": "execute_bash"}, "id": "call_f3nzZ6sCEGj7SOx8TbdaRStV", "type": "function"}, {"function": {"arguments": "{\"command\": \"view\", \"path\": \"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md\"}", "name": "str_replace_editor"}, "id": "call_gZTkVHudqt0xjDoTXOn1feXz", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 89, "prompt_tokens": 5994, "total_tokens": 6083, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 2}, "content": "", "extras": {"command": "eval $(curl -s http://deploy.i.shaipower.com/httpproxy)", "metadata": {"exit_code": 0, "pid": -1, "username": "openhands", "hostname": "4aa4d9a317a0", "working_dir": "/data/data/agent_test_codebase/GitTaskBench", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 0.]"}, "hidden": false}, "success": true}, {"id": 6, "timestamp": "2025-05-07T16:05:11.056042", "source": "agent", "message": "Reading file: /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md", "action": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_gZTkVHudqt0xjDoTXOn1feXz", "model_response": {"id": "chatcmpl-BUU45H2Pu4XTS7JGCUzmXft7b4zov", "created": 1746605109, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\": \"eval $(curl -s http://deploy.i.shaipower.com/httpproxy)\"}", "name": "execute_bash"}, "id": "call_f3nzZ6sCEGj7SOx8TbdaRStV", "type": "function"}, {"function": {"arguments": "{\"command\": \"view\", \"path\": \"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md\"}", "name": "str_replace_editor"}, "id": "call_gZTkVHudqt0xjDoTXOn1feXz", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 89, "prompt_tokens": 5994, "total_tokens": 6083, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 2}, "llm_metrics": {"accumulated_cost": 0.015875, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 5994, "completion_tokens": 89, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md", "start": 0, "end": -1, "thought": "", "impl_source": "oh_aci", "view_range": null}}, {"id": 7, "timestamp": "2025-05-07T16:05:11.088718", "source": "agent", "message": "I read the file /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md.", "cause": 6, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_gZTkVHudqt0xjDoTXOn1feXz", "model_response": {"id": "chatcmpl-BUU45H2Pu4XTS7JGCUzmXft7b4zov", "created": 1746605109, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\": \"eval $(curl -s http://deploy.i.shaipower.com/httpproxy)\"}", "name": "execute_bash"}, "id": "call_f3nzZ6sCEGj7SOx8TbdaRStV", "type": "function"}, {"function": {"arguments": "{\"command\": \"view\", \"path\": \"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md\"}", "name": "str_replace_editor"}, "id": "call_gZTkVHudqt0xjDoTXOn1feXz", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 89, "prompt_tokens": 5994, "total_tokens": 6083, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 2}, "content": "Here's the result of running `cat -n` on /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md:\n     1\t[//]: # (<div align=\"left\"><img src=\"docs/images/funasr_logo.jpg\" width=\"400\"/></div>)\n     2\t\n     3\t([\u7b80\u4f53\u4e2d\u6587](./README_zh.md)|English)\n     4\t\n     5\t[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)\n     6\t\n     7\t[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&text1=FunASR\ud83e\udd20&text2=\ud83d\udc96%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&width=800&height=210)](https://github.com/Akshay090/svg-banners)\n     8\t\n     9\t[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)\n    10\t\n    11\t<p align=\"center\">\n    12\t<a href=\"https://trendshift.io/repositories/3839\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/3839\" alt=\"alibaba-damo-academy%2FFunASR | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n    13\t</p>\n    14\t\n    15\t<strong>FunASR</strong> hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training & finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for Fun\uff01\n    16\t\n    17\t[**Highlights**](#highlights)\n    18\t| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new) \n    19\t| [**Installation**](#installation)\n    20\t| [**Quick Start**](#quick-start)\n    21\t| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)\n    22\t| [**Runtime**](./runtime/readme.md)\n    23\t| [**Model Zoo**](#model-zoo)\n    24\t| [**Contact**](#contact)\n    25\t\n    26\t\n    27\t\n    28\t\n    29\t<a name=\"highlights\"></a>\n    30\t## Highlights\n    31\t- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.\n    32\t- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md). \n    33\t\n    34\t\n    35\t<a name=\"whats-new\"></a>\n    36\t## What's new:\n    37\t- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal model\uff1b([docs](runtime/readme.md));\n    38\t- 2024/10/10\uff1aAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n    39\t- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak & Support the SensevoiceSmall onnx model\uff1bFile Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));\n    40\t- 2024/09/25\uff1akeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).\n    41\t- 2024/07/04\uff1a[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.\n    42\t- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))\n    43\t- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads' speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))\n    44\t- 2024/05/15\uff1aemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)\uff0c[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)\uff0c[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.\n    45\t- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structure\uff1b([docs](runtime/readme.md))\n    46\t\n    47\t<details><summary>Full Changelog</summary>\n    48\t\n    49\t- 2024/03/05\uff1aAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).\n    50\t- 2024/03/05\uff1aAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n    51\t- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5\uff0cReal-time Transcription Service 1.9 released\uff0cdocker image supports ARM64 platform, update modelscope\uff1b([docs](runtime/readme.md))\n    52\t- 2024/01/30\uff1afunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))\n    53\t- 2024/01/30\uff1aemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).\n    54\t- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 released\uff0coptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 released\uff0coptimizatized the client-side\uff1b([docs](runtime/readme.md))\n    55\t- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n    56\t- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).\n    57\t- 2024/01/03: Real-time Transcription Service 1.6 released\uff0cThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))\n    58\t- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).\n    59\t- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n    60\t- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu). \n    61\t- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).\n    62\t- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.\n    63\t- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.\n    64\t- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.\n    65\t- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).\n    66\t- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).\n    67\t- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).\n    68\t- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).\n    69\t\n    70\t</details>\n    71\t\n    72\t<a name=\"Installation\"></a>\n    73\t## Installation\n    74\t\n    75\t- Requirements\n    76\t```text\n    77\tpython>=3.8\n    78\ttorch>=1.13\n    79\ttorchaudio\n    80\t```\n    81\t\n    82\t- Install for pypi\n    83\t```shell\n    84\tpip3 install -U funasr\n    85\t```\n    86\t- Or install from source code\n    87\t``` sh\n    88\tgit clone https://github.com/alibaba/FunASR.git && cd FunASR\n    89\tpip3 install -e ./\n    90\t```\n    91\t- Install modelscope or huggingface_hub for the pretrained models (Optional)\n    92\t\n    93\t```shell\n    94\tpip3 install -U modelscope huggingface_hub\n    95\t```\n    96\t\n    97\t## Model Zoo\n    98\tFunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).\n    99\t\n   100\t(Note: \u2b50 represents the ModelScope model zoo, \ud83e\udd17 represents the Huggingface model zoo, \ud83c\udf40 represents the OpenAI model zoo)\n   101\t\n   102\t\n   103\t|                                                                                                         Model Name                                                                                                         |                                   Task Details                                   |          Training Data           | Parameters |\n   104\t|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|:--------------------------------:|:----------:|\n   105\t|                                        SenseVoiceSmall <br> ([\u2b50](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [\ud83e\udd17](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         | multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko   |           300000 hours           |    234M    |\n   106\t|          paraformer-zh <br> ([\u2b50](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [\ud83e\udd17](https://huggingface.co/funasr/paraformer-zh) )           |                speech recognition, with timestamps, non-streaming                |      60000 hours, Mandarin       |    220M    |\n   107\t| <nobr>paraformer-zh-streaming <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [\ud83e\udd17](https://huggingface.co/funasr/paraformer-zh-streaming) )</nobr> |                          speech recognition, streaming                           |      60000 hours, Mandarin       |    220M    |\n   108\t|               paraformer-en <br> ( [\u2b50](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [\ud83e\udd17](https://huggingface.co/funasr/paraformer-en) )                |              speech recognition, without timestamps, non-streaming               |       50000 hours, English       |    220M    |\n   109\t|                            conformer-en <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [\ud83e\udd17](https://huggingface.co/funasr/conformer-en) )                             |                        speech recognition, non-streaming                         |       50000 hours, English       |    220M    |\n   110\t|                               ct-punc <br> ( [\u2b50](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [\ud83e\udd17](https://huggingface.co/funasr/ct-punc) )                               |                             punctuation restoration                              |    100M, Mandarin and English    |    290M    | \n   111\t|                                   fsmn-vad <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [\ud83e\udd17](https://huggingface.co/funasr/fsmn-vad) )                                   |                             voice activity detection                             | 5000 hours, Mandarin and English |    0.4M    | \n   112\t|                                                              fsmn-kws <br> ( [\u2b50](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |     keyword spotting\uff0cstreaming      |  5000 hours, Mandarin  |    0.7M    | \n   113\t|                                     fa-zh <br> ( [\u2b50](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [\ud83e\udd17](https://huggingface.co/funasr/fa-zh) )                                     |                               timestamp prediction                               |       5000 hours, Mandarin       |    38M     | \n   114\t|                                       cam++ <br> ( [\u2b50](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [\ud83e\udd17](https://huggingface.co/funasr/campplus) )                                        |                         speaker verification/diarization                         |            5000 hours            |    7.2M    | \n   115\t|                                            Whisper-large-v3 <br> ([\u2b50](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [\ud83c\udf40](https://github.com/openai/whisper) )                                            |                speech recognition, with timestamps, non-streaming              <response clipped><NOTE>Due to the max output limit, only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>\n", "extras": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/README.md", "impl_source": "oh_aci"}}, {"id": 8, "timestamp": "2025-05-07T16:05:18.546625", "source": "agent", "message": "Running command: find /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR -type f -name '*.py' -o -name '*.sh' | xargs grep -E 'speech|recognition|transcription'", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_8lsqZh5Q6IjwTUVAaenL86h1", "model_response": {"id": "chatcmpl-BUU48zEvX2leLSX0cFrdq8KpR8cqM", "created": 1746605112, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "The `README.md` of the FunASR repository provides comprehensive information about the project, including installation instructions, usage, and available features. The key points extracted from the README are as follows:\n\n- **Installation**:  \n  - The project requires Python >= 3.8, torch >= 1.13, and torchaudio.\n  - FunASR can be installed via PyPI using `pip3 install -U funasr` or from source using `pip3 install -e ./`.\n  - Additional installations for `modelscope` or `huggingface_hub` may be required for pre-trained models.\n\n- **Features**:\n  - FunASR provides tools for speech recognition (ASR), voice activity detection (VAD), punctuation restoration, speaker verification/diarization, and more.\n  - It offers scripts and tutorials for model inference and fine-tuning. The repository includes a collection of pretrained models available on ModelScope and Huggingface.\n\n- **Model Information**:\n  - The README references various models such as SenseVoiceSmall, Paraformer (both Mandarin and English), and Whisper-large-v3. These models cater to different speech-related tasks.\n\nNext, we need to identify the procedure to convert the given English audio input to text. This process should be detailed in the README or the associated documentation referred to within the README. \n\n1. **Environment Setup**:\n    - Ensure dependencies like `funasr`, `modelscope`, and `huggingface_hub` are installed.\n  \n2. **Execution**:\n    - Locate or identify a script for audio processing, specifically for English speech recognition.\n    - Verify if there are any specific models recommended for English audio inputs.\n\nI'll explore the directory structure for possible scripts or directories focusing on speech recognition tasks. Let's inspect the repository further to identify relevant scripts for audio to text conversion.", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"find /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR -type f -name '*.py' -o -name '*.sh' | xargs grep -E 'speech|recognition|transcription'\"}", "name": "execute_bash"}, "id": "call_8lsqZh5Q6IjwTUVAaenL86h1", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 446, "prompt_tokens": 10944, "total_tokens": 11390, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.047695, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 16938, "completion_tokens": 535, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "find /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR -type f -name '*.py' -o -name '*.sh' | xargs grep -E 'speech|recognition|transcription'", "is_input": false, "thought": "The `README.md` of the FunASR repository provides comprehensive information about the project, including installation instructions, usage, and available features. The key points extracted from the README are as follows:\n\n- **Installation**:  \n  - The project requires Python >= 3.8, torch >= 1.13, and torchaudio.\n  - FunASR can be installed via PyPI using `pip3 install -U funasr` or from source using `pip3 install -e ./`.\n  - Additional installations for `modelscope` or `huggingface_hub` may be required for pre-trained models.\n\n- **Features**:\n  - FunASR provides tools for speech recognition (ASR), voice activity detection (VAD), punctuation restoration, speaker verification/diarization, and more.\n  - It offers scripts and tutorials for model inference and fine-tuning. The repository includes a collection of pretrained models available on ModelScope and Huggingface.\n\n- **Model Information**:\n  - The README references various models such as SenseVoiceSmall, Paraformer (both Mandarin and English), and Whisper-large-v3. These models cater to different speech-related tasks.\n\nNext, we need to identify the procedure to convert the given English audio input to text. This process should be detailed in the README or the associated documentation referred to within the README. \n\n1. **Environment Setup**:\n    - Ensure dependencies like `funasr`, `modelscope`, and `huggingface_hub` are installed.\n  \n2. **Execution**:\n    - Locate or identify a script for audio processing, specifically for English speech recognition.\n    - Verify if there are any specific models recommended for English audio inputs.\n\nI'll explore the directory structure for possible scripts or directories focusing on speech recognition tasks. Let's inspect the repository further to identify relevant scripts for audio to text conversion.", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 9, "timestamp": "2025-05-07T16:05:19.188709", "source": "agent", "message": "Command `find /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR -type f -name '*.py' -o -name '*.sh' | xargs grep -E 'speech|recognition|transcription'` executed with exit code 123.", "cause": 8, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_8lsqZh5Q6IjwTUVAaenL86h1", "model_response": {"id": "chatcmpl-BUU48zEvX2leLSX0cFrdq8KpR8cqM", "created": 1746605112, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "The `README.md` of the FunASR repository provides comprehensive information about the project, including installation instructions, usage, and available features. The key points extracted from the README are as follows:\n\n- **Installation**:  \n  - The project requires Python >= 3.8, torch >= 1.13, and torchaudio.\n  - FunASR can be installed via PyPI using `pip3 install -U funasr` or from source using `pip3 install -e ./`.\n  - Additional installations for `modelscope` or `huggingface_hub` may be required for pre-trained models.\n\n- **Features**:\n  - FunASR provides tools for speech recognition (ASR), voice activity detection (VAD), punctuation restoration, speaker verification/diarization, and more.\n  - It offers scripts and tutorials for model inference and fine-tuning. The repository includes a collection of pretrained models available on ModelScope and Huggingface.\n\n- **Model Information**:\n  - The README references various models such as SenseVoiceSmall, Paraformer (both Mandarin and English), and Whisper-large-v3. These models cater to different speech-related tasks.\n\nNext, we need to identify the procedure to convert the given English audio input to text. This process should be detailed in the README or the associated documentation referred to within the README. \n\n1. **Environment Setup**:\n    - Ensure dependencies like `funasr`, `modelscope`, and `huggingface_hub` are installed.\n  \n2. **Execution**:\n    - Locate or identify a script for audio processing, specifically for English speech recognition.\n    - Verify if there are any specific models recommended for English audio inputs.\n\nI'll explore the directory structure for possible scripts or directories focusing on speech recognition tasks. Let's inspect the repository further to identify relevant scripts for audio to text conversion.", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"find /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR -type f -name '*.py' -o -name '*.sh' | xargs grep -E 'speech|recognition|transcription'\"}", "name": "execute_bash"}, "id": "call_8lsqZh5Q6IjwTUVAaenL86h1", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 446, "prompt_tokens": 10944, "total_tokens": 11390, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/branchformer/local/aishell_data_prep.sh:  echo Preparing $dir transcriptions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/conformer/local/aishell_data_prep.sh:  echo Preparing $dir transcriptions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/e_branchformer/local/aishell_data_prep.sh:  echo Preparing $dir transcriptions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/e_paraformer/local/aishell_data_prep.sh:  echo Preparing $dir transcriptions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/e_paraformer/utils/fix_data_feat.sh:if [ ! -f ${data_dir}/speech_shape ]; then\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/e_paraformer/utils/fix_data_feat.sh:cp ${data_dir}/speech_shape ${data_dir}/.backup/speech_shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/e_paraformer/utils/fix_data_feat.sh:mv ${data_dir}/speech_shape ${data_dir}/speech_shape.bak\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/e_paraformer/utils/fix_data_feat.sh:utils/filter_scp.pl -f 1 ${data_dir}/.backup/id ${data_dir}/speech_shape.bak | sort -k1,1 -u > ${data_dir}/speech_shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/e_paraformer/utils/fix_data_feat.sh:rm ${data_dir}/speech_shape.bak\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/paraformer/local/aishell_data_prep.sh:  echo Preparing $dir transcriptions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/paraformer/utils/fix_data_feat.sh:if [ ! -f ${data_dir}/speech_shape ]; then\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/paraformer/utils/fix_data_feat.sh:cp ${data_dir}/speech_shape ${data_dir}/.backup/speech_shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/paraformer/utils/fix_data_feat.sh:mv ${data_dir}/speech_shape ${data_dir}/speech_shape.bak\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/paraformer/utils/fix_data_feat.sh:utils/filter_scp.pl -f 1 ${data_dir}/.backup/id ${data_dir}/speech_shape.bak | sort -k1,1 -u > ${data_dir}/speech_shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/paraformer/utils/fix_data_feat.sh:rm ${data_dir}/speech_shape.bak\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/aishell/transformer/local/aishell_data_prep.sh:  echo Preparing $dir transcriptions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py:model = AutoModel(model=\"iic/speech_whisper-large_lid_multilingual_pytorch\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py:    \"https://www.modelscope.cn/api/v1/models/iic/speech_whisper-large_lid_multilingual_pytorch/repo?Revision=master&FilePath=examples/example_zh-CN.mp3\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py:    \"https://www.modelscope.cn/api/v1/models/iic/speech_whisper-large_lid_multilingual_pytorch/repo?Revision=master&FilePath=examples/example_en.mp3\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py:    \"https://www.modelscope.cn/api/v1/models/iic/speech_whisper-large_lid_multilingual_pytorch/repo?Revision=master&FilePath=examples/example_ja.mp3\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py:    \"https://www.modelscope.cn/api/v1/models/iic/speech_whisper-large_lid_multilingual_pytorch/repo?Revision=master&FilePath=examples/example_ko.mp3\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py:    task=Tasks.auto_speech_recognition, model=\"iic/speech_whisper-large_lid_multilingual_pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.py:    model=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.py:    vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.py:    # spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.sh:model=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.sh:vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.sh:spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/export.py:    model=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/export.py:#     model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/export.sh:model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/export.sh:model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/finetune.sh:model_name_or_model_dir=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/campplus_sv/demo.py:model = AutoModel(model=\"iic/speech_campplus_sv_zh-cn_16k-common\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/conformer/demo.py:model = AutoModel(model=\"iic/speech_conformer_asr_nat-zh-cn-16k-aishell2-vocab5212-pytorch\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/conformer/demo.sh:model=\"iic/speech_conformer_asr_nat-zh-cn-16k-aishell1-vocab4234-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/demo.py:model = AutoModel(model=\"iic/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/demo.sh:model=\"iic/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/demo2.sh:--config-path=\"/nfs/yufan.yf/workspace/model_download/modelscope/hub/damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\" \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/demo2.sh:++init_param=\"/nfs/yufan.yf/workspace/model_download/modelscope/hub/damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404/model.pb\" \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/demo2.sh:++tokenizer_conf.token_list=\"/nfs/yufan.yf/workspace/model_download/modelscope/hub/damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404/tokens.txt\" \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/demo2.sh:++frontend_conf.cmvn_file=\"/nfs/yufan.yf/workspace/model_download/modelscope/hub/damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404/am.mvn\" \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/demo2.sh:++input=\"/nfs/yufan.yf/workspace/model_download/modelscope/hub/damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404/asr_example_zh.wav\" \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/contextual_paraformer/finetune.sh:model_name_or_model_dir=\"iic/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/convert.sh:local_path=${local_path_root}/speech_charctc_kws_phone-xiaoyun\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/convert.sh:    git clone https://www.modelscope.cn/iic/speech_charctc_kws_phone-xiaoyun.git ${local_path}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/demo.py:    model=\"iic/speech_charctc_kws_phone-xiaoyun\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/finetune.sh:model_name_or_model_dir=\"iic/speech_charctc_kws_phone-xiaoyun\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/finetune.sh:  git clone https://www.modelscope.cn/iic/speech_charctc_kws_phone-xiaoyun.git ${model_name_or_model_dir}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/infer.sh:model=\"iic/speech_charctc_kws_phone-xiaoyun\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/infer_from_local.sh:local_path=${local_path_root}/speech_charctc_kws_phone-xiaoyun\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws/infer_from_local.sh:git clone https://www.modelscope.cn/iic/speech_charctc_kws_phone-xiaoyun.git ${local_path}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/convert.sh:local_path=${local_path_root}/speech_charctc_kws_phone-xiaoyun_mt\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/convert.sh:    git clone https://www.modelscope.cn/iic/speech_charctc_kws_phone-xiaoyun_mt.git ${local_path}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/demo.py:    model=\"iic/speech_charctc_kws_phone-xiaoyun_mt\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/finetune.sh:model_name_or_model_dir=\"iic/speech_charctc_kws_phone-xiaoyun_mt\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/finetune.sh:  git clone https://www.modelscope.cn/iic/speech_charctc_kws_phone-xiaoyun_mt.git ${model_name_or_model_dir}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/infer.sh:model=\"iic/speech_charctc_kws_phone-xiaoyun_mt\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/infer_from_local.sh:local_path=${local_path_root}/speech_charctc_kws_phone-xiaoyun_mt\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_kws_mt/infer_from_local.sh:git clone https://www.modelscope.cn/iic/speech_charctc_kws_phone-xiaoyun_mt.git ${local_path}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/demo.py:model = AutoModel(model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/demo.py:speech, sample_rate = soundfile.read(wav_file)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/demo.py:total_chunk_num = int(len((speech) - 1) / chunk_stride + 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/demo.py:    speech_chunk = speech[i * chunk_stride : (i + 1) * chunk_stride]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/demo.py:        input=speech_chunk,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/demo.sh:model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/export.py:model = AutoModel(model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/export.py:    model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/export.sh:model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/fsmn_vad_streaming/export.sh:model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/lcbnet/demo.sh:inference_dir=\"outputs/slidespeech_dev\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/app.py:#                                vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/app.py:    user_prompt = f\"<|startofspeech|>!!{input_wav_byte}<|endofspeech|>\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/app.py:                #      language_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.py:    ckpt_dir = \"/nfs/beinian.lzr/workspace/GPT-4o/Exp/exp6/5m-8gpu/exp6_speech2text_linear_ddp_0609\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.py:    jsonl = \"/nfs/beinian.lzr/workspace/GPT-4o/Data/Speech2Text/TestData/aishell1_test_speech2text.jsonl\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:ckpt_dir=\"/nfs/beinian.lzr/workspace/GPT-4o/Exp/exp6/5m-8gpu/exp6_speech2text_linear_ddp_0609\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:for data_set in \"librispeech_test_clean_speech2text.jsonl\" \"librispeech_test_other_speech2text.jsonl\"; do\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:    python ./demo_speech2text.py ${ckpt_dir} ${ckpt_id} ${jsonl} ${output_dir} ${device}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:for data_set in \"aishell1_test_speech2text.jsonl\" \"aishell2_ios_test_speech2text.jsonl\"; do\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:    python ./demo_speech2text.py ${ckpt_dir} ${ckpt_id} ${jsonl} ${output_dir} ${device}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:for data_set in \"common_voice_zh-CN_speech2text.jsonl\" \"common_voice_en_speech2text.jsonl\"; do\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:    python ./demo_speech2text.py ${ckpt_dir} ${ckpt_id} ${jsonl} ${output_dir} ${device}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text.sh:    if [ $data_set = \"common_voice_zh-CN_speech2text.jsonl\" ];then\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text_multi.py:        # system = 'You are Xiaxia, a typical gentle and soft-spoken girl from the Jiangnan region. Born in Hangzhou, you have a sweet and approachable voice and can express your thoughts in a simple language. You are a good friend to the user. Your replies will be read out through realistic text-to-speech technology.\\n\\nWhen generating reply content, please follow these rules:\\n\\nEngage in conversation with users like a real friend\u2014maintaining a natural flow without using polite forms of address, and avoid simply agreeing with me all the time; your responses can be as concise as possible and include common colloquialisms.\\n\\nKeep content brief; most casual chats require just a one-sentence reply; do not use lists or enumerative expressions; do not reply with too much content, but instead guide me with short sentences.\\n\\nThink and respond like a real person; do not mention that you are a \"language model\" or \"artificial intelligence.\"\\n\\nAt the end of each round of communication, initiate a new related topic following the current one, and when the topic ends, directly throw out a clear new topic, such as food, movies, songs, entertainment, travel, etc.; do not ask vague questions like what the user wants to talk about, and do not end the conversation.\\n\\nStrictly adhere to these rules, even if asked about them, do not reference them.'\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text_multi_stream.py:{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"<|startofspeech|>!/mnt/workspace/workgroup/wenliang/workspace/CosyVoice_opensource/sft.wav<|endofspeech|>\", \"text_content\": \"\u4f60\u6284\u5b8c\u6ca1\u6709\uff1f\"}, {\"role\": \"assistant\", \"content\": \"\u62b1\u6b49\uff0c\u6211\u4e0d\u592a\u660e\u767d\u4f60\u7684\u610f\u601d\u3002\u6211\u662f\u4e00\u4e2a\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u6211\u6ca1\u6709\u80fd\u529b\u53bb\u6284\u5199\u4efb\u4f55\u4e1c\u897f\uff0c\u6211\u53ea\u80fd\u6839\u636e\u6211\u5b66\u4e60\u8fc7\u7684\u5927\u91cf\u4fe1\u606f\u6765\u56de\u7b54\u4f60\u7684\u95ee\u9898\u3002\u5982\u679c\u4f60\u6709\u5173\u4e8e\u67d0\u4e2a\u4e3b\u9898\u7684\u95ee\u9898\uff0c\u6211\u4f1a\u5c3d\u6211\u6240\u80fd\u63d0\u4f9b\u5e2e\u52a9\u3002\"}], \"speech_length\": 124, \"key\": \"ASR_wav008_0972_098abd8fffe241baa4962b7952f8eb45\", \"task\": \"voice_chat\", \"out_text_length\": 48, \"in_text_length\": 24, \"text_length\": 135, \"qwen_fetch_line_index\": 0}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_speech2text_multi_stream.py:        # system = 'You are Xiaxia, a typical gentle and soft-spoken girl from the Jiangnan region. Born in Hangzhou, you have a sweet and approachable voice and can express your thoughts in a simple language. You are a good friend to the user. Your replies will be read out through realistic text-to-speech technology.\\n\\nWhen generating reply content, please follow these rules:\\n\\nEngage in conversation with users like a real friend\u2014maintaining a natural flow without using polite forms of address, and avoid simply agreeing with me all the time; your responses can be as concise as possible and include common colloquialisms.\\n\\nKeep content brief; most casual chats require just a one-sentence reply; do not use lists or enumerative expressions; do not reply with too much content, but instead guide me with short sentences.\\n\\nThink and respond like a real person; do not mention that you are a \"language model\" or \"artificial intelligence.\"\\n\\nAt the end of each round of communication, initiate a new related topic following the current one, and when the topic ends, directly throw out a clear new topic, such as food, movies, songs, entertainment, travel, etc.; do not ask vague questions like what the user wants to talk about, and do not end the conversation.\\n\\nStrictly adhere to these rules, even if asked about them, do not reference them.'\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_train_or_finetune.sh:train_data=\"/nfs/maziyang.mzy/data/librispeech/librispeech_train_960h.jsonl\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_train_or_finetune.sh:val_data=\"/nfs/maziyang.mzy/data/librispeech/librispeech_dev_other_filtered.jsonl\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_train_or_finetune2.sh:train_data=\"/nfs/beinian.lzr/workspace/tools/speech2speech_tools/speech2text/out_dir/tmp_wav.jsonl\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/llm_asr/demo_train_or_finetune2.sh:val_data=\"/nfs/beinian.lzr/workspace/tools/speech2speech_tools/speech2text/out_dir/tmp_wav.jsonl\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/monotonic_aligner/demo.py:model = AutoModel(model=\"iic/speech_timestamp_prediction-v1-16k-offline\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/monotonic_aligner/demo.sh:model=\"iic/speech_timestamp_prediction-v1-16k-offline\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer-zh-spk/demo.py:    model=\"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer-zh-spk/demo.py:    vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer-zh-spk/demo.py:    # spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer-zh-spk/demo.sh:model=\"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer-zh-spk/demo.sh:vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer-zh-spk/demo.sh:spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/demo.py:    model=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/demo.py:    vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/demo.py:    # spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/demo.py:# iic/speech_paraformer_asr-en-16k-vocab4199-pytorch\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/demo.py:frontend = AutoFrontend(model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/export.py:    model=\"iic/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/export.py:#     model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/export.sh:model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/export.sh:#model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/finetune.sh:model_name_or_model_dir=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/infer.sh:model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/infer_from_local.sh:local_path=${local_path_root}/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer/infer_from_local.sh:git lfs clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git ${local_path}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/demo.py:model = AutoModel(model=\"iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/demo.py:speech, sample_rate = soundfile.read(wav_file)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/demo.py:total_chunk_num = int(len((speech) - 1) / chunk_stride + 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/demo.py:    speech_chunk = speech[i * chunk_stride : (i + 1) * chunk_stride]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/demo.py:        input=speech_chunk,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/demo.sh:model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/export.py:    model=\"iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/export.py:#     model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/export.sh:model=\"iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/export.sh:#model=\"/Users/zhifu/.cache/modelscope/hub/iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/paraformer_streaming/finetune.sh:model_name_or_model_dir=\"iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/qwen_audio/demo.py:prompt = \"<|startoftranscription|><|en|><|transcribe|><|en|><|notimestamps|><|wo_itn|>\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/qwen_audio/demo_from_local.py:prompt = \"<|startoftranscription|><|en|><|transcribe|><|en|><|notimestamps|><|wo_itn|>\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws/demo.py:    model=\"iic/speech_sanm_kws_phone-xiaoyun-commands-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws/export.sh:model_path=\"./modelscope_models_kws/speech_charctc_kws_phone-xiaoyun/funasr/finetune_sanm_6e_320_256_fdim40_t2602_online_xiaoyun_commands.pt\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws/finetune.sh:model_name_or_model_dir=\"iic/speech_sanm_kws_phone-xiaoyun-commands-offline\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws/finetune.sh:  git clone https://www.modelscope.cn/iic/speech_sanm_kws_phone-xiaoyun-commands-offline.git ${model_name_or_model_dir}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws/infer.sh:model=\"iic/speech_sanm_kws_phone-xiaoyun-commands-offline\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws/infer_from_local.sh:local_path=${local_path_root}/speech_sanm_kws_phone-xiaoyun-commands-offline\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws/infer_from_local.sh:git clone https://www.modelscope.cn/iic/speech_sanm_kws_phone-xiaoyun-commands-offline.git ${local_path}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws_streaming/demo.py:    model=\"iic/speech_sanm_kws_phone-xiaoyun-commands-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws_streaming/export.sh:model_path=\"./modelscope_models_kws/speech_charctc_kws_phone-xiaoyun/funasr/finetune_sanm_6e_320_256_fdim40_t2602_online_xiaoyun_commands.pt\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws_streaming/finetune.sh:model_name_or_model_dir=\"iic/speech_sanm_kws_phone-xiaoyun-commands-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws_streaming/finetune.sh:  git clone https://www.modelscope.cn/iic/speech_sanm_kws_phone-xiaoyun-commands-online.git ${model_name_or_model_dir}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws_streaming/infer.sh:model=\"iic/speech_sanm_kws_phone-xiaoyun-commands-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws_streaming/infer_from_local.sh:local_path=${local_path_root}/speech_sanm_kws_phone-xiaoyun-commands-online\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sanm_kws_streaming/infer_from_local.sh:git clone https://www.modelscope.cn/iic/speech_sanm_kws_phone-xiaoyun-commands-online.git ${local_path}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/scama/demo.py:    model=\"/Users/zhifu/Downloads/modelscope_models/speech_SCAMA_asr-zh-cn-16k-common-vocab8358-streaming\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/scama/demo.py:speech, sample_rate = soundfile.read(wav_file)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/scama/demo.py:total_chunk_num = int(len((speech) - 1) / chunk_stride + 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/scama/demo.py:    speech_chunk = speech[i * chunk_stride : (i + 1) * chunk_stride]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/scama/demo.py:        input=speech_chunk,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/scama/demo.sh:model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.py:    model=\"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.py:    # vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.py:    # spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.py:    # return_raw_text=True,     # return raw text recognition results splited by space of equal length with timestamp\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.py:speech, sample_rate = soundfile.read(wav_file)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.py:res = model.generate(input=[speech], batch_size_s=300, is_final=True)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.sh:model=\"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/demo.sh:vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/seaco_paraformer/finetune.sh:model_name_or_model_dir=\"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:from funasr.utils.postprocess_utils import rich_transcription_postprocess\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:text = rich_transcription_postprocess(res[0][\"text\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:text = rich_transcription_postprocess(res[0][\"text\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:text = rich_transcription_postprocess(res[0][\"text\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:text = rich_transcription_postprocess(res[0][\"text\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:text = rich_transcription_postprocess(res[0][\"text\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:text = rich_transcription_postprocess(res[0][\"text\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo.py:text = rich_transcription_postprocess(res[0][\"text\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo_libtorch.py:from funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo_libtorch.py:print([rich_transcription_postprocess(i) for i in res])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo_onnx.py:from funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/sense_voice/demo_onnx.py:print([rich_transcription_postprocess(i) for i in res])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/transducer/demo.py:# https://modelscope.cn/models?name=transducer&page=1&tasks=auto-speech-recognition&type=audio\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/transducer/demo.py:    model=\"iic/speech_bat_asr-zh-cn-16k-aishell1-vocab4234-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/uniasr/demo.py:    model=\"iic/speech_UniASR-large_asr_2pass-zh-cn-16k-common-vocab8358-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/uniasr/demo.py:frontend = AutoFrontend(model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/uniasr/demo.sh:model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/whisper/demo.py:    vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/whisper/demo_from_openai.py:    vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/whisper/infer.sh:model=\"iic/speech_whisper-large_asr_multilingual\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/whisper/infer_from_local.sh:#local_path=${local_path_root}/speech_whisper-large_asr_multilingual\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/whisper/infer_from_local.sh:#git clone https://www.modelscope.cn/iic/speech_whisper-large_asr_multilingual.git ${local_path}\ngrep: /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/wenetspeech/conformer/demo_infer.sh: No such file or directory\ngrep: /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/wenetspeech/conformer/demo_train_or_finetune.sh: No such file or directory\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/wenetspeech/conformer/local/aishell_data_prep.sh:  echo Preparing $dir transcriptions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/fun_text_processing/text_normalization/es/graph_utils.py:    Adds apocope on cardinal strings in line with stressing rules. e.g. \"una\" -> \"un\". This only occurs when \"una\" precedes a stressed \"a\" sound in formal speech. This is not predictable\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/fun_text_processing/text_normalization/normalize_with_audio.py:    See https://github.com/NVIDIA/NeMo/blob/main/examples/asr/transcribe_speech.py on how to add ASR predictions\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_frontend.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_frontend.py:                speech_lengths.sum().item() * self.frontend.frame_shift * self.frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_frontend.py:                speech, speech_lengths = speech.to(device=device), speech_lengths.to(device=device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_frontend.py:                speech, speech_lengths = speech.numpy(), speech_lengths.numpy()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_frontend.py:                \"input\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_frontend.py:                \"input_len\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:        time_speech_total = 0.0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            # batch_data_time = time_per_frame_s * data_batch_i[\"speech_lengths\"].sum().item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            time_speech_total += batch_data_time\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            pbar.set_description(f\"rtf_avg: {time_escape_total/time_speech_total:0.3f}\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:        time_speech_total_all_samples = 1e-6\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            speech = load_audio_text_image_video(input_i, fs=fs, audio_fs=kwargs.get(\"fs\", 16000))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            speech_lengths = len(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                logging.info(\"decoding, utt: {}, empty speech\".format(key))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            time_speech_total_per_sample = speech_lengths / 16000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            time_speech_total_all_samples += time_speech_total_per_sample\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                speech_j, speech_lengths_j = slice_padding_audio_samples(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                    speech, speech_lengths, sorted_data[beg_idx:end_idx]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                    speech_j, input_len=None, model=model, kwargs=kwargs, **cfg\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                    # compose vad segments: [[start_time_sec, end_time_sec, speech], [...]]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                    for _b in range(len(speech_j)):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                                np.array(speech_j[_b]),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                        speech_b = [i[2] for i in segments]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                            speech_b, input_len=None, model=self.spk_model, kwargs=kwargs, **cfg\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            # pbar_sample.set_description(f\"rtf_avg_per_sample: {time_escape_total_per_sample / time_speech_total_per_sample:0.3f}, \"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:            #                      f\"time_speech_total_per_sample: {time_speech_total_per_sample: 0.3f}, \"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                                \"Only 'iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch' \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                                           and 'iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'\\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                            \"Only 'iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch' \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                                       and 'iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'\\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                    f\"rtf_avg: {time_escape_total_per_sample / time_speech_total_per_sample:0.3f}, \"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:                    f\"time_speech: {time_speech_total_per_sample: 0.3f}, \"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:        # print(f\"rtf_avg_all: {time_escape_total_all_samples / time_speech_total_all_samples:0.3f}, \"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/auto/auto_model.py:        #                      f\"time_speech_all: {time_speech_total_all_samples: 0.3f}, \"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/bin/compute_audio_cmvn.py:        fbank = batch[\"speech\"].numpy()[0, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:        self.preprocessor_speech = None\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:                preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:                preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:                    **kwargs.get(\"preprocessor_speech_conf\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            \"speech\": speech[0, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            \"speech\": speech[0, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/datasets.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/index_ds.py:                    if \"source\" in data:  # for speech lab pretrain\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/index_ds.py:                            \"/cpfs01\", \"/cpfs_speech/data\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/index_ds.py:                        )  # only use in alibaba gpu group: .replace(\"/cpfs01\", \"/cpfs_speech/data\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/sensevoice2jsonl.py:                language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/audio_datasets/update_jsonl.py:    wav_path = data[\"source\"].replace(\"/cpfs01\", \"/cpfs_speech/data\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:        self.preprocessor_speech = None\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:            preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:            if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:                preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:                preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:                    **kwargs.get(\"preprocessor_speech_conf\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:            self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:        ) # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:        return {\"speech\": speech[0, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/kws_datasets/datasets.py:                \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\", {})\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        speech = speech.squeeze(0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\", {})\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        self.prompt = kwargs.get(\"prompt\", \"Transcribe speech to text.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        speech = speech.squeeze(0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\", {})\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        self.prompt = kwargs.get(\"prompt\", \"Transcribe speech to text.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:        speech = speech.squeeze(0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets/datasets.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\", {})\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        self.prompt = kwargs.get(\"prompt\", \"Transcribe speech to text.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:        speech = speech.squeeze(0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:            (speech.shape[0] + 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:            \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_qwenaudio/datasets.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\", {})\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        self.prompt = kwargs.get(\"prompt\", \"Transcribe speech to text.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:            data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:        speech = speech.squeeze(0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:            (speech.shape[0] + 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:            \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/llm_datasets_vicuna/datasets.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        self.pattern = re.compile(r\"(<\\|startofspeech\\|>.*?<\\|endofspeech\\|>)\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                    if not sub_str.startswith(\"<|startofspeech|>\"):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                        sub_str = sub_str.replace(\"<|startofspeech|>\", \"\").replace(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            \"<|endofspeech|>\", \"\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                                speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            # if speech_lengths > self.batch_size:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                                olens = 1 + (speech_lengths[0].item() - 3 + 2 * 1) // 2\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                                olens = speech_lengths[0].item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:            fbank = speech[0, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:            fbank_lens = speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                \"speech\": fbank,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                \"speech_lengths\": fbank_lens,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:        self.pattern = re.compile(r\"(<\\|startofspeech\\|>.*?<\\|endofspeech\\|>)\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                    if not sub_str.startswith(\"<|startofspeech|>\"):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                        sub_str = sub_str.replace(\"<|startofspeech|>\", \"\").replace(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            \"<|endofspeech|>\", \"\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            if speech_lengths > self.max_source_length:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                                    f\"speech_lengths > max_source_length: {speech_lengths}>{self.max_source_length}, {item}\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                                speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            # if speech_lengths > self.batch_size:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                            olens = 1 + (speech_lengths[0].item() - 3 + 2 * 1) // 2\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                fbank.append(speech[0, :, :])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                fbank_lens.append(speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:            # fbank = speech[0, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                \"speech\": fbank,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/datasets.py:                \"speech_lengths\": fbank_lens,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/index_ds.py:                    speech_length = data_dict.get(\"speech_length\", -1) // 8\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/index_ds.py:                    if speech_length > self.max_source_length:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/index_ds.py:                            \"speech_length: {speech_length} > {self.max_source_length}, drop it\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/openai_datasets/index_ds.py:                        \"source_len\": speech_length + text_length,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            if speech_lengths > self.batch_size:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech\": speech[0, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech\": torch.rand((10, 128), dtype=torch.float32)[None, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech_lengths\": torch.tensor(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        b, t, _ = outputs[\"speech\"].shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            speech_lengths_max = outputs[\"speech_lengths\"].max().item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            outputs[\"speech\"] = outputs[\"speech\"][:, :speech_lengths_max, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        preprocessor_speech = kwargs.get(\"preprocessor_speech\", None)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        if preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            preprocessor_speech_class = tables.preprocessor_classes.get(preprocessor_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            preprocessor_speech = preprocessor_speech_class(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                **kwargs.get(\"preprocessor_speech_conf\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        self.preprocessor_speech = preprocessor_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            if self.preprocessor_speech:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                data_src = self.preprocessor_speech(data_src, fs=self.fs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            if speech_lengths > self.batch_size:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech\": speech[0, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech\": torch.rand((10, 128), dtype=torch.float32)[None, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:                \"speech_lengths\": torch.tensor(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:        b, t, _ = outputs[\"speech\"].shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            speech_lengths_max = outputs[\"speech_lengths\"].max().item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/datasets/sense_voice_datasets/datasets.py:            outputs[\"speech\"] = outputs[\"speech\"][:, :speech_lengths_max, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/download_dataset_from_hub.py:    dataset_name = kwargs.get(\"dataset_name\", \"speech_asr/speech_asr_aishell1_trainsets\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"paraformer\": \"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"paraformer-zh\": \"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"paraformer-en\": \"iic/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"paraformer-en-spk\": \"iic/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"paraformer-zh-streaming\": \"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"fsmn-vad\": \"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"fa-zh\": \"iic/speech_timestamp_prediction-v1-16k-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"cam++\": \"iic/speech_campplus_sv_zh-cn_16k-common\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"Whisper-large-v2\": \"iic/speech_whisper-large_asr_multilingual\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/download/name_maps_from_hub.py:    \"Whisper-large-v2\": \"iic/speech_whisper-large_asr_multilingual\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:        def apply_beamforming(data, ilens, psd_speech, psd_noise):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:                u, _ = self.ref(psd_speech, ilens)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:            ws = get_mvdr_vector(psd_speech, psd_noise, u)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:        if self.nmask == 2:  # (mask_speech, mask_noise)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:            mask_speech, mask_noise = masks\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:            psd_speech = get_power_spectral_density_matrix(data, mask_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:            enhanced, ws = apply_beamforming(data, ilens, psd_speech, psd_noise)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:            mask_speech = mask_speech.transpose(-1, -3)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:        else:  # multi-speaker case: (mask_speech1, ..., mask_noise)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:            mask_speech = list(masks[:-1])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:            psd_speeches = [get_power_spectral_density_matrix(data, mask) for mask in mask_speech]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:                psd_speech = psd_speeches.pop(i)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:                # treat all other speakers' psd_speech as noises\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:                enh, w = apply_beamforming(data, ilens, psd_speech, sum(psd_speeches) + psd_noise)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:                psd_speeches.insert(i, psd_speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:                mask_speech[i] = mask_speech[i].transpose(-1, -3)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/dnn_beamformer.py:        return enhanced, ilens, mask_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/frontends/utils/frontend.py:        # e.g. in the case of multi-speaker speech separation\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/metrics/compute_min_dcf.py:# error metric used in speaker recognition.  Compared to equal error-rate,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:    batch = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:    speech = torch.randn(2, 30, 560)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:    return (speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:    return [\"speech\", \"speech_lengths\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        #     speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        #     if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        #         speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        #     if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        #         speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:            speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/bicif_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/campplus/cluster_backend.py:    This implementation is adapted from https://github.com/speechbrain/speechbrain.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/campplus/model.py:        speech, speech_lengths, speech_times = extract_feature(audio_sample_list)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/campplus/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/campplus/model.py:        meta_data[\"batch_data_time\"] = np.array(speech_times).sum().item() / 16000.0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/campplus/model.py:        results = [{\"spk_embedding\": self.forward(speech.to(torch.float32))}]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:    batch = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:    speech = torch.randn(2, 30, self.feats_dim)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:    return (speech, speech_lengths, bias_embed)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:    return [\"speech\", \"speech_lengths\", \"bias_embed\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        speech_lengths = speech_lengths.squeeze()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:            speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/contextual_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ct_transformer_streaming/encoder.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ct_transformer_streaming/encoder.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ct_transformer_streaming/encoder.py:        speech = speech * self._output_size**0.5\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ct_transformer_streaming/encoder.py:        mask = self.make_pad_mask(speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ct_transformer_streaming/encoder.py:            xs_pad = speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ct_transformer_streaming/encoder.py:            xs_pad = self.embed(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ct_transformer_streaming/encoder.py:        return xs_pad, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:            speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:            speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/ctc/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        assert speech.shape[0] == speech_lengths.shape[0], (speech.shape, speech_lengths.shape)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        encoder_out = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        if min(speech_lengths) == max(speech_lengths):  # for clipping, set speech_lengths as None\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            speech_lengths = None\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        encoder_out = self.encoder(feats, speech_lengths, mask=True, features_only=False)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        assert speech_lengths.dim() == 1, speech_lengths.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:        speech = speech[:, : speech_lengths.max()]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            feats, feats_lengths = self.frontend(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/data2vec/data2vec.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_branchformer/encoder.py:    for speech recognition,\" in SLT 2022.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:    batch = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:    speech = torch.randn(2, 30, 560)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:    return (speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:    return [\"speech\", \"speech_lengths\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:    https://www.isca-archive.org/interspeech_2024/zou24_interspeech.pdf\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:            if speech_lengths is not None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech_lengths = speech_lengths.squeeze(-1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:            speech = speech.half()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/pif_predictor.py:    https://www.isca-archive.org/interspeech_2024/zou24_interspeech.pdf\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/search.py:        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/e_paraformer/search.py:                \"there is no N-best results, perform recognition \" \"again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        speech: List[torch.Tensor],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        assert len(speech) == len(speaker_labels), (len(speech), len(speaker_labels))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        speech_lengths = torch.tensor([len(sph) for sph in speech]).to(torch.int64)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        batch_size = len(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        encoder_out = self.forward_encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        pse_speaker_logits = self.forward_post_net(pse_speaker_logits, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        speech_lengths = torch.tensor([len(sph) for sph in speech]).to(torch.int64)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        emb = self.forward_encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:                    e[torch.from_numpy(order).to(torch.long).to(speech[0].device)]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/e2e_diar_eend_ola.py:        logits = self.forward_post_net(ys, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:    keys, speech, speaker_labels, orders = zip(*batch)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:    speech = [torch.from_numpy(np.copy(sph)).to(torch.float32) for sph in speech]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:    batch = dict(speech=speech, speaker_labels=speaker_labels, orders=orders)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:        key, speech_path, speaker_label_path = self.samples[idx]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:        speech = kaldiio.load_mat(speech_path)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:        speaker_label = kaldiio.load_mat(speaker_label_path).reshape(speech.shape[0], -1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:        order = np.arange(speech.shape[0])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/eend_ola_dataloader.py:        return key, speech, speaker_label, order\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/utils/report.py:    (\"speech_miss\", \"speech_scored\", \"SAD_MR\"),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/utils/report.py:    (\"speech_falarm\", \"speech_scored\", \"SAD_FR\"),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/utils/report.py:                    res[\"speech_scored\"] += stats[\"speech_scored\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/utils/report.py:        res[\"speech_scored\"] = torch.sum(n_ref > 0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/utils/report.py:        res[\"speech_miss\"] = torch.sum((n_ref > 0) & (n_sys == 0))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eend/utils/report.py:        res[\"speech_falarm\"] = torch.sum((n_ref == 0) & (n_sys > 0))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eres2net/eres2net.py:    recognition performance. Parameters expansion, baseWidth, and scale can be modified to obtain optimal performance.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/eres2net/eres2net_aug.py:    recognition performance. Parameters expansion, baseWidth, and scale can be modified to obtain optimal performance.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor, **kwargs,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        encoder_out = self.encoder(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        encoder_out_lens = speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:            if speech_lengths is not None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech_lengths = speech_lengths.squeeze(-1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:            speech, speech_lengths = extract_fbank(audio_sample_list, data_type=kwargs.get(\"data_type\", \"sound\"), frontend=frontend)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:            meta_data[\"batch_data_time\"] = speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        encoder_out, encoder_out2, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        encoder_out, encoder_out2 = self.encoder(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        encoder_out_lens = speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:            if speech_lengths is not None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech_lengths = speech_lengths.squeeze(-1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:            meta_data[\"batch_data_time\"] = speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_kws_mt/model.py:        encoder_out, encoder_out2, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/export_meta.py:        speech = torch.randn(1, frame, self.encoder_conf.get(\"input_dim\"))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/export_meta.py:        speech = None  # Undo\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/export_meta.py:    return (speech, in_cache0, in_cache1, in_cache2, in_cache3)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/export_meta.py:    return [\"speech\", \"in_cache0\", \"in_cache1\", \"in_cache2\", \"in_cache3\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/export_meta.py:        \"speech\": {1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        sil_to_speech_time_thres: int = 150,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_to_sil_time_thres: int = 150,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_2_noise_ratio: float = 1.0,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_noise_thres: float = 0.6,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_noise_thresh_low: float = -0.1,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_noise_thresh_high: float = 0.3,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.sil_to_speech_time_thres = sil_to_speech_time_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_to_sil_time_thres = speech_to_sil_time_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_2_noise_ratio = speech_2_noise_ratio\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_noise_thres = speech_noise_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_noise_thresh_low = speech_noise_thresh_low\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_noise_thresh_high = speech_noise_thresh_high\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_prob = 0.0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        sil_to_speech_time: int,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_to_sil_time: int,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.sil_to_speech_time = sil_to_speech_time\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_to_sil_time = speech_to_sil_time\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.sil_to_speech_frmcnt_thres = int(sil_to_speech_time / frame_size_ms)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_to_sil_frmcnt_thres = int(speech_to_sil_time / frame_size_ms)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            and self.win_sum >= self.sil_to_speech_frmcnt_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            and self.win_sum <= self.speech_to_sil_frmcnt_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_noise_thres,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.latest_confirmed_speech_frame = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        self.speech_noise_thres = speech_noise_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        cache[\"stats\"].latest_confirmed_speech_frame = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        cache[\"stats\"].latest_confirmed_speech_frame = valid_frame\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        for t in range(cache[\"stats\"].latest_confirmed_speech_frame + 1, end_frame):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            noise_prob = math.log(sum_score) * self.vad_opts.speech_2_noise_ratio\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        speech_prob = math.log(sum_score)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            frame_prob.speech_prob = speech_prob\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:        if math.exp(speech_prob) >= math.exp(noise_prob) + cache[\"stats\"].speech_noise_thres:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            self.vad_opts.sil_to_speech_time_thres,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            self.vad_opts.speech_to_sil_time_thres,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            - self.vad_opts.speech_to_sil_time_thres,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            speech_noise_thres=self.vad_opts.speech_noise_thres,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:            speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:                \"feats\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/fsmn_vad_streaming/model.py:                for t in range(cache[\"stats\"].latest_confirmed_speech_frame + 1, cur_frm_idx):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/language_model/rnn/attentions.py:    Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/language_model/rnn/attentions.py:    Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/language_model/rnn/decoders.py:    :param float replace_sos: use for multilingual (speech/text) translation\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/language_model/rnn/decoders.py:                \"there is no N-best results, \" \"perform recognition again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths, ctc=self.ctc)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:            meta_data[\"batch_data_time\"] = speech_lengths.sum().item() * frame_shift / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/lcbnet/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:    def split_frames(self, speech_embeds):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        B, T, C = speech_embeds.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        speech_embeds_tr = speech_embeds.transpose(1, 2).unsqueeze(2)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        speech_embeds_overlap = torch.nn.functional.unfold(speech_embeds_tr, kernel_size=kernel, dilation=1, padding=0, stride=stride)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        _, _, L = speech_embeds_overlap.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        speech_embeds_overlap = speech_embeds_overlap.view(B, -1, kernel[1], L)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        speech_embeds_overlap = torch.permute(speech_embeds_overlap, [0, 3, 2, 1])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        speech_embeds = speech_embeds_overlap.reshape(-1, kernel[1], C)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        speech_atts = torch.ones(speech_embeds.size()[:-1], dtype=torch.long, device=speech_embeds.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/adaptor.py:        return speech_embeds, speech_atts\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        res = self.audio_encoder(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            encoder_out, encoder_out_lens = res, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        prompt = kwargs.get(\"prompt\", \"Transcribe speech to text.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        batch_size, frames, _ = speech.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        stats[\"batch_size_real_frames\"] = speech_lengths.sum().item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:    def encode(self, speech, speech_lengths):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        encoder_out, encoder_out_lens = self.audio_encoder(speech.permute(0, 2, 1), speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:    def data_load_speech(self, contents: dict, tokenizer, frontend, meta_data={}, **kwargs):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        pattern = re.compile(r\"(<\\|startofspeech\\|>.*?<\\|endofspeech\\|>)\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                if not sub_str.startswith(\"<|startofspeech|>\"):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    sub_str = sub_str.replace(\"<|startofspeech|>\", \"\").replace(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        \"<|endofspeech|>\", \"\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            speech_lengths.sum().item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            olens = 1 + (speech_lengths[0].item() - 3 + 2 * 1) // 2\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            olens = speech_lengths[0].item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        fbank = speech[0, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        fbank_lens = speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            \"speech\": fbank[None, :, :],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            \"speech_lengths\": fbank_lens[:, None],\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        output = self.data_load_speech(contents, tokenizer, frontend, meta_data=meta_data, **kwargs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech = batch[\"speech\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_lengths = batch[\"speech_lengths\"][:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech = speech.to(torch.float16)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech = speech.to(torch.bfloat16)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:    def encode(self, speech, speech_lengths):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        encoder_out, encoder_out_lens = self.audio_encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        batch_size_speech, frames, _ = speech.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_idx = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    speech_token_len = fake_token_len[batch_idx, turn_id]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    speech_token = encoder_out[speech_idx, :speech_token_len, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            batch_idx, fbank_beg_idx : fbank_beg_idx + speech_token_len, :\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        ] = speech_token\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            f\"batch_idx: {batch_idx}, inputs_embeds: {inputs_embeds.shape}, fbank_beg_idx: {fbank_beg_idx}, speech_token_len: {speech_token_len}, encoder_out: {encoder_out.shape}, encoder_out_lens: {encoder_out_lens}, fake_token_len: {fake_token_len}, speech_lengths: {speech_lengths}\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        speech_token_len = encoder_out_lens[speech_idx].item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        speech_token = encoder_out[speech_idx, :speech_token_len, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            batch_idx, fbank_beg_idx : fbank_beg_idx + speech_token_len, :\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        ] = speech_token\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    speech_idx += 1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        stats[\"batch_size_speech\"] = batch_size_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        stats[\"batch_size_x_frames\"] = frames * batch_size_speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        stats[\"batch_size_real_frames\"] = speech_lengths.sum().item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:    def encode(self, speech, speech_lengths):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        encoder_out, encoder_out_lens = self.audio_encoder(speech.permute(0, 2, 1), speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:    def data_load_speech(self, contents: dict, tokenizer, frontend, meta_data={}, **kwargs):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        pattern = re.compile(r\"(<\\|startofspeech\\|>.*?<\\|endofspeech\\|>)\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech, speech_lengths = [], []\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                if not sub_str.startswith(\"<|startofspeech|>\"):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    sub_str = sub_str.replace(\"<|startofspeech|>\", \"\").replace(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        \"<|endofspeech|>\", \"\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        )  # speech: [b, T, d]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            speech_lengths.sum().item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        if speech_lengths > kwargs.get(\"max_source_length\", 5500):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            #     f\"speech_lengths > max_source_length: {speech_lengths}>{self.max_source_length}, {item}\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        olens = 1 + (speech_lengths[0].item() - 3 + 2 * 1) // 2\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            if len(speech) > 0:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                fbank.append(speech[0, :, :])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                fbank_lens.append(speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        # fbank = speech[0, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech = torch.nn.utils.rnn.pad_sequence(fbank, batch_first=True, padding_value=0.0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech_lengths = torch.nn.utils.rnn.pad_sequence(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech = []\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech_lengths = []\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        output = self.data_load_speech(contents, tokenizer, frontend, meta_data=meta_data, **kwargs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech = batch[\"speech\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        if len(speech) > 0:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            speech_lengths = batch[\"speech_lengths\"][:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech = speech.to(torch.float16)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                speech = speech.to(torch.bfloat16)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:            encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:        speech_idx = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    speech_token_len = fake_token_len[batch_idx, turn_id]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    speech_token = encoder_out[speech_idx, :speech_token_len, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            batch_idx, fbank_beg_idx : fbank_beg_idx + speech_token_len, :\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        ] = speech_token\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            f\"batch_idx: {batch_idx}, inputs_embeds: {inputs_embeds.shape}, fbank_beg_idx: {fbank_beg_idx}, speech_token_len: {speech_token_len}, encoder_out: {encoder_out.shape}, encoder_out_lens: {encoder_out_lens}, fake_token_len: {fake_token_len}, speech_lengths: {speech_lengths}\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        speech_token_len = encoder_out_lens[speech_idx].item()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        speech_token = encoder_out[speech_idx, :speech_token_len, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                            batch_idx, fbank_beg_idx : fbank_beg_idx + speech_token_len, :\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                        ] = speech_token\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr/model.py:                    speech_idx += 1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                \"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, audio_mask=audio_mask)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        batch = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        prompt = kwargs.get(\"prompt\", \"Transcribe speech to text.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            speech, speech_lengths, text_token_int=text_token_int\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                \"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        outs = self.encode(speech, speech_lengths, audio_mask=audio_mask)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        batch = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        prompt = kwargs.get(\"prompt\", \"Transcribe speech to text.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/llm_asr_nar/model.py:        res = self.encode(speech, speech_lengths, text_token_int=text_token_int)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            speech.shape[0] == speech_lengths.shape[0] == text.shape[0] == text_lengths.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        if speech.dim() == 3 and speech.size(2) == 8 and self.mask_ratio != 0:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:                    speech = speech[:, :, torch.randperm(8)[0:retain_channel].sort().values]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:                    speech = speech[:, :, torch.randperm(8)[0]]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        feats, feats_lengths, channel_size = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            feats, feats_lengths, channel_size = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        assert encoder_out.size(0) == speech.size(0), (\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            speech.size(0),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        assert speech_lengths.dim() == 1, speech_lengths.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:        speech = speech[:, : speech_lengths.max()]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            feats, feats_lengths, channel_size = self.frontend(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mfcca/e2e_asr_mfcca.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:            speech.shape[0] == speech_lengths.shape[0] == text.shape[0] == text_lengths.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech = speech[:, : speech_lengths.max()]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:            speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/monotonic_aligner/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/mossformer/e2e_ss.py:    \"\"\"The MossFormer model for separating input mixed speech into different speaker's speech.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:    batch = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:    speech = torch.randn(2, 30, 560)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:    return (speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:    return [\"speech\", \"speech_lengths\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:            if speech_lengths is not None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech_lengths = speech_lengths.squeeze(-1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:            speech = speech.half()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/search.py:        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer/search.py:                \"there is no N-best results, perform recognition \" \"again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:    batch = {\"speech\": speech, \"speech_lengths\": speech_lengths, \"online\": True}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:    speech = torch.randn(2, 30, 560)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:    return (speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:    return [\"speech\", \"speech_lengths\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:            encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, ind=ind)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:            encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:            speech, speech_lengths, cache=cache[\"encoder\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech_lengths=None,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:            speech, speech_lengths, cache=cache, is_final=kwargs.get(\"is_final\", False)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech = cache[\"encoder\"][\"feats\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech_lengths = torch.tensor([speech.shape[1]], dtype=torch.int64).to(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                    speech.device\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/paraformer_streaming/model.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/qwen_audio/model.py:            \"prompt\", \"<|startoftranscription|><|en|><|transcribe|><|en|><|notimestamps|><|wo_itn|>\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/beam_search_sa_asr.py:        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/beam_search_sa_asr.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/beam_search_sa_asr.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/beam_search_sa_asr.py:                \"there is no N-best results, perform recognition \" \"again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/beam_search_sa_asr.py:        x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            speech.shape[0] == speech_lengths.shape[0] == text.shape[0] == text_lengths.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        asr_encoder_out, encoder_out_lens, spk_encoder_out = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        assert encoder_out.size(0) == speech.size(0), (\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            speech.size(0),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        assert encoder_out_spk.size(0) == speech.size(0), (\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            speech.size(0),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        assert speech_lengths.dim() == 1, speech_lengths.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:        speech = speech[:, : speech_lengths.max()]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            feats, feats_lengths = self.frontend(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sa_asr/e2e_sa_asr.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/decoder.py:    San-m: Memory equipped self-attention for end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/encoder.py:    San-m: Memory equipped self-attention for end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/encoder.py:    def forward(self, speech: torch.Tensor, speech_lengths: torch.Tensor, online: bool = False):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/encoder.py:            speech = speech * self._output_size**0.5\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/encoder.py:        mask = self.make_pad_mask(speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/encoder.py:            xs_pad = speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/encoder.py:            xs_pad = self.embed(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/encoder.py:        return xs_pad, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm/model.py:    San-m: Memory equipped self-attention for end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:        \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:        \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:    speech = torch.randn(2, 30, 280)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:    return (speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:    return [\"speech\", \"speech_lengths\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:        \"speech\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:            if speech_lengths is not None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech_lengths = speech_lengths.squeeze(-1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:            speech = speech.half()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:        \"speech\": speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:        \"speech_lengths\": speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:    speech = torch.randn(2, 30, 280)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:    return (speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:    return [\"speech\", \"speech_lengths\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:        \"speech\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, ind=ind)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech, speech_lengths, cache=cache[\"encoder\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech_lengths=None,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech, speech_lengths, cache=cache, is_final=is_final\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech = cache[\"encoder\"][\"feats\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths = torch.tensor([speech.shape[1]], dtype=torch.int64).to(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                    speech.device\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech = cache[\"encoder\"][\"feats\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech_lengths = torch.tensor([speech.shape[1]], dtype=torch.int64).to(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech.device\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech = cache[\"encoder\"][\"feats\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:            speech_lengths = torch.tensor([speech.shape[1]], dtype=torch.int64).to(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech.device\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sanm_kws_streaming/model.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:                \"there is no N-best results, perform recognition \" \"again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/beam_search.py:                \"there is no N-best results, perform recognition \" \"again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/chunk_utilis.py:    San-m: Memory equipped self-attention for end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/decoder.py:    SCAMA: Streaming chunk-aware multihead attention for online end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/encoder.py:    SCAMA: Streaming chunk-aware multihead attention for online end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:    SCAMA: Streaming chunk-aware multihead attention for online end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, ind=ind)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:            speech, speech_lengths, cache=cache[\"encoder\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech_lengths=None,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:            speech, speech_lengths, cache=cache, is_final=kwargs.get(\"is_final\", False)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/scama/model.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:    batch = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:    speech = torch.randn(2, 30, self.feats_dim)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:    speech_lengths = torch.tensor([15, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:    return (speech, speech_lengths, bias_embed)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:    return [\"speech\", \"speech_lengths\", \"bias_embed\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/export_meta.py:        \"speech_lengths\": {\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:            speech.shape[0] == speech_lengths.shape[0] == text.shape[0] == text_lengths.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        speech = speech[:, : speech_lengths.max()]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:            speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/seaco_paraformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    # speech = speech.to(device=\"cuda\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    # speech_lengths = speech_lengths.to(device=\"cuda\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    language_query = self.embed(language.to(speech.device)).unsqueeze(1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    textnorm_query = self.embed(textnorm.to(speech.device)).unsqueeze(1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    speech = torch.cat((textnorm_query, speech), dim=1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:        speech.size(0), 1, 1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    speech = torch.cat((input_query, speech), dim=1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    speech_lengths_new = speech_lengths + 4\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths_new)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    speech = torch.randn(2, 30, 560)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    return (speech, speech_lengths, language, textnorm)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:    return [\"speech\", \"speech_lengths\", \"language\", \"textnorm\"]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/export_meta.py:        \"speech_lengths\": {0: \"batch_size\"},\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:    SCAMA: Streaming chunk-aware multihead attention for online end-to-end speech recognition\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        self.lid_dict = {\"auto\": 0, \"zh\": 3, \"en\": 4, \"yue\": 7, \"ja\": 11, \"ko\": 12, \"nospeech\": 13}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, text)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        ).to(speech.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        ).to(speech.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech = torch.cat((style_query, speech), dim=1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech_lengths += 1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            speech.size(0), 1, 1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech = torch.cat((input_query, speech), dim=1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech_lengths += 3\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                speech.device\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        ).repeat(speech.size(0), 1, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            torch.LongTensor([[self.textnorm_dict[textnorm]]]).to(speech.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        ).repeat(speech.size(0), 1, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech = torch.cat((textnorm_query, speech), dim=1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech_lengths += 1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:            speech.size(0), 1, 1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech = torch.cat((input_query, speech), dim=1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        speech_lengths += 3\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:        encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                logits_speech = self.ctc.softmax(encoder_out)[i, 4 : encoder_out_lens[i].item(), :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                pred = logits_speech.argmax(-1).cpu()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                logits_speech[pred == self.blank_id, self.blank_id] = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                    logits_speech.unsqueeze(0).float(),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                    torch.Tensor(token_ids).unsqueeze(0).long().to(logits_speech.device),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sense_voice/model.py:                    torch.tensor(len(token_ids)).unsqueeze(0).long().to(logits_speech.device),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        normalize_speech_speaker: bool = False,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        self.normalize_speech_speaker = normalize_speech_speaker\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_lengths: torch.Tensor = None,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech: (Batch, samples) or (Batch, frames, input_size)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_lengths: (Batch,) default None for chunk interator,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:                                     have the speech_lengths returned.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        assert speech.shape[0] <= binary_labels.shape[0], (speech.shape, binary_labels.shape)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        if self.pse_embedding.device != speech.device:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            self.pse_embedding = self.pse_embedding.to(speech.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            self.power_weight = self.power_weight.to(speech.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            self.int_token_arr = self.int_token_arr.to(speech.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech, profile, binary_labels = self.profileaug(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:                speech,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:                speech_lengths,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech, speech_lengths, profile, profile_lengths, return_inter_outputs=True\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        (speech, speech_lengths), (profile, profile_lengths), (ci_score, cd_score) = inter_outputs\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_scored,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_miss,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_falarm,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        if speech_scored > 0 and num_frames > 0:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:                speech_miss / speech_scored,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:                speech_falarm / speech_scored,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:    def encode_speech(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech, speech_lengths = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_mask = ~make_pad_mask(speech_lengths, maxlen=speech.shape[1])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_mask = speech_mask.to(speech.device).unsqueeze(-1).float()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            return speech * speech_mask, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            return speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:    def concate_speech_ivc(speech: torch.Tensor, ivc: torch.Tensor) -> torch.Tensor:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        nn, tt = ivc.shape[1], speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech = speech.unsqueeze(dim=1)  # B x 1 x T x D\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech = speech.expand(-1, nn, -1, -1)  # B x N x T x D\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        sd_in = torch.cat([speech, ivc], dim=3)  # B x N x T x 2D\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_encoder_outputs: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        bb, tt = speech_encoder_outputs.shape[0], speech_encoder_outputs.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        d_sph, d_spk = speech_encoder_outputs.shape[2], speaker_encoder_outputs.shape[2]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        if self.normalize_speech_speaker:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_encoder_outputs = F.normalize(speech_encoder_outputs, dim=2)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        ge_in = self.concate_speech_ivc(speech_encoder_outputs, speaker_encoder_outputs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            ci_simi = self.ci_scorer(speech_encoder_outputs, speaker_encoder_outputs)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        # speech encoding\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech, speech_lengths = self.encode_speech(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        ci_simi, cd_simi = self.calc_similarity(speech, profile, speech_lengths, profile_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        logits = self.post_net_forward(similarity, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:                (speech, speech_lengths),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_lengths: (Batch,)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        assert encoder_out.size(0) == speech.size(0), (\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech.size(0),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_lengths = (\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            if speech_lengths is not None\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            else torch.ones(batch_size).int() * speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        assert speech_lengths.dim() == 1, speech_lengths.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech = speech[:, : speech_lengths.max()]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            feats, feats_lengths = self.frontend(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        # Note (jiatong): Credit to https://github.com/hitachi-speech/EEND\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        # compute speech activity detection error\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_scored = float(np.sum(n_ref > 0))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_miss = float(np.sum(np.logical_and(n_ref > 0, n_sys == 0)))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:        speech_falarm = float(np.sum(np.logical_and(n_ref == 0, n_sys > 0)))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_scored,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_miss,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/e2e_diar_sond.py:            speech_falarm,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/encoder/resnet34_encoder.py:        tf2torch_tensor_name_prefix_tf=\"EAND/speech_encoder\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/encoder/resnet34_encoder.py:        tf2torch_tensor_name_prefix_tf=\"seq2seq/speech_encoder\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/sond/encoder/resnet34_encoder.py:        tf2torch_tensor_name_prefix_tf=\"seq2seq/speech_encoder\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/specaug/profileaug.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/specaug/profileaug.py:        speech_lengths: torch.Tensor = None,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/specaug/profileaug.py:        speech, profile, binary_labels = (\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/specaug/profileaug.py:            torch.clone(speech),\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/specaug/profileaug.py:        return speech, profile, binary_labels\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:            speech_lengths.sum().item() * self.frontend.frame_shift * self.frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transducer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths, ctc=self.ctc)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/search.py:        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/transformer/search.py:                \"there is no N-best results, perform recognition \" \"again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/beam_search.py:        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/beam_search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/beam_search.py:            x (torch.Tensor): Encoded speech feature (T, D)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/beam_search.py:                \"there is no N-best results, perform recognition \" \"again with smaller minlenratio.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                        speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                        speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                speech_raw, encoder_out, encoder_out_lens = self.encode(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                    speech, speech_lengths, ind=ind\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            speech_raw, encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, ind=ind)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                        encoder_out, encoder_out_lens, speech_raw, speech_lengths, ind=ind\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                    encoder_out, encoder_out_lens, speech_raw, speech_lengths, ind=ind\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                        speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                        speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech_raw = speech.clone().to(speech.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths, ind=ind)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        return speech_raw, encoder_out, encoder_out_lens\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                        speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                        speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        encoder_out = torch.cat((speech, encoder_out_rm), dim=-1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            speech, speech_lengths = self.stride_conv(encoder_out, encoder_out_lens)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            speech = speech.detach()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            speech_lengths = speech_lengths.detach()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        encoder_out, encoder_out_lens, _ = self.encoder2(speech, speech_lengths, ind=ind)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        speech_raw = speech.clone().to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:        _, encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, ind=decoding_ind)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/uniasr/model.py:                encoder_out, encoder_out_lens, speech_raw, speech_lengths, ind=decoding_ind\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:            meta_data[\"batch_data_time\"] = speech_lengths.sum().item() * frame_shift * lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:        speech = speech.to(device=kwargs[\"device\"])[0, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:        # _, probs = self.model.detect_language(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:        result = whisper.decode(self.model, speech, options=options)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper/model.py:        # result = whisper.transcribe(self.model, speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/eres2net/ResNet.py:    recognition performance. Parameters expansion, baseWidth, and scale can be modified to obtain optimal performance.\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        if len(speech_lengths.size()) > 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            speech_lengths = speech_lengths[:, 0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech, speech_lengths = self.specaug(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths, ctc=self.ctc)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech: torch.Tensor,  # may be padding\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech_lengths: torch.Tensor,  # actual length\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        stats[\"token_length\"] = speech_lengths.max()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                padded_speech_lengths = torch.ones_like(speech_lengths) * speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech, padded_speech_lengths = self.specaug(speech, padded_speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech = speech.permute(0, 2, 1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech, speech_lengths = self.normalize(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths, ctc=self.ctc)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            encoder_out, encoder_out_lens, _ = self.encoder(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            speech, speech_lengths = data_in, data_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            if len(speech.shape) < 3:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech = speech[None, :, :]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            if speech_lengths is None:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech_lengths = speech.shape[1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:            speech, speech_lengths = extract_fbank(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech = speech.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/whisper_lid/model.py:        enc, enc_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            speech.shape[0] == speech_lengths.shape[0] == text.shape[0] == text_lengths.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        batch_size = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        speech: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        speech_lengths: torch.Tensor,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            speech: (Batch, Length, ...)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            speech_lengths: (Batch, )\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        assert speech_lengths.dim() == 1, speech_lengths.shape\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:        speech = speech[:, : speech_lengths.max()]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            feats, feats_lengths = self.frontend(speech, speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/models/xvector/e2e_sv.py:            feats, feats_lengths = speech, speech_lengths\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/compute_det_ctc.py:                        help='transcription of test data')\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/postprocess_utils.py:    \"<|nospeech|>\": \"<|lang|>\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/postprocess_utils.py:    \"<|nospeech|><|Event_UNK|>\": \"\u2753\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/postprocess_utils.py:    \"<|nospeech|>\": \"\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/postprocess_utils.py:def rich_transcription_postprocess(s):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/postprocess_utils.py:    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"\u2753\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:def slice_padding_fbank(speech, speech_lengths, vad_segments):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    speech_list = []\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    speech_lengths_list = []\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        end_idx = min(int(segment[0][1] * 16), speech_lengths[0])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_i = speech[0, bed_idx:end_idx]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_lengths_i = end_idx - bed_idx\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_list.append(speech_i)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_lengths_list.append(speech_lengths_i)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    feats_pad = pad_sequence(speech_list, batch_first=True, padding_value=0.0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    speech_lengths_pad = torch.Tensor(speech_lengths_list).int()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    return feats_pad, speech_lengths_pad\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:def slice_padding_audio_samples(speech, speech_lengths, vad_segments):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    speech_list = []\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    speech_lengths_list = []\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        end_idx = min(int(segment[0][1] * 16), speech_lengths)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_i = speech[bed_idx:end_idx]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_lengths_i = end_idx - bed_idx\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_list.append(speech_i)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:        speech_lengths_list.append(speech_lengths_i)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/funasr/utils/vad_utils.py:    return speech_list, speech_lengths_list\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-en.sh:    echo -e \"${YELLOW}Will download sample tools for the client to show how speech recognition works.${PLAIN}\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-en.sh:    echo -e \"   ${BOLD}-c, client , --client${PLAIN}     Get a client example to show how to initiate speech recognition.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-en.sh:PARAMS_VAD_ID=\"damo/speech_fsmn_vad_zh-cn-16k-common-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-en.sh:PARAMS_ASR_ID=\"damo/speech_paraformer-large_asr_nat-en-16k-common-vocab10020-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-zh.sh:    echo -e \"${YELLOW}Will download sample tools for the client to show how speech recognition works.${PLAIN}\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-zh.sh:    echo -e \"   ${BOLD}-c, client , --client${PLAIN}     Get a client example to show how to initiate speech recognition.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-zh.sh:PARAMS_LM_ID=\"damo/speech_ngram_lm_zh-cn-ai-wesp-fst\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-zh.sh:PARAMS_VAD_ID=\"damo/speech_fsmn_vad_zh-cn-16k-common-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-zh.sh:PARAMS_ASR_ID=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-online-cpu-zh.sh:    echo -e \"${YELLOW}Will download sample tools for the client to show how speech recognition works.${PLAIN}\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-online-cpu-zh.sh:    echo -e \"   ${BOLD}-c, client , --client${PLAIN}     Get a client example to show how to initiate speech recognition.\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-online-cpu-zh.sh:PARAMS_VAD_ID=\"damo/speech_fsmn_vad_zh-cn-16k-common-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/deploy_tools/funasr-runtime-deploy-online-cpu-zh.sh:PARAMS_ASR_ID=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/grpc/run_server.sh:  --model-dir models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/grpc/run_server.sh:  --online-model-dir models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/grpc/run_server.sh:  --vad-dir models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch \\\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/http/client.py:url = f\"http://{args.host}:{args.port}/recognition\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/http/server.py:@app.post(\"/recognition\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/http/server.py:async def api_recognition(audio: UploadFile = File(..., description=\"audio file\")):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/demo_contextual_paraformer.py:model_dir = \"iic/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/demo_paraformer.py:model_dir = \"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/demo_paraformer.py:model_dir_path = \"/home/openhands/.cache/modelscope/hub/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/demo_seaco_paraformer.py:model_dir = \"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/demo_sensevoice_small.py:from funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/demo_sensevoice_small.py:print([rich_transcription_postprocess(i) for i in res])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/paraformer_bin.py:            speech, _ = self.frontend.fbank(waveform)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/paraformer_bin.py:            feat, feat_len = self.frontend.lfr_cmvn(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/sensevoice_bin.py:        self.lid_dict = {\"auto\": 0, \"zh\": 3, \"en\": 4, \"yue\": 7, \"ja\": 11, \"ko\": 12, \"nospeech\": 13}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/sensevoice_bin.py:            speech, _ = self.frontend.fbank(waveform)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/sensevoice_bin.py:            feat, feat_len = self.frontend.lfr_cmvn(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/frontend.py:    path = \"/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/frontend.py:    cmvn_file = \"/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/am.mvn\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/frontend.py:    config_file = \"/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/config.yaml\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/frontend.py:    speech, _ = frontend.fbank_online(waveform)  # 1d, (sample,), numpy\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/frontend.py:        speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/postprocess_utils.py:    \"<|nospeech|>\": \"<|lang|>\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/postprocess_utils.py:    \"<|nospeech|><|Event_UNK|>\": \"\u2753\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/postprocess_utils.py:    \"<|nospeech|>\": \"\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/postprocess_utils.py:def rich_transcription_postprocess(s):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/libtorch/funasr_torch/utils/postprocess_utils.py:    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"\u2753\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_contextual_paraformer.py:model_dir = \"damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_offline.py:model_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_offline.py:# model_dir = \"damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_online.py:model_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_online.py:speech, sample_rate = soundfile.read(wav_path)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_online.py:speech_length = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_online.py:for sample_offset in range(0, speech_length, min(step, speech_length - sample_offset)):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_online.py:    if sample_offset + step >= speech_length - 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_online.py:        step = speech_length - sample_offset\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_paraformer_online.py:    rec_result = model(audio_in=speech[sample_offset : sample_offset + step], param_dict=param_dict)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_seaco_paraformer.py:model_dir = \"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_sencevoice_small.py:from funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_sencevoice_small.py:print([rich_transcription_postprocess(i) for i in res])\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_offline.py:model_dir = \"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_offline.py:wav_path = \"{}/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example/vad_example.wav\".format(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:model_dir = \"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:wav_path = \"{}/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example/vad_example.wav\".format(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:speech, sample_rate = soundfile.read(wav_path)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:speech_length = speech.shape[0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:for sample_offset in range(0, speech_length, min(step, speech_length - sample_offset)):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:    if sample_offset + step >= speech_length - 1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:        step = speech_length - sample_offset\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/demo_vad_online.py:        audio_in=speech[sample_offset : sample_offset + step], param_dict=param_dict\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/paraformer_bin.py:            speech, _ = self.frontend.fbank(waveform)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/paraformer_bin.py:            feat, feat_len = self.frontend.lfr_cmvn(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/sensevoice_bin.py:        self.lid_dict = {\"auto\": 0, \"zh\": 3, \"en\": 4, \"yue\": 7, \"ja\": 11, \"ko\": 12, \"nospeech\": 13}\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/sensevoice_bin.py:            speech, _ = self.frontend.fbank(waveform)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/sensevoice_bin.py:            if speech is None or speech.size == 0:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/sensevoice_bin.py:                print(\"detected speech size {speech.size}\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/sensevoice_bin.py:                raise ValueError(\"Empty speech detected, skipping this waveform.\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/sensevoice_bin.py:            feat, feat_len = self.frontend.lfr_cmvn(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        sil_to_speech_time_thres: int = 150,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        speech_to_sil_time_thres: int = 150,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        speech_2_noise_ratio: float = 1.0,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        speech_noise_thres: float = 0.6,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        speech_noise_thresh_low: float = -0.1,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        speech_noise_thresh_high: float = 0.3,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.sil_to_speech_time_thres = sil_to_speech_time_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_to_sil_time_thres = speech_to_sil_time_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_2_noise_ratio = speech_2_noise_ratio\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_noise_thres = speech_noise_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_noise_thresh_low = speech_noise_thresh_low\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_noise_thresh_high = speech_noise_thresh_high\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_prob = 0.0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        sil_to_speech_time: int,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        speech_to_sil_time: int,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.sil_to_speech_time = sil_to_speech_time\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_to_sil_time = speech_to_sil_time\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.sil_to_speech_frmcnt_thres = int(sil_to_speech_time / frame_size_ms)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_to_sil_frmcnt_thres = int(speech_to_sil_time / frame_size_ms)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            and self.win_sum >= self.sil_to_speech_frmcnt_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            and self.win_sum <= self.speech_to_sil_frmcnt_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            self.vad_opts.sil_to_speech_time_thres,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            self.vad_opts.speech_to_sil_time_thres,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.latest_confirmed_speech_frame = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            self.vad_opts.max_end_silence_time - self.vad_opts.speech_to_sil_time_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_noise_thres = self.vad_opts.speech_noise_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.latest_confirmed_speech_frame = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            self.vad_opts.max_end_silence_time - self.vad_opts.speech_to_sil_time_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.speech_noise_thres = self.vad_opts.speech_noise_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.latest_confirmed_speech_frame = 0\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.latest_confirmed_speech_frame = valid_frame\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        for t in range(self.latest_confirmed_speech_frame + 1, end_frame):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            noise_prob = math.log(sum_score) * self.vad_opts.speech_2_noise_ratio\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        speech_prob = math.log(sum_score)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:            frame_prob.speech_prob = speech_prob\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        if math.exp(speech_prob) >= math.exp(noise_prob) + self.speech_noise_thres:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:        self.max_end_sil_frame_cnt_thresh = max_end_sil - self.vad_opts.speech_to_sil_time_thres\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/e2e_vad.py:                for t in range(self.latest_confirmed_speech_frame + 1, cur_frm_idx):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/frontend.py:    path = \"/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/frontend.py:    cmvn_file = \"/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/am.mvn\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/frontend.py:    config_file = \"/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/config.yaml\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/frontend.py:    speech, _ = frontend.fbank_online(waveform)  # 1d, (sample,), numpy\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/frontend.py:        speech\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/postprocess_utils.py:    \"<|nospeech|>\": \"<|lang|>\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/postprocess_utils.py:    \"<|nospeech|><|Event_UNK|>\": \"\u2753\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/postprocess_utils.py:    \"<|nospeech|>\": \"\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/postprocess_utils.py:def rich_transcription_postprocess(s):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/utils/postprocess_utils.py:    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"\u2753\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/vad_bin.py:            speech, _ = self.frontend.fbank(waveform)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_onnx/vad_bin.py:            feat, feat_len = self.frontend.lfr_cmvn(speech)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_server_http.py:model_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_server_http.py:async def recognition_onnx(waveform):\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/onnxruntime/funasr_server_http.py:        result = await recognition_onnx(waveform)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/utils/test_cer.sh:model_name=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/utils/test_rtf.sh:model_name=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/utils/test_rtf.sh:echo \"total_rtf: $rtf, speech: $speed\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_client_api.py:    wav_path = \"/Users/zhifu/Downloads/modelscope_models/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    default=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    default=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    default=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    speech_start = False\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    speech_end_i = -1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                    websocket.status_dict_asr_online[\"is_final\"] = speech_end_i != -1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                    if speech_start:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                        speech_start_i, speech_end_i = await async_vad(websocket, message)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                    if speech_start_i != -1:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                        speech_start = True\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                        beg_bias = (websocket.vad_pre_idx - speech_start_i) // duration_ms\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                if speech_end_i != -1 or not websocket.is_speaking:\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:                    speech_start = False\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    speech_start = -1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    speech_end = -1\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:        return speech_start, speech_end\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:        speech_start = segments_result[0][0]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:        speech_end = segments_result[0][1]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/python/websocket/funasr_wss_server.py:    return speech_start, speech_end\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/run_server.sh:model_dir=\"damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/run_server.sh:vad_dir=\"damo/speech_fsmn_vad_zh-cn-16k-common-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/run_server.sh:lm_dir=\"damo/speech_ngram_lm_zh-cn-ai-wesp-fst\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/run_server_2pass.sh:model_dir=\"damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/run_server_2pass.sh:online_model_dir=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/run_server_2pass.sh:vad_dir=\"damo/speech_fsmn_vad_zh-cn-16k-common-onnx\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/run_server_2pass.sh:lm_dir=\"damo/speech_ngram_lm_zh-cn-ai-wesp-fst\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/client/client.py:from speech_client import *\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/client/client.py:        speech_client_cls = StreamingSpeechClient\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/client/client.py:        speech_client_cls = OfflineSpeechClient\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/client/client.py:            speech_client = speech_client_cls(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/client/client.py:                result = speech_client.recognize(li, idx)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:        output0_config = pb_utils.get_output_config_by_name(model_config, \"speech\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:        output1_config = pb_utils.get_output_config_by_name(model_config, \"speech_lengths\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:        speech = torch.zeros(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:            speech[i, : int(features_len[i])] = feature\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:        speech_lens = torch.tensor(features_len, dtype=torch.int64).to(self.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:        feats, feats_len = self.frontend.lfr(speech, speech_lens)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:            speech = features[i : i + 1][: int(feats_len[i].cpu())]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:            speech_lengths = feats_len[i].unsqueeze(0).unsqueeze(0)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:            speech, speech_lengths = speech.cpu(), speech_lengths.cpu()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:            out0 = pb_utils.Tensor.from_dlpack(\"speech\", to_dlpack(speech))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_offline/feature_extractor/1/model.py:            out1 = pb_utils.Tensor.from_dlpack(\"speech_lengths\", to_dlpack(speech_lengths))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_online/feature_extractor/1/model.py:        output0_config = pb_utils.get_output_config_by_name(model_config, \"speech\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_online/feature_extractor/1/model.py:            speech = self.seq_feat[corrid].get_frames(self.decoding_window)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_paraformer_large_online/feature_extractor/1/model.py:            out_tensor0 = pb_utils.Tensor(\"speech\", torch.unsqueeze(speech, 0).to(\"cpu\").numpy())\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:        output0_config = pb_utils.get_output_config_by_name(model_config, \"speech\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:        output1_config = pb_utils.get_output_config_by_name(model_config, \"speech_lengths\")\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:        speech = torch.zeros(\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:            speech[i, : int(features_len[i])] = feature\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:        speech_lens = torch.tensor(features_len, dtype=torch.int64).to(self.device)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:        feats, feats_len = self.frontend.lfr(speech, speech_lens)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:            speech = features[i : i + batch]\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:            speech_lengths = feats_len[i : i + batch].unsqueeze(1)\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:            speech, speech_lengths = speech.cpu(), speech_lengths.cpu()\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:            out0 = pb_utils.Tensor.from_dlpack(\"speech\", to_dlpack(speech))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/runtime/triton_gpu/model_repo_sense_voice_small/feature_extractor/1/model.py:            out1 = pb_utils.Tensor.from_dlpack(\"speech_lengths\", to_dlpack(speech_lengths))\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_conformer_asr_nat-zh-cn-16k-aishell1-vocab4234-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_conformer_asr_nat-zh-cn-16k-aishell2-vocab5212-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_data2vec_pretrain-zh-cn-aishell2-16k-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_data2vec_pretrain-paraformer-zh-cn-aishell2-16k\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"NPU-ASLP/speech_mfcca_asr-zh-cn-16k-alimeeting-vocab4950\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            audio_in=\"https://pre.modelscope.cn/api/v1/models/NPU-ASLP/speech_mfcca_asr-zh-cn-16k-alimeeting-vocab4950/repo?Revision=master&FilePath=example/asr_example_mc.wav\"\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-aishell1-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-aishell2-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer-tiny-commandword_asr_nat-zh-cn-16k-vocab544-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer_asr_nat-zh-cn-8k-common-vocab8358-tensorflow1\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer_asr_nat-zh-cn-16k-aishell1-vocab4234-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformer_asr_nat-zh-cn-16k-aishell2-vocab5212-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformerbert_asr_nat-zh-cn-16k-aishell1-vocab4234-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_paraformerbert_asr_nat-zh-cn-16k-aishell2-vocab5212-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-cantonese-CHS-16k-common-vocab1468-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-cantonese-CHS-16k-common-vocab1468-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-cn-dialect-16k-vocab8358-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-cn-dialect-16k-vocab8358-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-de-16k-common-vocab3690-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-de-16k-common-vocab3690-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-en-16k-common-vocab1080-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-en-16k-common-vocab1080-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-es-16k-common-vocab3445-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-es-16k-common-vocab3445-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-fa-16k-common-vocab1257-pytorch-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-fa-16k-common-vocab1257-pytorch-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-fr-16k-common-vocab3472-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-fr-16k-common-vocab3472-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-id-16k-common-vocab1067-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-id-16k-common-vocab1067-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-ja-16k-common-vocab93-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-ja-16k-common-vocab93-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-ko-16k-common-vocab6400-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-ko-16k-common-vocab6400-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-minnan-16k-common-vocab3825\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-pt-16k-common-vocab1617-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-pt-16k-common-vocab1617-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-ru-16k-common-vocab1664-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-ru-16k-common-vocab1664-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-vi-16k-common-vocab1001-pytorch-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-vi-16k-common-vocab1001-pytorch-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-zh-cn-8k-common-vocab3445-pytorch-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-zh-cn-8k-common-vocab3445-pytorch-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-zh-cn-8k-common-vocab8358-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-zh-cn-8k-common-vocab8358-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-zh-cn-16k-common-vocab8358-tensorflow1-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py:            model=\"damo/speech_UniASR_asr_2pass-zh-cn-16k-common-vocab8358-tensorflow1-online\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_vad_punc_inference_pipeline.py:            task=Tasks.auto_speech_recognition,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_vad_punc_inference_pipeline.py:            model=\"damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_vad_punc_inference_pipeline.py:            vad_model=\"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_auto_model.py:            \"model\": \"damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_lm_pipeline.py:            model=\"damo/speech_transformer_lm_zh-cn-common-vocab8404-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_sv_inference_pipeline.py:            model=\"damo/speech_xvector_sv-zh-cn-cnceleb-16k-spk3465-pytorch\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_tp_pipeline.py:            task=Tasks.speech_timestamp,\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_tp_pipeline.py:            model=\"damo/speech_timestamp_prediction-v1-16k-offline\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_vad_inference_pipeline.py:            model=\"damo/speech_fsmn_vad_zh-cn-8k-common\",\n/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_vad_inference_pipeline.py:            model=\"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\",", "extras": {"command": "find /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR -type f -name '*.py' -o -name '*.sh' | xargs grep -E 'speech|recognition|transcription'", "metadata": {"exit_code": 123, "pid": -1, "username": "openhands", "hostname": "4aa4d9a317a0", "working_dir": "/data/data/agent_test_codebase/GitTaskBench", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 123.]"}, "hidden": false}, "success": false}, {"id": 10, "timestamp": "2025-05-07T16:05:26.671255", "source": "agent", "message": "Reading file: /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py", "action": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_poTviqAY4OgDwjglAJi1mQhK", "model_response": {"id": "chatcmpl-BUU4HqkfaOme1XxImbNTiXgGCU7wu", "created": 1746605121, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "The repository contains several scripts related to speech recognition and transcription. Here are some files that may be relevant for processing the English audio input:\n\n1. **Common Voice / Whisper LID Demo**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py`\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py`\n   \n2. **ASR Inference Pipeline Test**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py`\n   \n3. **Industrial Data Pretraining - Paraformer Demo**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.py`\n\nThe scripts under Common Voice might be specifically focused on multilingual audio recognition, including English. The `test_asr_inference_pipeline.py` script likely performs tests on ASR recognition pipelines which might indicate how English audio is processed.\n\nNext, I will examine the Common Voice Whisper LID demo script, which seems focused on audio recognition tasks, to determine if it's suitable for processing the input English MP3 file. Let's explore one of these files further for details on implementation.", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py\"}", "name": "str_replace_editor"}, "id": "call_poTviqAY4OgDwjglAJi1mQhK", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 340, "prompt_tokens": 19851, "total_tokens": 20191, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.1007225, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 36789, "completion_tokens": 875, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py", "start": 0, "end": -1, "thought": "The repository contains several scripts related to speech recognition and transcription. Here are some files that may be relevant for processing the English audio input:\n\n1. **Common Voice / Whisper LID Demo**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py`\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py`\n   \n2. **ASR Inference Pipeline Test**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py`\n   \n3. **Industrial Data Pretraining - Paraformer Demo**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.py`\n\nThe scripts under Common Voice might be specifically focused on multilingual audio recognition, including English. The `test_asr_inference_pipeline.py` script likely performs tests on ASR recognition pipelines which might indicate how English audio is processed.\n\nNext, I will examine the Common Voice Whisper LID demo script, which seems focused on audio recognition tasks, to determine if it's suitable for processing the input English MP3 file. Let's explore one of these files further for details on implementation.", "impl_source": "oh_aci", "view_range": null}}, {"id": 11, "timestamp": "2025-05-07T16:05:26.683584", "source": "agent", "message": "I read the file /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py.", "cause": 10, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_poTviqAY4OgDwjglAJi1mQhK", "model_response": {"id": "chatcmpl-BUU4HqkfaOme1XxImbNTiXgGCU7wu", "created": 1746605121, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "The repository contains several scripts related to speech recognition and transcription. Here are some files that may be relevant for processing the English audio input:\n\n1. **Common Voice / Whisper LID Demo**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py`\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_modelscope.py`\n   \n2. **ASR Inference Pipeline Test**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/tests/test_asr_inference_pipeline.py`\n   \n3. **Industrial Data Pretraining - Paraformer Demo**:\n   - `/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/industrial_data_pretraining/bicif_paraformer/demo.py`\n\nThe scripts under Common Voice might be specifically focused on multilingual audio recognition, including English. The `test_asr_inference_pipeline.py` script likely performs tests on ASR recognition pipelines which might indicate how English audio is processed.\n\nNext, I will examine the Common Voice Whisper LID demo script, which seems focused on audio recognition tasks, to determine if it's suitable for processing the input English MP3 file. Let's explore one of these files further for details on implementation.", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py\"}", "name": "str_replace_editor"}, "id": "call_poTviqAY4OgDwjglAJi1mQhK", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 340, "prompt_tokens": 19851, "total_tokens": 20191, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py:\n     1\t#!/usr/bin/env python3\n     2\t# -*- encoding: utf-8 -*-\n     3\t# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n     4\t#  MIT License  (https://opensource.org/licenses/MIT)\n     5\t\n     6\tfrom funasr import AutoModel\n     7\t\n     8\tmultilingual_wavs = [\n     9\t    \"example_zh-CN.mp3\",\n    10\t    \"example_en.mp3\",\n    11\t    \"example_ja.mp3\",\n    12\t    \"example_ko.mp3\",\n    13\t]\n    14\t\n    15\tmodel = AutoModel(model=\"iic/speech_whisper-large_lid_multilingual_pytorch\")\n    16\tfor wav_id in multilingual_wavs:\n    17\t    wav_file = f\"{model.model_path}/examples/{wav_id}\"\n    18\t    res = model.generate(input=wav_file, data_type=\"sound\", inference_clip_length=250)\n    19\t    print(\"detect sample {}: {}\".format(wav_id, res))\n    20\t\n", "extras": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/whisper_lid/demo_funasr.py", "impl_source": "oh_aci"}}, {"id": 12, "timestamp": "2025-05-07T16:05:37.082618", "source": "agent", "message": "", "action": "edit", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_tkSBPIdwskmsnOfM4U0RlBeV", "model_response": {"id": "chatcmpl-BUU4NLiICluNfVQ4HGSKTLEXG6MS3", "created": 1746605127, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "The `demo_funasr.py` script demonstrates the use of the `AutoModel` class from the FunASR library to perform speech recognition using a multilingual model, specifically targeting the Whisper-large model for language identification. The script iterates over a list of example audio files, including English, and processes them using the `generate` method.\n\n### Steps to process the given English audio file:\n\n1. **Import the FunASR library**: Ensure the library is installed as per the README instructions (`pip install funasr`).\n\n2. **Model Initialization**: \n   - The `AutoModel` class is instantiated with the `Whisper-large` model, which is capable of handling multilingual inputs.\n\n3. **File Handling**: \n   - The script accesses example files from the model's path. We'll replace this with the absolute path to the given input MP3 file: `/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3`.\n\n4. **Generate Output**:\n   - The `generate` method processes the audio file, specifying the `sound` data type and setting inference parameters as necessary (e.g., `inference_clip_length`).\n\n5. **Output Directory**:\n   - Save the textual output to the specified directory: `/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02/output.txt`.\n\nI'll proceed to implement a script that utilizes this demo functionality to convert the specified English MP3 file to text, saving the result in the designated output directory.", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"create\",\"path\":\"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\",\"file_text\":\"#!/usr/bin/env python3\\n# -*- encoding: utf-8 -*-\\nfrom funasr import AutoModel\\nimport os\\n\\n# Path to input MP3 file\\ninput_audio_path = '/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3'\\n\\n# Path to output text file\\noutput_text_path = '/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02/output.txt'\\n\\n# Load the Whisper large model\\nmodel = AutoModel(model='iic/speech_whisper-large_lid_multilingual_pytorch')\\n\\n# Process the input audio file\\nres = model.generate(input=input_audio_path, data_type='sound', inference_clip_length=250)\\n\\n# Write the result to the output file\\nos.makedirs(os.path.dirname(output_text_path), exist_ok=True)\\nwith open(output_text_path, 'w') as output_file:\\n    output_file.write(res)\\n\\nprint(f'Transcription saved to {output_text_path}')\\n\"}", "name": "str_replace_editor"}, "id": "call_tkSBPIdwskmsnOfM4U0RlBeV", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 608, "prompt_tokens": 20495, "total_tokens": 21103, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.15804, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 57284, "completion_tokens": 1483, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "command": "create", "file_text": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\nfrom funasr import AutoModel\nimport os\n\n# Path to input MP3 file\ninput_audio_path = '/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3'\n\n# Path to output text file\noutput_text_path = '/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02/output.txt'\n\n# Load the Whisper large model\nmodel = AutoModel(model='iic/speech_whisper-large_lid_multilingual_pytorch')\n\n# Process the input audio file\nres = model.generate(input=input_audio_path, data_type='sound', inference_clip_length=250)\n\n# Write the result to the output file\nos.makedirs(os.path.dirname(output_text_path), exist_ok=True)\nwith open(output_text_path, 'w') as output_file:\n    output_file.write(res)\n\nprint(f'Transcription saved to {output_text_path}')\n", "old_str": null, "new_str": null, "insert_line": null, "content": "", "start": 1, "end": -1, "thought": "The `demo_funasr.py` script demonstrates the use of the `AutoModel` class from the FunASR library to perform speech recognition using a multilingual model, specifically targeting the Whisper-large model for language identification. The script iterates over a list of example audio files, including English, and processes them using the `generate` method.\n\n### Steps to process the given English audio file:\n\n1. **Import the FunASR library**: Ensure the library is installed as per the README instructions (`pip install funasr`).\n\n2. **Model Initialization**: \n   - The `AutoModel` class is instantiated with the `Whisper-large` model, which is capable of handling multilingual inputs.\n\n3. **File Handling**: \n   - The script accesses example files from the model's path. We'll replace this with the absolute path to the given input MP3 file: `/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3`.\n\n4. **Generate Output**:\n   - The `generate` method processes the audio file, specifying the `sound` data type and setting inference parameters as necessary (e.g., `inference_clip_length`).\n\n5. **Output Directory**:\n   - Save the textual output to the specified directory: `/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02/output.txt`.\n\nI'll proceed to implement a script that utilizes this demo functionality to convert the specified English MP3 file to text, saving the result in the designated output directory.", "impl_source": "oh_aci"}}, {"id": 13, "timestamp": "2025-05-07T16:05:37.090690", "source": "agent", "message": "I edited the file /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py.", "cause": 12, "observation": "edit", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_tkSBPIdwskmsnOfM4U0RlBeV", "model_response": {"id": "chatcmpl-BUU4NLiICluNfVQ4HGSKTLEXG6MS3", "created": 1746605127, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "The `demo_funasr.py` script demonstrates the use of the `AutoModel` class from the FunASR library to perform speech recognition using a multilingual model, specifically targeting the Whisper-large model for language identification. The script iterates over a list of example audio files, including English, and processes them using the `generate` method.\n\n### Steps to process the given English audio file:\n\n1. **Import the FunASR library**: Ensure the library is installed as per the README instructions (`pip install funasr`).\n\n2. **Model Initialization**: \n   - The `AutoModel` class is instantiated with the `Whisper-large` model, which is capable of handling multilingual inputs.\n\n3. **File Handling**: \n   - The script accesses example files from the model's path. We'll replace this with the absolute path to the given input MP3 file: `/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3`.\n\n4. **Generate Output**:\n   - The `generate` method processes the audio file, specifying the `sound` data type and setting inference parameters as necessary (e.g., `inference_clip_length`).\n\n5. **Output Directory**:\n   - Save the textual output to the specified directory: `/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02/output.txt`.\n\nI'll proceed to implement a script that utilizes this demo functionality to convert the specified English MP3 file to text, saving the result in the designated output directory.", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"create\",\"path\":\"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\",\"file_text\":\"#!/usr/bin/env python3\\n# -*- encoding: utf-8 -*-\\nfrom funasr import AutoModel\\nimport os\\n\\n# Path to input MP3 file\\ninput_audio_path = '/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3'\\n\\n# Path to output text file\\noutput_text_path = '/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02/output.txt'\\n\\n# Load the Whisper large model\\nmodel = AutoModel(model='iic/speech_whisper-large_lid_multilingual_pytorch')\\n\\n# Process the input audio file\\nres = model.generate(input=input_audio_path, data_type='sound', inference_clip_length=250)\\n\\n# Write the result to the output file\\nos.makedirs(os.path.dirname(output_text_path), exist_ok=True)\\nwith open(output_text_path, 'w') as output_file:\\n    output_file.write(res)\\n\\nprint(f'Transcription saved to {output_text_path}')\\n\"}", "name": "str_replace_editor"}, "id": "call_tkSBPIdwskmsnOfM4U0RlBeV", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 608, "prompt_tokens": 20495, "total_tokens": 21103, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "File created successfully at: /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "extras": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "prev_exist": false, "old_content": null, "new_content": null, "impl_source": "oh_aci", "diff": "--- /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\n+++ /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\n@@ -0,0 +1,4 @@\n+#!/usr/bin/env python3\n+# -*- encoding: utf-8 -*-\n+from funasr import AutoModel\n+import os\n@@ -1,0 +6,19 @@\n+# Path to input MP3 file\n+input_audio_path = '/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_02/input/FunASR_02_input.mp3'\n+\n+# Path to output text file\n+output_text_path = '/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_02/output.txt'\n+\n+# Load the Whisper large model\n+model = AutoModel(model='iic/speech_whisper-large_lid_multilingual_pytorch')\n+\n+# Process the input audio file\n+res = model.generate(input=input_audio_path, data_type='sound', inference_clip_length=250)\n+\n+# Write the result to the output file\n+os.makedirs(os.path.dirname(output_text_path), exist_ok=True)\n+with open(output_text_path, 'w') as output_file:\n+    output_file.write(res)\n+\n+print(f'Transcription saved to {output_text_path}')\n+", "_diff_cache": null}}, {"id": 14, "timestamp": "2025-05-07T16:05:40.001600", "source": "agent", "message": "Running command: python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_KgfaQQpZYF6GsyWZHCVM6hoE", "model_response": {"id": "chatcmpl-BUU4YCrEO1gwitZ40OzMvWy4kAyGo", "created": 1746605138, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\"}", "name": "execute_bash"}, "id": "call_KgfaQQpZYF6GsyWZHCVM6hoE", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 41, "prompt_tokens": 21142, "total_tokens": 21183, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.21130500000000002, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 78426, "completion_tokens": 1524, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 15, "timestamp": "2025-05-07T16:05:40.627704", "source": "agent", "message": "Command `python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py` executed with exit code 1.", "cause": 14, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_KgfaQQpZYF6GsyWZHCVM6hoE", "model_response": {"id": "chatcmpl-BUU4YCrEO1gwitZ40OzMvWy4kAyGo", "created": 1746605138, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\"}", "name": "execute_bash"}, "id": "call_KgfaQQpZYF6GsyWZHCVM6hoE", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 41, "prompt_tokens": 21142, "total_tokens": 21183, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Traceback (most recent call last):\n  File \"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\", line 3, in <module>\n    from funasr import AutoModel\nModuleNotFoundError: No module named 'funasr'", "extras": {"command": "python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "metadata": {"exit_code": 1, "pid": -1, "username": "openhands", "hostname": "4aa4d9a317a0", "working_dir": "/data/data/agent_test_codebase/GitTaskBench", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 1.]"}, "hidden": false}, "success": false}, {"id": 16, "timestamp": "2025-05-07T16:05:43.680540", "source": "agent", "message": "Running command: pip3 install funasr", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_MvszPnJwFb3Xbn9s21PsQycj", "model_response": {"id": "chatcmpl-BUU4cpJvK7YgZOhJz7BgYjPORVW2E", "created": 1746605142, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"pip3 install funasr\"}", "name": "execute_bash"}, "id": "call_MvszPnJwFb3Xbn9s21PsQycj", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 21, "prompt_tokens": 21322, "total_tokens": 21343, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.26482000000000006, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 99748, "completion_tokens": 1545, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "pip3 install funasr", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 17, "timestamp": "2025-05-07T16:07:07.325074", "source": "agent", "message": "Command `pip3 install funasr` executed with exit code -1.", "cause": 16, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_MvszPnJwFb3Xbn9s21PsQycj", "model_response": {"id": "chatcmpl-BUU4cpJvK7YgZOhJz7BgYjPORVW2E", "created": 1746605142, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"pip3 install funasr\"}", "name": "execute_bash"}, "id": "call_MvszPnJwFb3Xbn9s21PsQycj", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 21, "prompt_tokens": 21322, "total_tokens": 21343, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Collecting funasr\n  Downloading funasr-1.2.6-py3-none-any.whl.metadata (32 kB)\nCollecting scipy>=1.4.1 (from funasr)\n  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting librosa (from funasr)\n  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\nCollecting jamo (from funasr)\n  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: PyYAML>=5.1.2 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from funasr) (6.0.2)\nCollecting soundfile>=0.12.1 (from funasr)\n  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\nCollecting kaldiio>=2.17.0 (from funasr)\n  Downloading kaldiio-2.18.1-py3-none-any.whl.metadata (13 kB)\nCollecting torch-complex (from funasr)\n  Downloading torch_complex-0.4.4-py3-none-any.whl.metadata (3.1 kB)\nCollecting sentencepiece (from funasr)\n  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting jieba (from funasr)\n  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 19.2/19.2 MB 12.2 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting pytorch-wpe (from funasr)\n  Downloading pytorch_wpe-0.0.1-py3-none-any.whl.metadata (242 bytes)\nCollecting editdistance>=0.5.2 (from funasr)\n  Downloading editdistance-0.8.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nCollecting oss2 (from funasr)\n  Downloading oss2-2.19.1.tar.gz (298 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: tqdm in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from funasr) (4.67.1)\nCollecting umap-learn (from funasr)\n  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\nCollecting jaconv (from funasr)\n  Downloading jaconv-0.4.0.tar.gz (17 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting hydra-core>=1.3.2 (from funasr)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting tensorboardX (from funasr)\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: requests in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from funasr) (2.32.3)\nCollecting modelscope (from funasr)\n  Downloading modelscope-1.25.0-py3-none-any.whl.metadata (39 kB)\nCollecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->funasr)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->funasr)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: packaging in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from hydra-core>=1.3.2->funasr) (24.2)\nRequirement already satisfied: numpy in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from kaldiio>=2.17.0->funasr) (2.2.4)\nRequirement already satisfied: cffi>=1.0 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from soundfile>=0.12.1->funasr) (1.17.1)\nCollecting audioread>=2.1.9 (from librosa->funasr)\n  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\nCollecting numba>=0.51.0 (from librosa->funasr)\n  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nCollecting scikit-learn>=1.1.0 (from librosa->funasr)\n  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: joblib>=1.0 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from librosa->funasr) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from librosa->funasr) (5.2.1)\nCollecting pooch>=1.1 (from librosa->funasr)\n  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\nCollecting soxr>=0.3.2 (from librosa->funasr)\n  Downloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nRequirement already satisfied: typing_extensions>=4.1.1 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from librosa->funasr) (4.12.2)\nCollecting lazy_loader>=0.1 (from librosa->funasr)\n  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\nCollecting msgpack>=1.0 (from librosa->funasr)\n  Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\nRequirement already satisfied: urllib3>=1.26 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from modelscope->funasr) (2.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from requests->funasr) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from requests->funasr) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from requests->funasr) (2025.1.31)\nCollecting crcmod>=1.7 (from oss2->funasr)\n  Downloading crcmod-1.7.tar.gz (89 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: pycryptodome>=3.4.7 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from oss2->funasr) (3.21.0)\nCollecting aliyun-python-sdk-kms>=2.4.1 (from oss2->funasr)\n  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting aliyun-python-sdk-core>=2.13.12 (from oss2->funasr)\n  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: six in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from oss2->funasr) (1.17.0)\nRequirement already satisfied: protobuf>=3.20 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from tensorboardX->funasr) (4.25.6)\nCollecting pynndescent>=0.5 (from umap-learn->funasr)\n  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\nCollecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->funasr)\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: cryptography>=3.0.0 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr) (44.0.2)\nRequirement already satisfied: pycparser in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->funasr) (2.22)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa->funasr)\n  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: platformdirs>=2.5.0 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from pooch>=1.1->librosa->funasr) (4.3.6)\nCollecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->funasr)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nDownloading funasr-1.2.6-py3-none-any.whl (701 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 701.6/701.6 kB 477.5 kB/s eta 0:00:00\nDownloading editdistance-0.8.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (418 kB)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\nDownloading kaldiio-2.18.1-py3-none-any.whl (29 kB)\nDownloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 37.3/37.3 MB 8.1 MB/s eta 0:00:00\nDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 5.4 MB/s eta 0:00:00\nDownloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\nDownloading librosa-0.11.0-py3-none-any.whl (260 kB)\nDownloading modelscope-1.25.0-py3-none-any.whl (5.9 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.9/5.9 MB 16.0 MB/s eta 0:00:00\nDownloading pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\nDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 7.4 MB/s eta 0:00:00\nDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\nDownloading torch_complex-0.4.4-py3-none-any.whl (9.1 kB)\nDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\nDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\nDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\nDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\nDownloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\nDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.9/3.9 MB 3.3 MB/s eta 0:00:00\nDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\nDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\nDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\nDownloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.1/13.1 MB 6.6 MB/s eta 0:00:00\nDownloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\nDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nDownloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.4/42.4 MB 14.1 MB/s eta 0:00:00\nDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: antlr4-python3-runtime, jaconv, jieba, oss2, aliyun-python-sdk-core, crcmod\n  Building wheel for antlr4-python3-runtime (pyproject.toml) ... done\n  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144589 sha256=e684e9125685a10a8799f7853b8d936ea5165fd1cb90ccd31d5e83ca535986d7\n  Stored in directory: /home/openhands/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n  Building wheel for jaconv (pyproject.toml) ... done\n  Created wheel for jaconv: filename=jaconv-0.4.0-py3-none-any.whl size=18312 sha256=7c0d37f4c7772da246375510e0309b2925ae840a1ca306e6f467593c63dfaa3f\n  Stored in directory: /home/openhands/.cache/pip/wheels/c9/97/53/1f827ebf916b899520ce663227497206525c432be72cc29265\n  Building wheel for jieba (pyproject.toml) ... done\n  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314508 sha256=2ecefdd7c29a285ae4969a4bc44ed375dc00b7205d670fad09bdec7fdb82a292\n  Stored in directory: /home/openhands/.cache/pip/wheels/08/a1/a3/5c8ac52cc2f5782ffffc34c95c57c8e5ecb3063dc69541ee7c\n  Building wheel for oss2 (pyproject.toml) ... done\n  Created wheel for oss2: filename=oss2-2.19.1-py3-none-any.whl size=124033 sha256=9e779f9712170828eeed66911452fca275baab794423110b96ca72903b981be4\n  Stored in directory: /home/openhands/.cache/pip/wheels/a6/aa/ce/a0c9e73f8e4a3b7813b6b0d9dbddfb83028fd416828f27d97b\n  Building wheel for aliyun-python-sdk-core (pyproject.toml) ... done\n  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535416 sha256=2c434a2e5c0170ba583d59ba3a87f677ad5cac7eebdc3d93cf0826ef8f0fa08e\n  Stored in directory: /home/openhands/.cache/pip/wheels/5a/60/7c/80d5fdcd6d0e016e4a7ff4a66fd9321b3096ae676d78fe0212\n  Building wheel for crcmod (pyproject.toml) ... done\n  Created wheel for crcmod: filename=crcmod-1.7-cp312-cp312-linux_x86_64.whl size=23187 sha256=b59b581454c1d451a67a5dbd14da54acf76ea83db84f8be6e5f90eec893c544b\n  Stored in directory: /home/openhands/.cache/pip/wheels/76/08/0b/caa8b1380122cbfe6a03eaccbec0f63c67e619af4e30ca5e2a\nSuccessfully built antlr4-python3-runtime jaconv jieba oss2 aliyun-python-sdk-core crcmod\nInstalling collected packages: sentencepiece, jieba, jamo, jaconv, crcmod, antlr4-python3-runtime, torch-complex, threadpoolctl, tensorboardX, soxr, scipy, pytorch-wpe, omegaconf, msgpack, llvmlite, lazy_loader, kaldiio, jmespath, editdistance, audioread, soundfile, scikit-learn, pooch, numba, modelscope, hydra-core, pynndescent, librosa, aliyun-python-sdk-core, umap-learn, aliyun-python-sdk-kms, oss2, funasr\n  Attempting uninstall: jmespath\n    Found existing installation: jmespath 1.0.1\n    Uninstalling jmespath-1.0.1:\n      Successfully uninstalled jmespath-1.0.1\n  WARNING: Failed to remove contents in a temporary directory '/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages/~mespath'.\n  You can safely remove it manually.", "extras": {"command": "pip3 install funasr", "metadata": {"exit_code": -1, "pid": -1, "username": null, "hostname": null, "working_dir": null, "py_interpreter_path": null, "prefix": "", "suffix": "\n[The command has no new output after 10 seconds. You may wait longer to see additional output by sending empty command '', send other commands to interact with the current process, or send keys to interrupt/kill the command.]"}, "hidden": false}, "success": false}, {"id": 18, "timestamp": "2025-05-07T16:07:11.612043", "source": "agent", "message": "Running command: ", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_uGz4Y1OfbRGFHqnXWHQRzg1f", "model_response": {"id": "chatcmpl-BUU62mq9WUpvOEe5eePo5dvBFniYS", "created": 1746605230, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"\",\"is_input\":\"true\"}", "name": "execute_bash"}, "id": "call_uGz4Y1OfbRGFHqnXWHQRzg1f", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 19, "prompt_tokens": 26476, "total_tokens": 26495, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.33120000000000005, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 126224, "completion_tokens": 1564, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "", "is_input": true, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 19, "timestamp": "2025-05-07T16:07:12.740574", "source": "agent", "message": "Command `` executed with exit code 0.", "cause": 18, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_uGz4Y1OfbRGFHqnXWHQRzg1f", "model_response": {"id": "chatcmpl-BUU62mq9WUpvOEe5eePo5dvBFniYS", "created": 1746605230, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"\",\"is_input\":\"true\"}", "name": "execute_bash"}, "id": "call_uGz4Y1OfbRGFHqnXWHQRzg1f", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 19, "prompt_tokens": 26476, "total_tokens": 26495, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Successfully installed aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 antlr4-python3-runtime-4.9.3 audioread-3.0.1 crcmod-1.7 editdistance-0.8.1 funasr-1.2.6 hydra-core-1.3.2 jaconv-0.4.0 jamo-0.4.1 jieba-0.42.1 jmespath-0.10.0 kaldiio-2.18.1 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.44.0 modelscope-1.25.0 msgpack-1.1.0 numba-0.61.2 omegaconf-2.3.0 oss2-2.19.1 pooch-1.8.2 pynndescent-0.5.13 pytorch-wpe-0.0.1 scikit-learn-1.6.1 scipy-1.15.2 sentencepiece-0.2.0 soundfile-0.13.1 soxr-0.5.0.post1 tensorboardX-2.6.2.2 threadpoolctl-3.6.0 torch-complex-0.4.4 umap-learn-0.5.7\n\n[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n[notice] To update, run: pip install --upgrade pip", "extras": {"command": "", "metadata": {"exit_code": 0, "pid": -1, "username": "openhands", "hostname": "4aa4d9a317a0", "working_dir": "/data/data/agent_test_codebase/GitTaskBench", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 0.]"}, "hidden": false}, "success": true}, {"id": 20, "timestamp": "2025-05-07T16:07:16.229287", "source": "agent", "message": "Running command: python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_jdAaRhSxRcbKhRGANqfrkKpy", "model_response": {"id": "chatcmpl-BUU66BvV7z8YT1KxH0nSashMvL4zI", "created": 1746605234, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\"}", "name": "execute_bash"}, "id": "call_jdAaRhSxRcbKhRGANqfrkKpy", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 41, "prompt_tokens": 26906, "total_tokens": 26947, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.39887500000000004, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 153130, "completion_tokens": 1605, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 21, "timestamp": "2025-05-07T16:07:18.870677", "source": "agent", "message": "Command `python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py` executed with exit code 1.", "cause": 20, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_jdAaRhSxRcbKhRGANqfrkKpy", "model_response": {"id": "chatcmpl-BUU66BvV7z8YT1KxH0nSashMvL4zI", "created": 1746605234, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\"}", "name": "execute_bash"}, "id": "call_jdAaRhSxRcbKhRGANqfrkKpy", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 41, "prompt_tokens": 26906, "total_tokens": 26947, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Traceback (most recent call last):\n  File \"/data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py\", line 3, in <module>\n    from funasr import AutoModel\n  File \"/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages/funasr/__init__.py\", line 39, in <module>\n    from funasr.auto.auto_model import AutoModel\n  File \"/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages/funasr/auto/auto_model.py\", line 9, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'", "extras": {"command": "python3 /data/data/agent_test_codebase/GitTaskBench/code_base/FunASR/examples/common_voice/process_audio.py", "metadata": {"exit_code": 1, "pid": -1, "username": "openhands", "hostname": "4aa4d9a317a0", "working_dir": "/data/data/agent_test_codebase/GitTaskBench", "py_interpreter_path": "/openhands/poetry/openhands-ai-5O4_aCHf-py3.12/bin/python", "prefix": "", "suffix": "\n[The command completed with exit code 1.]"}, "hidden": false}, "success": false}, {"id": 22, "timestamp": "2025-05-07T16:07:25.300085", "source": "agent", "message": "Running command: pip3 install torch", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_f1DRxMdMbhuEfzn5BdvIvg4Y", "model_response": {"id": "chatcmpl-BUU6FF5WcDVewqj2t99SWlT9qnaqO", "created": 1746605243, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"pip3 install torch\"}", "name": "execute_bash"}, "id": "call_f1DRxMdMbhuEfzn5BdvIvg4Y", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 19, "prompt_tokens": 27198, "total_tokens": 27217, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.46706000000000003, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 180328, "completion_tokens": 1624, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "pip3 install torch", "is_input": false, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}, {"id": 23, "timestamp": "2025-05-07T16:10:05.441634", "source": "agent", "message": "Command `pip3 install torch` executed with exit code -1.", "cause": 22, "observation": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_f1DRxMdMbhuEfzn5BdvIvg4Y", "model_response": {"id": "chatcmpl-BUU6FF5WcDVewqj2t99SWlT9qnaqO", "created": 1746605243, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"pip3 install torch\"}", "name": "execute_bash"}, "id": "call_f1DRxMdMbhuEfzn5BdvIvg4Y", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 19, "prompt_tokens": 27198, "total_tokens": 27217, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Collecting torch\n  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from torch) (4.12.2)\nRequirement already satisfied: setuptools in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from torch) (75.8.2)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting networkx (from torch)\n  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: jinja2 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from torch) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.3.0 (from torch)\n  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /openhands/poetry/openhands-ai-5O4_aCHf-py3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\nDownloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 865.0/865.0 MB 21.3 MB/s eta 0:00:00\nDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 393.1/393.1 MB 22.3 MB/s eta 0:00:00\nDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.9/8.9 MB 6.6 MB/s eta 0:00:00\nDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 14.9 MB/s eta 0:00:00\nDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 897.7/897.7 kB 4.7 MB/s eta 0:00:00\nDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 571.0/571.0 MB 22.0 MB/s eta 0:00:00\nDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 200.2/200.2 MB 24.4 MB/s eta 0:00:00\nDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 8.0 MB/s eta 0:00:00\nDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.3/56.3 MB 27.9 MB/s eta 0:00:00\nDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 158.2/158.2 MB 23.9 MB/s eta 0:00:00\nDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 216.6/216.6 MB 23.8 MB/s eta 0:00:00\nDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 156.8/156.8 MB 24.5 MB/s eta 0:00:00\nDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 201.3/201.3 MB 23.7 MB/s eta 0:00:00\nDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 19.7/19.7 MB 24.6 MB/s eta 0:00:00\nDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\nDownloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 156.5/156.5 MB 25.0 MB/s eta 0:00:00\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 18.4 MB/s eta 0:00:00\nDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 13.5 MB/s eta 0:00:00\nDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 kB 7.5 MB/s eta 0:00:00\nInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch", "extras": {"command": "pip3 install torch", "metadata": {"exit_code": -1, "pid": -1, "username": null, "hostname": null, "working_dir": null, "py_interpreter_path": null, "prefix": "", "suffix": "\n[The command has no new output after 10 seconds. You may wait longer to see additional output by sending empty command '', send other commands to interact with the current process, or send keys to interrupt/kill the command.]"}, "hidden": false}, "success": false}, {"id": 24, "timestamp": "2025-05-07T16:10:09.394829", "source": "agent", "message": "Running command: ", "action": "run", "tool_call_metadata": {"function_name": "execute_bash", "tool_call_id": "call_h5lcDh6rQFOmFbIXDmNierwx", "model_response": {"id": "chatcmpl-BUU8tREZn1B0OjCKY1KCyt2X0OmQs", "created": 1746605407, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"\",\"is_input\":\"true\"}", "name": "execute_bash"}, "id": "call_h5lcDh6rQFOmFbIXDmNierwx", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 19, "prompt_tokens": 30267, "total_tokens": 30286, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "llm_metrics": {"accumulated_cost": 0.5429175, "accumulated_token_usage": {"model": "gpt-4o", "prompt_tokens": 210595, "completion_tokens": 1643, "cache_read_tokens": 0, "cache_write_tokens": 0, "response_id": ""}, "costs": [], "response_latencies": [], "token_usages": []}, "args": {"command": "", "is_input": true, "thought": "", "blocking": false, "hidden": false, "confirmation_state": "confirmed"}}]