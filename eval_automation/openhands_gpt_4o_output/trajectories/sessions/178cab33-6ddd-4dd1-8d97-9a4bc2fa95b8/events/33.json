{"id": 33, "timestamp": "2025-05-07T18:57:54.740221", "source": "agent", "message": "I read the file /data/data/agent_test_codebase/GitTaskBench/code_base/SpeechBrain/recipes/AMI/Diarization/experiment.py.", "cause": 32, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_O6AYvQHarFJn9kfogRaUUzco", "model_response": {"id": "chatcmpl-BUWlD7rAjyzxZze9Ac2UH7Xyx4TTY", "created": 1746615471, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/data/data/agent_test_codebase/GitTaskBench/code_base/SpeechBrain/recipes/AMI/Diarization/experiment.py\"}", "name": "str_replace_editor"}, "id": "call_O6AYvQHarFJn9kfogRaUUzco", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 46, "prompt_tokens": 35113, "total_tokens": 35159, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /data/data/agent_test_codebase/GitTaskBench/code_base/SpeechBrain/recipes/AMI/Diarization/experiment.py:\n     1\t#!/usr/bin/python3\n     2\t\"\"\"This recipe implements diarization system using deep embedding extraction followed by spectral clustering.\n     3\t\n     4\tTo run this recipe:\n     5\t> python experiment.py hparams/<your_hyperparams_file.yaml>\n     6\t e.g., python experiment.py hparams/ecapa_tdnn.yaml\n     7\t\n     8\tCondition: Oracle VAD (speech regions taken from the groundtruth).\n     9\t\n    10\tNote: There are multiple ways to write this recipe. We iterate over individual recordings.\n    11\t This approach is less GPU memory demanding and also makes code easy to understand.\n    12\t\n    13\tCitation: This recipe is based on the following paper,\n    14\t N. Dawalatabad, M. Ravanelli, F. Grondin, J. Thienpondt, B. Desplanques, H. Na,\n    15\t \"ECAPA-TDNN Embeddings for Speaker Diarization,\" arXiv:2104.01466, 2021.\n    16\t\n    17\tAuthors\n    18\t * Nauman Dawalatabad 2020\n    19\t\"\"\"\n    20\t\n    21\timport glob\n    22\timport json\n    23\timport os\n    24\timport pickle\n    25\timport shutil\n    26\timport sys\n    27\t\n    28\timport numpy as np\n    29\timport torch\n    30\tfrom hyperpyyaml import load_hyperpyyaml\n    31\tfrom tqdm.contrib import tqdm\n    32\t\n    33\timport speechbrain as sb\n    34\tfrom speechbrain.dataio.dataio import read_audio, read_audio_multichannel\n    35\tfrom speechbrain.processing import diarization as diar\n    36\tfrom speechbrain.processing.PLDA_LDA import StatObject_SB\n    37\tfrom speechbrain.utils.DER import DER\n    38\tfrom speechbrain.utils.distributed import run_on_main\n    39\tfrom speechbrain.utils.logger import get_logger\n    40\t\n    41\tnp.random.seed(1234)\n    42\t\n    43\t# Logger setup\n    44\tlogger = get_logger(__name__)\n    45\tcurrent_dir = os.path.dirname(os.path.abspath(__file__))\n    46\tsys.path.append(os.path.dirname(current_dir))\n    47\t\n    48\t\n    49\ttry:\n    50\t    import sklearn  # noqa F401\n    51\texcept ImportError:\n    52\t    err_msg = \"Cannot import optional dependency `scikit-learn` (sklearn) used in this module.\\n\"\n    53\t    err_msg += \"Please follow the below instructions\\n\"\n    54\t    err_msg += \"=============================\\n\"\n    55\t    err_msg += \"Using pip:\\n\"\n    56\t    err_msg += \"pip install scikit-learn\\n\"\n    57\t    err_msg += \"================================ \\n\"\n    58\t    err_msg += \"Using conda:\\n\"\n    59\t    err_msg += \"conda install scikit-learn\"\n    60\t    raise ImportError(err_msg)\n    61\t\n    62\t\n    63\tdef compute_embeddings(wavs, lens):\n    64\t    \"\"\"Definition of the steps for computation of embeddings from the waveforms.\"\"\"\n    65\t    with torch.no_grad():\n    66\t        wavs = wavs.to(run_opts[\"device\"])\n    67\t        feats = params[\"compute_features\"](wavs)\n    68\t        feats = params[\"mean_var_norm\"](feats, lens)\n    69\t        emb = params[\"embedding_model\"](feats, lens)\n    70\t        emb = params[\"mean_var_norm_emb\"](\n    71\t            emb, torch.ones(emb.shape[0], device=run_opts[\"device\"])\n    72\t        )\n    73\t\n    74\t    return emb\n    75\t\n    76\t\n    77\tdef embedding_computation_loop(split, set_loader, stat_file):\n    78\t    \"\"\"Extracts embeddings for a given dataset loader.\"\"\"\n    79\t\n    80\t    # Note: We use speechbrain.processing.PLDA_LDA.StatObject_SB type to store embeddings.\n    81\t    # Extract embeddings (skip if already done).\n    82\t    if not os.path.isfile(stat_file):\n    83\t        logger.debug(\"Extracting deep embeddings and diarizing\")\n    84\t        embeddings = np.empty(shape=[0, params[\"emb_dim\"]], dtype=np.float64)\n    85\t        modelset = []\n    86\t        segset = []\n    87\t\n    88\t        # Different data may have different statistics.\n    89\t        params[\"mean_var_norm_emb\"].count = 0\n    90\t\n    91\t        for batch in set_loader:\n    92\t            ids = batch.id\n    93\t            wavs, lens = batch.sig\n    94\t\n    95\t            mod = [x for x in ids]\n    96\t            seg = [x for x in ids]\n    97\t            modelset = modelset + mod\n    98\t            segset = segset + seg\n    99\t\n   100\t            # Embedding computation.\n   101\t            emb = (\n   102\t                compute_embeddings(wavs, lens)\n   103\t                .contiguous()\n   104\t                .squeeze(1)\n   105\t                .cpu()\n   106\t                .numpy()\n   107\t            )\n   108\t            embeddings = np.concatenate((embeddings, emb), axis=0)\n   109\t\n   110\t        modelset = np.array(modelset, dtype=\"|O\")\n   111\t        segset = np.array(segset, dtype=\"|O\")\n   112\t\n   113\t        # Initialize variables for start, stop and stat0.\n   114\t        s = np.array([None] * embeddings.shape[0])\n   115\t        b = np.array([[1.0]] * embeddings.shape[0])\n   116\t\n   117\t        stat_obj = StatObject_SB(\n   118\t            modelset=modelset,\n   119\t            segset=segset,\n   120\t            start=s,\n   121\t            stop=s,\n   122\t            stat0=b,\n   123\t            stat1=embeddings,\n   124\t        )\n   125\t        logger.debug(\"Saving Embeddings...\")\n   126\t        stat_obj.save_stat_object(stat_file)\n   127\t\n   128\t    else:\n   129\t        logger.debug(\"Skipping embedding extraction (as already present).\")\n   130\t        logger.debug(\"Loading previously saved embeddings.\")\n   131\t\n   132\t        with open(stat_file, \"rb\") as in_file:\n   133\t            stat_obj = pickle.load(in_file)\n   134\t\n   135\t    return stat_obj\n   136\t\n   137\t\n   138\tdef prepare_subset_json(full_meta_data, rec_id, out_meta_file):\n   139\t    \"\"\"Prepares metadata for a given recording ID.\n   140\t\n   141\t    Arguments\n   142\t    ---------\n   143\t    full_meta_data : json\n   144\t        Full meta (json) containing all the recordings\n   145\t    rec_id : str\n   146\t        The recording ID for which meta (json) has to be prepared\n   147\t    out_meta_file : str\n   148\t        Path of the output meta (json) file.\n   149\t    \"\"\"\n   150\t\n   151\t    subset = {}\n   152\t    for key in full_meta_data:\n   153\t        k = str(key)\n   154\t        if k.startswith(rec_id):\n   155\t            subset[key] = full_meta_data[key]\n   156\t\n   157\t    with open(out_meta_file, mode=\"w\", encoding=\"utf-8\") as json_f:\n   158\t        json.dump(subset, json_f, indent=2)\n   159\t\n   160\t\n   161\tdef diarize_dataset(full_meta, split_type, n_lambdas, pval, n_neighbors=10):\n   162\t    \"\"\"This function diarizes all the recordings in a given dataset. It performs\n   163\t    computation of embedding and clusters them using spectral clustering (or other backends).\n   164\t    The output speaker boundary file is stored in the RTTM format.\n   165\t    \"\"\"\n   166\t\n   167\t    # Prepare `spkr_info` only once when Oracle num of speakers is selected.\n   168\t    # spkr_info is essential to obtain number of speakers from groundtruth.\n   169\t    if params[\"oracle_n_spkrs\"] is True:\n   170\t        full_ref_rttm_file = (\n   171\t            params[\"ref_rttm_dir\"] + \"/fullref_ami_\" + split_type + \".rttm\"\n   172\t        )\n   173\t\n   174\t        rttm = diar.read_rttm(full_ref_rttm_file)\n   175\t\n   176\t        spkr_info = list(  # noqa F841\n   177\t            filter(lambda x: x.startswith(\"SPKR-INFO\"), rttm)\n   178\t        )\n   179\t\n   180\t    # Get all the recording IDs in this dataset.\n   181\t    all_keys = full_meta.keys()\n   182\t    A = [word.rstrip().split(\"_\")[0] for word in all_keys]\n   183\t    all_rec_ids = list(set(A[1:]))\n   184\t    all_rec_ids.sort()\n   185\t    split = \"AMI_\" + split_type\n   186\t    i = 1\n   187\t\n   188\t    # Setting eval modality.\n   189\t    params[\"embedding_model\"].eval()\n   190\t    msg = \"Diarizing \" + split_type + \" set\"\n   191\t    logger.info(msg)\n   192\t\n   193\t    if len(all_rec_ids) <= 0:\n   194\t        msg = \"No recording IDs found! Please check if meta_data json file is properly generated.\"\n   195\t        raise ValueError(msg)\n   196\t\n   197\t    # Diarizing different recordings in a dataset.\n   198\t    for rec_id in tqdm(all_rec_ids):\n   199\t        # This tag will be displayed in the log.\n   200\t        tag = (\n   201\t            \"[\"\n   202\t            + str(split_type)\n   203\t            + \": \"\n   204\t            + str(i)\n   205\t            + \"/\"\n   206\t            + str(len(all_rec_ids))\n   207\t            + \"]\"\n   208\t        )\n   209\t        i = i + 1\n   210\t\n   211\t        # Log message.\n   212\t        msg = \"Diarizing %s : %s \" % (tag, rec_id)\n   213\t        logger.debug(msg)\n   214\t\n   215\t        # Embedding directory.\n   216\t        if not os.path.exists(os.path.join(params[\"embedding_dir\"], split)):\n   217\t            os.makedirs(os.path.join(params[\"embedding_dir\"], split))\n   218\t\n   219\t        # File to store embeddings.\n   220\t        emb_file_name = rec_id + \".\" + params[\"mic_type\"] + \".emb_stat.pkl\"\n   221\t        diary_stat_emb_file = os.path.join(\n   222\t            params[\"embedding_dir\"], split, emb_file_name\n   223\t        )\n   224\t\n   225\t        # Prepare a metadata (json) for one recording. This is basically a subset of full_meta.\n   226\t        # Lets keep this meta-info in embedding directory itself.\n   227\t        json_file_name = rec_id + \".\" + params[\"mic_type\"] + \".json\"\n   228\t        meta_per_rec_file = os.path.join(\n   229\t            params[\"embedding_dir\"], split, json_file_name\n   230\t        )\n   231\t\n   232\t        # Write subset (meta for one recording) json metadata.\n   233\t        prepare_subset_json(full_meta, rec_id, meta_per_rec_file)\n   234\t\n   235\t        # Prepare data loader.\n   236\t        diary_set_loader = dataio_prep(params, meta_per_rec_file)\n   237\t\n   238\t        # Putting modules on the device.\n   239\t        params[\"compute_features\"].to(run_opts[\"device\"])\n   240\t        params[\"mean_var_norm\"].to(run_opts[\"device\"])\n   241\t        params[\"embedding_model\"].to(run_opts[\"device\"])\n   242\t        params[\"mean_var_norm_emb\"].to(run_opts[\"device\"])\n   243\t\n   244\t        # Compute Embeddings.\n   245\t        diary_obj = embedding_computation_loop(\n   246\t            \"diary\", diary_set_loader, diary_stat_emb_file\n   247\t        )\n   248\t\n   249\t        # Adding tag for directory path.\n   250\t        type_of_num_spkr = \"oracle\" if params[\"oracle_n_spkrs\"] else \"est\"\n   251\t        tag = (\n   252\t            type_of_num_spkr\n   253\t            + \"_\"\n   254\t            + str(params[\"affinity\"])\n   255\t            + \"_\"\n   256\t            + params[\"backend\"]\n   257\t        )\n   258\t        out_rttm_dir = os.path.join(\n   259\t            params[\"sys_rttm_dir\"], params[\"mic_type\"], split, tag\n   260\t        )\n   261\t        if not os.path.exists(out_rttm_dir):\n   262\t            os.makedirs(out_rttm_dir)\n   263\t        out_rttm_file = out_rttm_dir + \"/\" + rec_id + \".rttm\"\n   264\t\n   265\t        # Processing starts from here.\n   266\t        if params[\"oracle_n_spkrs\"] is True:\n   267\t            # Oracle num of speakers.\n   268\t            num_spkrs = diar.get_oracle_num_spkrs(rec_id, spkr_info)\n   269\t        else:\n   270\t            if params[\"affinity\"] == \"nn\":\n   271\t                # Num of speakers tuned on dev set (only for nn affinity).\n   272\t                num_spkrs = n_lambdas\n   273\t            else:\n   274\t                # Num of speakers will be estimated using max eigen gap for cos based affinity.\n   275\t                # So adding None here. Will use this None later-on.\n   276\t                num_spkrs = None\n   277\t\n   278\t        if params[\"backend\"] == \"kmeans\":\n   279\t            diar.do_kmeans_clustering(\n   280\t                diary_obj, out_rttm_file, rec_id, num_spkrs, pval\n   281\t            )\n   282\t\n   283\t        if params[\"backend\"] == \"SC\":\n   284\t            # Go for Spectral Clustering (SC).\n   285\t            diar.do_spec_clustering(\n   286\t                diary_obj,\n   287\t                out_rttm_file,\n   288\t                rec_id,\n   289\t                num_spkrs,\n   290\t                pval,\n   291\t                params[\"affinity\"],\n   292\t                n_neighbors,\n   293\t            )\n   294\t\n   295\t        # Can used for AHC later. Likewise one can add different backends here.\n   296\t        if params[\"backend\"] == \"AHC\":\n   297\t            # call AHC\n   298\t            threshold = pval  # pval for AHC is nothing but threshold.\n   299\t            diar.do_AHC(diary_obj, out_rttm_file, rec_id, num_spkrs, threshold)\n   300\t\n   301\t    # Once all RTTM outputs are generated, concatenate individual RTTM files to obtain single RTTM file.\n   302\t    # This is not needed but just staying with the standards.\n   303\t    concate_rttm_file = out_rttm_dir + \"/sys_output.rttm\"\n   304\t    logger.debug(\"Concatenating individual RTTM files...\")\n   305\t    with open(concate_rttm_file, \"w\", encoding=\"utf-8\") as cat_file:\n   306\t        for f in glob.glob(out_rttm_dir + \"/*.rttm\"):\n   307\t            if f == concate_rttm_file:\n   308\t                continue\n   309\t            with open(f, \"r\", encoding=\"utf-8\") as indi_rttm_file:\n   310\t                shutil.copyfileobj(indi_rttm_file, cat_file)\n   311\t\n   312\t    msg = \"The system generated RTTM file for %s set : %s\" % (\n   313\t        split_type,\n   314\t        concate_rttm_file,\n   315\t    )\n   316\t    logger.debug(msg)\n   317\t\n   318\t    return concate_rttm_file\n   319\t\n   320\t\n   321\tdef dev_pval_tuner(full_meta, split_type):\n   322\t    \"\"\"Tuning p_value for affinity matrix.\n   323\t    The p_value used so that only p% of the values in each row is retained.\n   324\t    \"\"\"\n   325\t\n   326\t    DER_list = []\n   327\t    prange = np.arange(0.002, 0.015, 0.001)\n   328\t\n   329\t    n_lambdas = None  # using it as flag later.\n   330\t    for p_v in prange:\n   331\t        # Process whole dataset for value of p_v.\n   332\t        concate_rttm_file = diarize_dataset(\n   333\t            full_meta, split_type, n_lambdas, p_v\n   334\t        )\n   335\t\n   336\t        ref_rttm = os.path.join(params[\"ref_rttm_dir\"], \"fullref_ami_dev.rttm\")\n   337\t        sys_rttm = concate_rttm_file\n   338\t        [MS, FA, SER, DER_] = DER(\n   339\t            ref_rttm,\n   340\t            sys_rttm,\n   341\t            params[\"ignore_overlap\"],\n   342\t            params[\"forgiveness_collar\"],\n   343\t        )\n   344\t\n   345\t        DER_list.append(DER_)\n   346\t\n   347\t        if params[\"oracle_n_spkrs\"] is True and params[\"backend\"] == \"kmeans\":\n   348\t            # no need of p_val search. Note p_val is needed for SC for both oracle and est num of speakers.\n   349\t            # p_val is needed in oracle_n_spkr=False when using kmeans backend.\n   350\t            break\n   351\t\n   352\t    # Take p_val that gave minimum DER on Dev dataset.\n   353\t    tuned_p_val = prange[DER_list.index(min(DER_list))]\n   354\t\n   355\t    return tuned_p_val\n   356\t\n   357\t\n   358\tdef dev_ahc_threshold_tuner(full_meta, split_type):\n   359\t    \"\"\"Tuning threshold for affinity matrix. This function is called when AHC is used as backend.\"\"\"\n   360\t\n   361\t    DER_list = []\n   362\t    prange = np.arange(0.0, 1.0, 0.1)\n   363\t\n   364\t    n_lambdas = None  # using it as flag later.\n   365\t\n   366\t    # Note: p_val is threshold in case of AHC.\n   367\t    for p_v in prange:\n   368\t        # Process whole dataset for value of p_v.\n   369\t        concate_rttm_file = diarize_dataset(\n   370\t            full_meta, split_type, n_lambdas, p_v\n   371\t        )\n   372\t\n   373\t        ref_rttm = os.path.join(params[\"ref_rttm_dir\"], \"fullref_ami_dev.rttm\")\n   374\t        sys_rttm = concate_rttm_file\n   375\t        [MS, FA, SER, DER_] = DER(\n   376\t            ref_rttm,\n   377\t            sys_rttm,\n   378\t            params[\"ignore_overlap\"],\n   379\t            params[\"forgiveness_collar\"],\n   380\t        )\n   381\t\n   382\t        DER_list.append(DER_)\n   383\t\n   384\t        if params[\"oracle_n_spkrs\"] is True:\n   385\t            break  # no need of threshold search.\n   386\t\n   387\t    # Take p_val that gave minimum DER on Dev dataset.\n   388\t    tuned_p_val = prange[DER_list.index(min(DER_list))]\n   389\t\n   390\t    return tuned_p_val\n   391\t\n   392\t\n   393\tdef dev_nn_tuner(full_meta, split_type):\n   394\t    \"\"\"Tuning n_neighbors on dev set. Assuming oracle num of speakers.\n   395\t    This is used when nn based affinity is selected.\n   396\t    \"\"\"\n   397\t\n   398\t    DER_list = []\n   399\t    pval = None\n   400\t\n   401\t    # Now assuming oracle num of speakers.\n   402\t    n_lambdas = 4\n   403\t\n   404\t    for nn in range(5, 15):\n   405\t        # Process whole dataset for value of n_lambdas.\n   406\t        concate_rttm_file = diarize_dataset(\n   407\t            full_meta, split_type, n_lambdas, pval, nn\n   408\t        )\n   409\t\n   410\t        ref_rttm = os.path.join(params[\"ref_rttm_dir\"], \"fullref_ami_dev.rttm\")\n   411\t        sys_rttm = concate_rttm_file\n   412\t        [MS, FA, SER, DER_] = DER(\n   413\t            ref_rttm,\n   414\t            sys_rttm,\n   415\t            params[\"ignore_overlap\"],\n   416\t            params[\"forgiveness_collar\"],\n   417\t        )\n   418\t\n   419\t        DER_list.append([nn, DER_])\n   420\t\n   421\t        if params[\"oracle_n_spkrs\"] is True and params[\"backend\"] == \"kmeans\":\n   422\t            break\n   423\t\n   424\t    DER_list.sort(key=lambda x: x[1])\n   425\t    tunned_nn = DER_list[0]\n   426\t\n   427\t    return tunned_nn[0]\n   428\t\n   429\t\n   430\tdef dev_tuner(full_meta, split_type):\n   431\t    \"\"\"Tuning n_components on dev set. Used for nn based affinity matrix.\n   432\t    Note: This is a very basic tuning for nn based affinity.\n   433\t    This is work in progress till we find a better way.\n   434\t    \"\"\"\n   435\t\n   436\t    DER_list = []\n   437\t    pval = None\n   438\t    for n_lambdas in range(1, params[\"max_num_spkrs\"] + 1):\n   439\t        # Process whole dataset for value of n_lambdas.\n   440\t        concate_rttm_file = diarize_dataset(\n   441\t            full_meta, split_type, n_lambdas, pval\n   442\t        )\n   443\t\n   444\t        ref_rttm = os.path.join(params[\"ref_rttm_dir\"], \"fullref_ami_dev.rttm\")\n   445\t        sys_rttm = concate_rttm_file\n   446\t        [MS, FA, SER, DER_] = DER(\n   447\t            ref_rttm,\n   448\t            sys_rttm,\n   449\t            params[\"ignore_overlap\"],\n   450\t            params[\"forgiveness_collar\"],\n   451\t        )\n   452\t\n   453\t        DER_list.append(DER_)\n   454\t\n   455\t    # Take n_lambdas with minimum DER.\n   456\t    tuned_n_lambdas = DER_list.index(min(DER_list)) + 1\n   457\t\n   458\t    return tuned_n_lambdas\n   459\t\n   460\t\n   461\tdef dataio_prep(hparams, json_file):\n   462\t    \"\"\"Creates the datasets and their data processing pipelines.\n   463\t    This is used for multi-mic processing.\n   464\t    \"\"\"\n   465\t\n   466\t    # 1. Datasets\n   467\t    data_folder = hparams[\"data_folder\"]\n   468\t    dataset = sb.dataio.dataset.DynamicItemDataset.from_json(\n   469\t        json_path=json_file,\n   470\t        replacements={\"data_root\": data_folder},\n   471\t    )\n   472\t\n   473\t    # 2. Define audio pipeline.\n   474\t    if params[\"mic_type\"] == \"Array1\":\n   475\t        # Multi-mic (Microphone Array)\n   476\t        @sb.utils.data_pipeline.takes(\"wav\")\n   477\t        @sb.utils.data_pipeline.provides(\"sig\")\n   478\t        def audio_pipeline(wav):\n   479\t            mics_signals = read_audio_multichannel(wav).unsqueeze(0)\n   480\t            sig = params[\"multimic_beamformer\"](mics_signals)\n   481\t            sig = sig.squeeze()\n   482\t            return sig\n   483\t\n   484\t    else:\n   485\t        # Single microphone\n   486\t        @sb.utils.data_pipeline.takes(\"wav\")\n   487\t        @sb.utils.data_pipeline.provides(\"sig\")\n   488\t        def audio_pipeline(wav):\n   489\t            sig = read_audio(wav)\n   490\t  <response clipped><NOTE>Due to the max output limit, only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>\n", "extras": {"path": "/data/data/agent_test_codebase/GitTaskBench/code_base/SpeechBrain/recipes/AMI/Diarization/experiment.py", "impl_source": "oh_aci"}}