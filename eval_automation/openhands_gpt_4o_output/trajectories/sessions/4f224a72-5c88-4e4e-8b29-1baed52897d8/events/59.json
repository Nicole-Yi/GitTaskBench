{"id": 59, "timestamp": "2025-05-07T18:44:04.893832", "source": "agent", "message": "I read the file /data/data/agent_test_codebase/GitTaskBench/temp_speaker_separation.py.", "cause": 58, "observation": "read", "tool_call_metadata": {"function_name": "str_replace_editor", "tool_call_id": "call_qIVjhiOjT4guE5SxclR1LUJo", "model_response": {"id": "chatcmpl-BUWXqF6ISdyvcDUGlkVE8LiXQ0Koq", "created": 1746614642, "model": "gpt-4o", "object": "chat.completion", "system_fingerprint": "fp_ee1d74bde0", "choices": [{"finish_reason": "tool_calls", "index": 0, "message": {"content": "", "role": "assistant", "tool_calls": [{"function": {"arguments": "{\"command\":\"view\",\"path\":\"/data/data/agent_test_codebase/GitTaskBench/temp_speaker_separation.py\"}", "name": "str_replace_editor"}, "id": "call_qIVjhiOjT4guE5SxclR1LUJo", "type": "function"}], "function_call": null}}], "usage": {"completion_tokens": 37, "prompt_tokens": 25733, "total_tokens": 25770, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": 0, "rejected_prediction_tokens": null}, "prompt_tokens_details": null}, "service_tier": null}, "total_calls_in_response": 1}, "content": "Here's the result of running `cat -n` on /data/data/agent_test_codebase/GitTaskBench/temp_speaker_separation.py:\n     1\t#!/usr/bin/env/python3\n     2\t\"\"\"Recipe for training a neural speech separation system on the wsjmix\n     3\tdataset. The system employs an encoder, a decoder, and a masking network.\n     4\t\n     5\tTo run this recipe, do the following:\n     6\t> python train.py hparams/sepformer.yaml\n     7\t> python train.py hparams/dualpath_rnn.yaml\n     8\t> python train.py hparams/convtasnet.yaml\n     9\t\n    10\tThe experiment file is flexible enough to support different neural\n    11\tnetworks. By properly changing the parameter files, you can try\n    12\tdifferent architectures. The script supports both wsj2mix and\n    13\twsj3mix.\n    14\t\n    15\t\n    16\tAuthors\n    17\t * Cem Subakan 2020\n    18\t * Mirco Ravanelli 2020\n    19\t * Samuele Cornell 2020\n    20\t * Mirko Bronzi 2020\n    21\t * Jianyuan Zhong 2020\n    22\t\"\"\"\n    23\t\n    24\timport csv\n    25\timport os\n    26\timport sys\n    27\t\n    28\timport numpy as np\n    29\timport torch\n    30\timport torch.nn.functional as F\n    31\timport torchaudio\n    32\tfrom hyperpyyaml import load_hyperpyyaml\n    33\tfrom tqdm import tqdm\n    34\t\n    35\timport speechbrain as sb\n    36\timport speechbrain.nnet.schedulers as schedulers\n    37\tfrom speechbrain.utils.distributed import run_on_main\n    38\tfrom speechbrain.utils.logger import get_logger\n    39\t\n    40\t\n    41\t# Define training procedure\n    42\tclass Separation(sb.Brain):\n    43\t    def compute_forward(self, mix, targets, stage, noise=None):\n    44\t        \"\"\"Forward computations from the mixture to the separated signals.\"\"\"\n    45\t\n    46\t        # Unpack lists and put tensors in the right device\n    47\t        mix, mix_lens = mix\n    48\t        mix, mix_lens = mix.to(self.device), mix_lens.to(self.device)\n    49\t\n    50\t        # Convert targets to tensor\n    51\t        targets = torch.cat(\n    52\t            [targets[i][0].unsqueeze(-1) for i in range(self.hparams.num_spks)],\n    53\t            dim=-1,\n    54\t        ).to(self.device)\n    55\t\n    56\t        # Add speech distortions\n    57\t        if stage == sb.Stage.TRAIN:\n    58\t            with torch.no_grad():\n    59\t                if self.hparams.use_speedperturb:\n    60\t                    mix, targets = self.add_speed_perturb(targets, mix_lens)\n    61\t\n    62\t                    mix = targets.sum(-1)\n    63\t\n    64\t                if self.hparams.use_wavedrop:\n    65\t                    mix = self.hparams.drop_chunk(mix, mix_lens)\n    66\t                    mix = self.hparams.drop_freq(mix)\n    67\t\n    68\t                if self.hparams.limit_training_signal_len:\n    69\t                    mix, targets = self.cut_signals(mix, targets)\n    70\t\n    71\t        # Separation\n    72\t        mix_w = self.hparams.Encoder(mix)\n    73\t        est_mask = self.hparams.MaskNet(mix_w)\n    74\t        mix_w = torch.stack([mix_w] * self.hparams.num_spks)\n    75\t        sep_h = mix_w * est_mask\n    76\t\n    77\t        # Decoding\n    78\t        est_source = torch.cat(\n    79\t            [\n    80\t                self.hparams.Decoder(sep_h[i]).unsqueeze(-1)\n    81\t                for i in range(self.hparams.num_spks)\n    82\t            ],\n    83\t            dim=-1,\n    84\t        )\n    85\t\n    86\t        # T changed after conv1d in encoder, fix it here\n    87\t        T_origin = mix.size(1)\n    88\t        T_est = est_source.size(1)\n    89\t        if T_origin > T_est:\n    90\t            est_source = F.pad(est_source, (0, 0, 0, T_origin - T_est))\n    91\t        else:\n    92\t            est_source = est_source[:, :T_origin, :]\n    93\t\n    94\t        return est_source, targets\n    95\t\n    96\t    def compute_objectives(self, predictions, targets):\n    97\t        \"\"\"Computes the sinr loss\"\"\"\n    98\t        return self.hparams.loss(targets, predictions)\n    99\t\n   100\t    def fit_batch(self, batch):\n   101\t        \"\"\"Trains one batch\"\"\"\n   102\t\n   103\t        # Unpacking batch list\n   104\t        mixture = batch.mix_sig\n   105\t        targets = [batch.s1_sig, batch.s2_sig]\n   106\t\n   107\t        if self.hparams.num_spks == 3:\n   108\t            targets.append(batch.s3_sig)\n   109\t\n   110\t        with self.training_ctx:\n   111\t            predictions, targets = self.compute_forward(\n   112\t                mixture, targets, sb.Stage.TRAIN\n   113\t            )\n   114\t            loss = self.compute_objectives(predictions, targets)\n   115\t\n   116\t            # hard threshold the easy dataitems\n   117\t            if self.hparams.threshold_byloss:\n   118\t                th = self.hparams.threshold\n   119\t                loss = loss[loss > th]\n   120\t                if loss.nelement() > 0:\n   121\t                    loss = loss.mean()\n   122\t            else:\n   123\t                loss = loss.mean()\n   124\t\n   125\t        if loss.nelement() > 0 and loss < self.hparams.loss_upper_lim:\n   126\t            self.scaler.scale(loss).backward()\n   127\t            if self.hparams.clip_grad_norm >= 0:\n   128\t                self.scaler.unscale_(self.optimizer)\n   129\t                torch.nn.utils.clip_grad_norm_(\n   130\t                    self.modules.parameters(),\n   131\t                    self.hparams.clip_grad_norm,\n   132\t                )\n   133\t            self.scaler.step(self.optimizer)\n   134\t            self.scaler.update()\n   135\t        else:\n   136\t            self.nonfinite_count += 1\n   137\t            logger.info(\n   138\t                \"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n   139\t                    self.nonfinite_count\n   140\t                )\n   141\t            )\n   142\t            loss.data = torch.tensor(0.0).to(self.device)\n   143\t        self.optimizer.zero_grad()\n   144\t\n   145\t        return loss.detach().cpu()\n   146\t\n   147\t    def evaluate_batch(self, batch, stage):\n   148\t        \"\"\"Computations needed for validation/test batches\"\"\"\n   149\t        snt_id = batch.id\n   150\t        mixture = batch.mix_sig\n   151\t        targets = [batch.s1_sig, batch.s2_sig]\n   152\t        if self.hparams.num_spks == 3:\n   153\t            targets.append(batch.s3_sig)\n   154\t\n   155\t        with torch.no_grad():\n   156\t            predictions, targets = self.compute_forward(mixture, targets, stage)\n   157\t            loss = self.compute_objectives(predictions, targets)\n   158\t\n   159\t        # Manage audio file saving\n   160\t        if stage == sb.Stage.TEST and self.hparams.save_audio:\n   161\t            if hasattr(self.hparams, \"n_audio_to_save\"):\n   162\t                if self.hparams.n_audio_to_save > 0:\n   163\t                    self.save_audio(snt_id[0], mixture, targets, predictions)\n   164\t                    self.hparams.n_audio_to_save += -1\n   165\t            else:\n   166\t                self.save_audio(snt_id[0], mixture, targets, predictions)\n   167\t\n   168\t        return loss.mean().detach()\n   169\t\n   170\t    def on_stage_end(self, stage, stage_loss, epoch):\n   171\t        \"\"\"Gets called at the end of a epoch.\"\"\"\n   172\t        # Compute/store important stats\n   173\t        stage_stats = {\"si-snr\": stage_loss}\n   174\t        if stage == sb.Stage.TRAIN:\n   175\t            self.train_stats = stage_stats\n   176\t\n   177\t        # Perform end-of-iteration things, like annealing, logging, etc.\n   178\t        if stage == sb.Stage.VALID:\n   179\t            # Learning rate annealing\n   180\t            if isinstance(\n   181\t                self.hparams.lr_scheduler, schedulers.ReduceLROnPlateau\n   182\t            ):\n   183\t                current_lr, next_lr = self.hparams.lr_scheduler(\n   184\t                    [self.optimizer], epoch, stage_loss\n   185\t                )\n   186\t                schedulers.update_learning_rate(self.optimizer, next_lr)\n   187\t            else:\n   188\t                # if we do not use the reducelronplateau, we do not change the lr\n   189\t                current_lr = self.hparams.optimizer.optim.param_groups[0][\"lr\"]\n   190\t\n   191\t            self.hparams.train_logger.log_stats(\n   192\t                stats_meta={\"epoch\": epoch, \"lr\": current_lr},\n   193\t                train_stats=self.train_stats,\n   194\t                valid_stats=stage_stats,\n   195\t            )\n   196\t            self.checkpointer.save_and_keep_only(\n   197\t                meta={\"si-snr\": stage_stats[\"si-snr\"]}, min_keys=[\"si-snr\"]\n   198\t            )\n   199\t        elif stage == sb.Stage.TEST:\n   200\t            self.hparams.train_logger.log_stats(\n   201\t                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n   202\t                test_stats=stage_stats,\n   203\t            )\n   204\t\n   205\t    def add_speed_perturb(self, targets, targ_lens):\n   206\t        \"\"\"Adds speed perturbation and random_shift to the input signals\"\"\"\n   207\t\n   208\t        min_len = -1\n   209\t        recombine = False\n   210\t\n   211\t        if self.hparams.use_speedperturb or self.hparams.use_rand_shift:\n   212\t            # Performing speed change (independently on each source)\n   213\t            new_targets = []\n   214\t            recombine = True\n   215\t\n   216\t            for i in range(targets.shape[-1]):\n   217\t                new_target = self.hparams.speed_perturb(targets[:, :, i])\n   218\t                new_targets.append(new_target)\n   219\t                if i == 0:\n   220\t                    min_len = new_target.shape[-1]\n   221\t                else:\n   222\t                    if new_target.shape[-1] < min_len:\n   223\t                        min_len = new_target.shape[-1]\n   224\t\n   225\t            if self.hparams.use_rand_shift:\n   226\t                # Performing random_shift (independently on each source)\n   227\t                recombine = True\n   228\t                for i in range(targets.shape[-1]):\n   229\t                    rand_shift = torch.randint(\n   230\t                        self.hparams.min_shift, self.hparams.max_shift, (1,)\n   231\t                    )\n   232\t                    new_targets[i] = new_targets[i].to(self.device)\n   233\t                    new_targets[i] = torch.roll(\n   234\t                        new_targets[i], shifts=(rand_shift[0],), dims=1\n   235\t                    )\n   236\t\n   237\t            # Re-combination\n   238\t            if recombine:\n   239\t                if self.hparams.use_speedperturb:\n   240\t                    targets = torch.zeros(\n   241\t                        targets.shape[0],\n   242\t                        min_len,\n   243\t                        targets.shape[-1],\n   244\t                        device=targets.device,\n   245\t                        dtype=torch.float,\n   246\t                    )\n   247\t                for i, new_target in enumerate(new_targets):\n   248\t                    targets[:, :, i] = new_targets[i][:, 0:min_len]\n   249\t\n   250\t        mix = targets.sum(-1)\n   251\t        return mix, targets\n   252\t\n   253\t    def cut_signals(self, mixture, targets):\n   254\t        \"\"\"This function selects a random segment of a given length within the mixture.\n   255\t        The corresponding targets are selected accordingly\"\"\"\n   256\t        randstart = torch.randint(\n   257\t            0,\n   258\t            1 + max(0, mixture.shape[1] - self.hparams.training_signal_len),\n   259\t            (1,),\n   260\t        ).item()\n   261\t        targets = targets[\n   262\t            :, randstart : randstart + self.hparams.training_signal_len, :\n   263\t        ]\n   264\t        mixture = mixture[\n   265\t            :, randstart : randstart + self.hparams.training_signal_len\n   266\t        ]\n   267\t        return mixture, targets\n   268\t\n   269\t    def reset_layer_recursively(self, layer):\n   270\t        \"\"\"Reinitializes the parameters of the neural networks\"\"\"\n   271\t        if hasattr(layer, \"reset_parameters\"):\n   272\t            layer.reset_parameters()\n   273\t        for child_layer in layer.modules():\n   274\t            if layer != child_layer:\n   275\t                self.reset_layer_recursively(child_layer)\n   276\t\n   277\t    def save_results(self, test_data):\n   278\t        \"\"\"This script computes the SDR and SI-SNR metrics and saves\n   279\t        them into a csv file\"\"\"\n   280\t\n   281\t        # This package is required for SDR computation\n   282\t        from mir_eval.separation import bss_eval_sources\n   283\t\n   284\t        # Create folders where to store audio\n   285\t        save_file = os.path.join(self.hparams.output_folder, \"test_results.csv\")\n   286\t\n   287\t        # Variable init\n   288\t        all_sdrs = []\n   289\t        all_sdrs_i = []\n   290\t        all_sisnrs = []\n   291\t        all_sisnrs_i = []\n   292\t        csv_columns = [\"snt_id\", \"sdr\", \"sdr_i\", \"si-snr\", \"si-snr_i\"]\n   293\t\n   294\t        test_loader = sb.dataio.dataloader.make_dataloader(\n   295\t            test_data, **self.hparams.dataloader_opts\n   296\t        )\n   297\t\n   298\t        with open(save_file, \"w\", newline=\"\", encoding=\"utf-8\") as results_csv:\n   299\t            writer = csv.DictWriter(results_csv, fieldnames=csv_columns)\n   300\t            writer.writeheader()\n   301\t\n   302\t            # Loop over all test sentence\n   303\t            with tqdm(test_loader, dynamic_ncols=True) as t:\n   304\t                for i, batch in enumerate(t):\n   305\t                    # Apply Separation\n   306\t                    mixture, mix_len = batch.mix_sig\n   307\t                    snt_id = batch.id\n   308\t                    targets = [batch.s1_sig, batch.s2_sig]\n   309\t                    if self.hparams.num_spks == 3:\n   310\t                        targets.append(batch.s3_sig)\n   311\t\n   312\t                    with torch.no_grad():\n   313\t                        predictions, targets = self.compute_forward(\n   314\t                            batch.mix_sig, targets, sb.Stage.TEST\n   315\t                        )\n   316\t\n   317\t                    # Compute SI-SNR\n   318\t                    sisnr = self.compute_objectives(predictions, targets)\n   319\t\n   320\t                    # Compute SI-SNR improvement\n   321\t                    mixture_signal = torch.stack(\n   322\t                        [mixture] * self.hparams.num_spks, dim=-1\n   323\t                    )\n   324\t                    mixture_signal = mixture_signal.to(targets.device)\n   325\t                    sisnr_baseline = self.compute_objectives(\n   326\t                        mixture_signal, targets\n   327\t                    )\n   328\t                    sisnr_i = sisnr - sisnr_baseline\n   329\t\n   330\t                    # Compute SDR\n   331\t                    sdr, _, _, _ = bss_eval_sources(\n   332\t                        targets[0].t().cpu().numpy(),\n   333\t                        predictions[0].t().detach().cpu().numpy(),\n   334\t                    )\n   335\t\n   336\t                    sdr_baseline, _, _, _ = bss_eval_sources(\n   337\t                        targets[0].t().cpu().numpy(),\n   338\t                        mixture_signal[0].t().detach().cpu().numpy(),\n   339\t                    )\n   340\t\n   341\t                    sdr_i = sdr.mean() - sdr_baseline.mean()\n   342\t\n   343\t                    # Saving on a csv file\n   344\t                    row = {\n   345\t                        \"snt_id\": snt_id[0],\n   346\t                        \"sdr\": sdr.mean(),\n   347\t                        \"sdr_i\": sdr_i,\n   348\t                        \"si-snr\": -sisnr.item(),\n   349\t                        \"si-snr_i\": -sisnr_i.item(),\n   350\t                    }\n   351\t                    writer.writerow(row)\n   352\t\n   353\t                    # Metric Accumulation\n   354\t                    all_sdrs.append(sdr.mean())\n   355\t                    all_sdrs_i.append(sdr_i.mean())\n   356\t                    all_sisnrs.append(-sisnr.item())\n   357\t                    all_sisnrs_i.append(-sisnr_i.item())\n   358\t\n   359\t                row = {\n   360\t                    \"snt_id\": \"avg\",\n   361\t                    \"sdr\": np.array(all_sdrs).mean(),\n   362\t                    \"sdr_i\": np.array(all_sdrs_i).mean(),\n   363\t                    \"si-snr\": np.array(all_sisnrs).mean(),\n   364\t                    \"si-snr_i\": np.array(all_sisnrs_i).mean(),\n   365\t                }\n   366\t                writer.writerow(row)\n   367\t\n   368\t        logger.info(\"Mean SISNR is {}\".format(np.array(all_sisnrs).mean()))\n   369\t        logger.info(\"Mean SISNRi is {}\".format(np.array(all_sisnrs_i).mean()))\n   370\t        logger.info(\"Mean SDR is {}\".format(np.array(all_sdrs).mean()))\n   371\t        logger.info(\"Mean SDRi is {}\".format(np.array(all_sdrs_i).mean()))\n   372\t\n   373\t    def save_audio(self, snt_id, mixture, targets, predictions):\n   374\t        \"saves the test audio (mixture, targets, and estimated sources) on disk\"\n   375\t\n   376\t        # Create output folder\n   377\t        save_path = os.path.join(self.hparams.save_folder, \"audio_results\")\n   378\t        if not os.path.exists(save_path):\n   379\t            os.mkdir(save_path)\n   380\t\n   381\t        for ns in range(self.hparams.num_spks):\n   382\t            # Estimated source\n   383\t            signal = predictions[0, :, ns]\n   384\t            signal = signal / signal.abs().max()\n   385\t            save_file = os.path.join(\n   386\t                save_path, \"item{}_source{}hat.wav\".format(snt_id, ns + 1)\n   387\t            )\n   388\t            torchaudio.save(\n   389\t                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n   390\t            )\n   391\t\n   392\t            # Original source\n   393\t            signal = targets[0, :, ns]\n   394\t            signal = signal / signal.abs().max()\n   395\t            save_file = os.path.join(\n   396\t                save_path, \"item{}_source{}.wav\".format(snt_id, ns + 1)\n   397\t            )\n   398\t            torchaudio.save(\n   399\t                save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n   400\t            )\n   401\t\n   402\t        # Mixture\n   403\t        signal = mixture[0][0, :]\n   404\t        signal = signal / signal.abs().max()\n   405\t        save_file = os.path.join(save_path, \"item{}_mix.wav\".format(snt_id))\n   406\t        torchaudio.save(\n   407\t            save_file, signal.unsqueeze(0).cpu(), self.hparams.sample_rate\n   408\t        )\n   409\t\n   410\t\n   411\tdef dataio_prep(hparams):\n   412\t    \"\"\"Creates data processing pipeline\"\"\"\n   413\t\n   414\t    # 1. Define datasets\n   415\t    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n   416\t        csv_path=hparams[\"train_data\"],\n   417\t        replacements={\"data_root\": hparams[\"data_folder\"]},\n   418\t    )\n   419\t\n   420\t    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n   421\t        csv_path=hparams[\"valid_data\"],\n   422\t        replacements={\"data_root\": hparams[\"data_folder\"]},\n   423\t    )\n   424\t\n   425\t    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n   426\t        csv_path=hparams[\"test_data\"],\n   427\t        replacements={\"data_root\": hparams[\"data_folder\"]},\n   428\t    )\n   429\t\n   430\t    datasets = [train_data, valid_data, test_data]\n   431\t\n   432\t    # 2. Provide audio pipelines\n   433\t\n   434\t    @sb.utils.data_pipeline.takes(\"mix_wav\")\n   435\t    @sb.utils.data_pipeline.provides(\"mix_sig\")\n   436\t    def <response clipped><NOTE>Due to the max output limit, only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>\n", "extras": {"path": "/data/data/agent_test_codebase/GitTaskBench/temp_speaker_separation.py", "impl_source": "oh_aci"}}