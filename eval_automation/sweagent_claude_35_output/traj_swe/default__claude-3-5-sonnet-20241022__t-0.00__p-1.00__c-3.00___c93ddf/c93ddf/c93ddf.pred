{"model_name_or_path": "default__claude-3-5-sonnet-20241022__t-0.00__p-1.00__c-3.00___c93ddf", "instance_id": "c93ddf", "model_patch": "diff --git a/extract_content.py b/extract_content.py\nnew file mode 100644\nindex 00000000..fc805a3c\n--- /dev/null\n+++ b/extract_content.py\n@@ -0,0 +1,91 @@\n+\n+import os\n+import subprocess\n+from html.parser import HTMLParser\n+\n+class ContentExtractor(HTMLParser):\n+    def __init__(self):\n+        super().__init__()\n+        self.recording = False\n+        self.content = []\n+        self.title = None\n+        self.current_tag = None\n+        self.in_main_content = False\n+        \n+    def handle_starttag(self, tag, attrs):\n+        attrs = dict(attrs)\n+        self.current_tag = tag\n+        \n+        # Look for the main article content\n+        if tag == 'div' and 'class' in attrs and 'post-content' in attrs['class']:\n+            self.in_main_content = True\n+            self.recording = True\n+        elif tag == 'h1' and not self.title:\n+            self.recording = True\n+            \n+    def handle_endtag(self, tag):\n+        if tag == 'div' and self.in_main_content:\n+            self.in_main_content = False\n+            self.recording = False\n+        elif tag == 'h1':\n+            self.recording = False\n+        self.current_tag = None\n+            \n+    def handle_data(self, data):\n+        if self.recording:\n+            text = data.strip()\n+            if text:\n+                if not self.title and self.current_tag == 'h1':\n+                    self.title = text\n+                    self.content.append(f\"Title: {text}\\n\")\n+                elif text not in self.content and self.in_main_content:\n+                    self.content.append(text)\n+\n+# URL to process\n+url = 'https://github.blog/engineering/leader-spotlight-erin-spiceland/'\n+\n+# Output path\n+output_path = '/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/Trafilatura_01/output'\n+temp_path = output_path + '.tmp'\n+\n+try:\n+    # Download the page using wget\n+    subprocess.run(['curl', '-s', '-k', '-L', '--max-time', '10', '--retry', '2', '-o', temp_path, url], check=True)\n+    \n+    # Check if the file was downloaded and has content\n+    if os.path.exists(temp_path) and os.path.getsize(temp_path) > 0:\n+        # Read the HTML content\n+        with open(temp_path, 'r', encoding='utf-8') as f:\n+            html_content = f.read()\n+        \n+        # Extract the main content using our custom parser\n+        parser = ContentExtractor()\n+        parser.feed(html_content)\n+        \n+        # Join the extracted content\n+        extracted_text = '\\n'.join(parser.content)\n+        \n+        if extracted_text:\n+            # Write the extracted content to the output file\n+            with open(output_path, 'w', encoding='utf-8') as f:\n+                f.write(extracted_text)\n+            print(f\"Content has been successfully extracted and saved to {output_path}\")\n+        else:\n+            print(\"Error: Could not extract content from HTML\")\n+            exit(1)\n+        \n+        # Remove temporary file\n+        os.remove(temp_path)\n+    else:\n+        print(\"Error: Downloaded file is empty\")\n+        exit(1)\n+except subprocess.CalledProcessError as e:\n+    print(f\"Error downloading content: {e}\")\n+    if os.path.exists(temp_path):\n+        os.remove(temp_path)\n+    exit(1)\n+except Exception as e:\n+    print(f\"Error: {e}\")\n+    if os.path.exists(temp_path):\n+        os.remove(temp_path)\n+    exit(1)\n\\ No newline at end of file\n"}