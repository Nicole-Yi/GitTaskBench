# 测试结果汇总表

| Agent | 模型 | 总测试脚本数 | 脚本执行成功 | 脚本执行失败 | 内容验证通过 | 内容验证失败 | 最终成功率 (%) |
|------------|-------------|--------------|----------------|----------------|----------------|----------------|-----------------|
| openhands | claude_35 | 54 | 26 | 28 | 13 | 13 | 24.07 |
| openhands | claude_37 | 54 | 27 | 27 | 13 | 14 | 24.07 |
| openhands | gpt_4o | 54 | 20 | 34 | 8 | 12 | 14.81 |
| sweagent | claude_35 | 54 | 24 | 30 | 8 | 16 | 14.81 |
| sweagent | gpt_4o | 54 | 16 | 38 | 5 | 11 | 9.26 |

**注：**
- **总测试脚本数**：尝试执行的测试脚本总数量。
- **脚本执行成功**：脚本本身成功运行（退出码为0）的数量。
- **脚本执行失败**：脚本运行失败（非零退出码或未知）的数量。
- **内容验证通过**：脚本成功执行后，其产出内容也通过了验证标准的数量。
- **内容验证失败**：脚本成功执行后，其产出内容未通过验证标准或 results.jsonl 文件存在问题的数量。
- **最终成功率 (%)**：(内容验证通过 / 总测试脚本数) * 100%。这是衡量端到端任务完成质量的关键指标。

## 结论

根据上述数据，我们可以得出以下几点结论：

### Agent 性能对比:
- **openhands Agent** 在与相同模型组合时，其"内容验证通过"数量和"最终成功率"普遍优于 **sweagent**。
  - 例如，openhands + claude_35 (24.07%) vs sweagent + claude_35 (14.81%)。
  - openhands + gpt_4o (14.81%) vs sweagent + gpt_4o (9.26%)。

### 模型性能对比:
- 在 **openhands Agent** 下，claude_35 和 claude_37 的表现相近且最佳，最终成功率均为 24.07%。gpt_4o 的表现次之 (14.81%)。
- 在 **sweagent Agent** 下，claude_35 (14.81%) 的表现优于 gpt_4o (9.26%)。
- 总体来看，Claude 系列模型（特别是 claude_35 和 claude_37）在本次测试中的表现优于 gpt_4o。

### 最佳组合:
- 从"最终成功率"来看，**openhands Agent** 与 claude_35 或 claude_37 模型的组合表现最佳，均达到了 24.07% 的成功率。这意味着它们在54个测试脚本中，有13个不仅成功执行了脚本，并且其结果也通过了内容验证。

### 脚本执行成功率 vs 内容验证成功率:
- 所有组合的"脚本执行成功"次数都远高于"内容验证通过"次数。这表明，虽然这些 Agent 和模型能够成功执行许多脚本，但在确保任务结果的正确性和质量方面仍有较大提升空间。例如，openhands + claude_37 成功执行了27个脚本，但只有13个通过了内容验证。

### 普遍挑战:
- 即使是表现最好的组合，最终成功率也仅为 24.07%，说明在 GitTaskBench 这类复杂的代码任务基准上，当前的 Agent 和模型组合仍面临显著挑战。大部分任务未能完全成功（即脚本执行成功且内容验证通过）。

总结而言，**openhands Agent** 结合 claude_35 或 claude_37 模型是本次评估中表现最好的组合。然而，所有参评组合在任务的最终内容验证环节都显示出较大的改进需求。