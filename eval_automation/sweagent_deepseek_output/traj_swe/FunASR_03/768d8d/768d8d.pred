{"model_name_or_path": "deepseek-v3__t-0.00__p-1.00__c-0.00___768d8d", "instance_id": "768d8d", "model_patch": "diff --git a/process_speech.py b/process_speech.py\nnew file mode 100644\nindex 00000000..02757376\n--- /dev/null\n+++ b/process_speech.py\n@@ -0,0 +1,46 @@\n+\n+#!/usr/bin/env python3\n+# -*- encoding: utf-8 -*-\n+# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n+#  MIT License  (https://opensource.org/licenses/MIT)\n+\n+from funasr import AutoModel\n+import os\n+\n+# Initialize the model\n+model = AutoModel(\n+    model=\"Whisper-large-v3-turbo\",\n+    vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n+    vad_kwargs={\"max_single_segment_time\": 30000},\n+)\n+\n+# Input and output paths\n+input_wav = \"/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_03/input/FunASR_03_input.wav\"\n+output_dir = \"/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_03\"\n+\n+# Ensure output directory exists\n+os.makedirs(output_dir, exist_ok=True)\n+\n+# Decoding options\n+DecodingOptions = {\n+    \"task\": \"transcribe\",\n+    \"language\": None,\n+    \"beam_size\": None,\n+    \"fp16\": True,\n+    \"without_timestamps\": False,\n+    \"prompt\": None,\n+}\n+\n+# Process the input file\n+res = model.generate(\n+    DecodingOptions=DecodingOptions,\n+    batch_size_s=0,\n+    input=input_wav,\n+)\n+\n+# Save the result to a file\n+output_file = os.path.join(output_dir, \"output.txt\")\n+with open(output_file, \"w\", encoding=\"utf-8\") as f:\n+    f.write(res)\n+\n+print(f\"Output saved to {output_file}\")\n\\ No newline at end of file\n"}