{"model_name_or_path": "deepseek-v3__t-0.00__p-1.00__c-0.00___f598ac", "instance_id": "f598ac", "model_patch": "diff --git a/speaker_diarization.py b/speaker_diarization.py\nnew file mode 100644\nindex 00000000..b617746f\n--- /dev/null\n+++ b/speaker_diarization.py\n@@ -0,0 +1,57 @@\n+\n+import torch\n+import torchaudio\n+from speechbrain.inference.speaker import SpeakerRecognition\n+from sklearn.cluster import KMeans\n+import os\n+\n+def segment_audio_by_speaker(input_path, output_dir, segment_length=5.0):\n+    \"\"\"\n+    Segment an audio file by speaker using speaker embeddings and clustering.\n+    \n+    Args:\n+        input_path (str): Path to the input audio file.\n+        output_dir (str): Directory to save the segmented audio files.\n+        segment_length (float): Length of each audio segment in seconds.\n+    \"\"\"\n+    # Load the audio file\n+    waveform, sample_rate = torchaudio.load(input_path)\n+    \n+    # Initialize the speaker recognition model\n+    model = SpeakerRecognition.from_hparams(\n+        source=\"speechbrain/spkrec-ecapa-voxceleb\",\n+        savedir=\"tmp_pretrained\"\n+    )\n+    \n+    # Calculate the number of samples per segment\n+    samples_per_segment = int(segment_length * sample_rate)\n+    num_segments = int(waveform.size(1) / samples_per_segment)\n+    \n+    # Extract embeddings for each segment\n+    embeddings = []\n+    for i in range(num_segments):\n+        start = i * samples_per_segment\n+        end = start + samples_per_segment\n+        segment = waveform[:, start:end]\n+        embedding = model.encode_batch(segment, normalize=False)\n+        embeddings.append(embedding.squeeze().cpu().numpy())\n+    \n+    # Cluster embeddings to identify speakers\n+    kmeans = KMeans(n_clusters=2)  # Assuming 2 speakers for simplicity\n+    labels = kmeans.fit_predict(embeddings)\n+    \n+    # Save segments by speaker\n+    os.makedirs(output_dir, exist_ok=True)\n+    for i, label in enumerate(labels):\n+        start = i * samples_per_segment\n+        end = start + samples_per_segment\n+        segment = waveform[:, start:end]\n+        output_path = os.path.join(output_dir, f\"output_{label}_{i}.wav\")\n+        torchaudio.save(output_path, segment, sample_rate)\n+    \n+    print(f\"Segmentation complete. Files saved to {output_dir}\")\n+\n+if __name__ == \"__main__\":\n+    input_path = \"/data/data/agent_test_codebase/GitTaskBench/queries/SpeechBrain_02/input/SpeechBrain_02_input.mp3\"\n+    output_dir = \"/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/SpeechBrain_02\"\n+    segment_audio_by_speaker(input_path, output_dir)\n\\ No newline at end of file\n"}