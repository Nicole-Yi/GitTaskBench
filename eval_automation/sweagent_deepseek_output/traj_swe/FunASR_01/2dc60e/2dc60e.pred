{"model_name_or_path": "deepseek-v3__t-0.00__p-1.00__c-0.00___2dc60e", "instance_id": "2dc60e", "model_patch": "diff --git a/config.yaml b/config.yaml\nnew file mode 100644\nindex 00000000..50138e92\n--- /dev/null\n+++ b/config.yaml\n@@ -0,0 +1,3 @@\n+\n+input: /data/data/agent_test_codebase/GitTaskBench/queries/FunASR_01/input/FunASR_01_input.wav\n+model: speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n\\ No newline at end of file\ndiff --git a/recognize_chinese_speech.sh b/recognize_chinese_speech.sh\nnew file mode 100644\nindex 00000000..eb240bf4\n--- /dev/null\n+++ b/recognize_chinese_speech.sh\n@@ -0,0 +1,23 @@\n+\n+#!/bin/bash\n+# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n+#  MIT License  (https://opensource.org/licenses/MIT)\n+\n+# Input and output paths\n+input=\"/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_01/input/FunASR_01_input.wav\"\n+output_dir=\"/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_01\"\n+\n+# Model for Chinese speech recognition\n+model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n+\n+device=\"cpu\"  # Use CPU for inference\n+\n+# Ensure output directory exists\n+mkdir -p \"${output_dir}\"\n+\n+# Perform speech recognition\n+python -m funasr.bin.inference \\\n+++model=\"${model}\" \\\n+++input=\"${input}\" \\\n+++output_dir=\"${output_dir}\" \\\n+++device=\"${device}\"\n\\ No newline at end of file\ndiff --git a/run_asr.py b/run_asr.py\nnew file mode 100644\nindex 00000000..400701ba\n--- /dev/null\n+++ b/run_asr.py\n@@ -0,0 +1,36 @@\n+\n+#!/usr/bin/env python3\n+# -*- encoding: utf-8 -*-\n+# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n+#  MIT License  (https://opensource.org/licenses/MIT)\n+\n+from funasr import AutoModel\n+import os\n+\n+# Define input and output paths\n+input_audio = \"/data/data/agent_test_codebase/GitTaskBench/queries/FunASR_01/input/FunASR_01_input.wav\"\n+output_dir = \"/data/data/agent_test_codebase/GitTaskBench/eval_automation/output/FunASR_01\"\n+\n+# Ensure output directory exists\n+os.makedirs(output_dir, exist_ok=True)\n+\n+# Initialize the model\n+model = AutoModel(\n+    model=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n+    vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n+    vad_kwargs={\"max_single_segment_time\": 60000},\n+    punc_model=\"iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch\",\n+)\n+\n+# Perform speech recognition\n+result = model.generate(\n+    input=input_audio,\n+    cache={},\n+)\n+\n+# Save the result to a file\n+output_file = os.path.join(output_dir, \"output.txt\")\n+with open(output_file, \"w\") as f:\n+    f.write(str(result))\n+\n+print(f\"Speech recognition completed. Result saved to {output_file}\")\n\\ No newline at end of file\n"}